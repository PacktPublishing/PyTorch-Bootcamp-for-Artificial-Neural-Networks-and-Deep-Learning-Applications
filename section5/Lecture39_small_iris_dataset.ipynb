{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#For reading data from CSV\n",
    "import pandas as pd\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#for loading data into pytorch model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#data preprocessing and splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Performance metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "#defining model\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and visualizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the dataset\n",
    "data_set = pd.read_csv(\"Iris.csv\")\n",
    "#Let's have a look at the dataset\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperating Examples and Labels and string conversion to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (135, 4)\n",
      "y_train shape:  (135,)\n",
      "X_test shape:  (15, 4)\n",
      "y_test shape:  (15,)\n"
     ]
    }
   ],
   "source": [
    "Species = {'Iris-setosa': 0,'Iris-versicolor': 1, 'Iris-virginica':2} \n",
    "data_set.Species = [Species[item] for item in data_set.Species]\n",
    "X= data_set.iloc[:, 0:4] #predictors\n",
    "y= data_set.iloc[:, 4]\n",
    "from sklearn.preprocessing import normalize\n",
    "X = normalize(X)\n",
    "y = np.array(y)\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test= train_test_split(X, y, test_size= 0.10, random_state= 1)\n",
    "print ('X_train shape: ',X_train.shape)\n",
    "print ('y_train shape: ',Y_train.shape)\n",
    "print ('X_test shape: ',X_test.shape)\n",
    "print ('y_test shape: ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dataloader to convert numpy arrays to Tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading data into pytorch model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "##It represents a Python iterable over a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainloader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train)),\n",
    "                         batch_size=135, shuffle=True)\n",
    "testloader = DataLoader(TensorDataset(torch.from_numpy(X_test), torch.from_numpy(Y_test)),\n",
    "                         batch_size=135, shuffle=False)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": trainloader,\n",
    "    \"validation\": testloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This class will define our model\n",
    "### Using __init__ we will define numbers of nodes in our particular layer\n",
    "### Using forward() we will define functionality of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 227) ##4 predictors\n",
    "        self.fc2 = nn.Linear(227, 94) ##227neuron in hidden layer 1, 94 in 2\n",
    "        self.fc3 = nn.Linear(94, 75) ##75 neurons in hidden layer 3\n",
    "        self.fc4 = nn.Linear(75, 3) ##3 classes\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = x.view(x.shape[0], -1)\n",
    "        #print(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x)) ##activation function for 1\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        #x = F.log_softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model declaration, Type of loss and optimizer.\n",
    "### We are using adam optimizer to optimize our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This block is showing summary off our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=4, out_features=227, bias=True)\n",
       "  (fc2): Linear(in_features=227, out_features=94, bias=True)\n",
       "  (fc3): Linear(in_features=94, out_features=75, bias=True)\n",
       "  (fc4): Linear(in_features=75, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function is predicting output of examples we will feed in.\n",
    "### Will be useful in  calculating model accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, inputs):\n",
    "    output = model(inputs)\n",
    "    return output.data.numpy().argmax(axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we will perform forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Iter [1] Loss: 1.0820 Training Accuracy: 0.41481\n",
      "Epoch [2/2000], Iter [1] Loss: 1.0839 Training Accuracy: 0.42963\n",
      "Epoch [3/2000], Iter [1] Loss: 1.0820 Training Accuracy: 0.36296\n",
      "Epoch [4/2000], Iter [1] Loss: 1.0791 Training Accuracy: 0.43704\n",
      "Epoch [5/2000], Iter [1] Loss: 1.0770 Training Accuracy: 0.45926\n",
      "Epoch [6/2000], Iter [1] Loss: 1.0778 Training Accuracy: 0.47407\n",
      "Epoch [7/2000], Iter [1] Loss: 1.0769 Training Accuracy: 0.51852\n",
      "Epoch [8/2000], Iter [1] Loss: 1.0761 Training Accuracy: 0.45185\n",
      "Epoch [9/2000], Iter [1] Loss: 1.0756 Training Accuracy: 0.47407\n",
      "Epoch [10/2000], Iter [1] Loss: 1.0733 Training Accuracy: 0.51111\n",
      "Epoch [11/2000], Iter [1] Loss: 1.0753 Training Accuracy: 0.50370\n",
      "Epoch [12/2000], Iter [1] Loss: 1.0738 Training Accuracy: 0.53333\n",
      "Epoch [13/2000], Iter [1] Loss: 1.0708 Training Accuracy: 0.54074\n",
      "Epoch [14/2000], Iter [1] Loss: 1.0721 Training Accuracy: 0.54074\n",
      "Epoch [15/2000], Iter [1] Loss: 1.0694 Training Accuracy: 0.52593\n",
      "Epoch [16/2000], Iter [1] Loss: 1.0687 Training Accuracy: 0.52593\n",
      "Epoch [17/2000], Iter [1] Loss: 1.0695 Training Accuracy: 0.51852\n",
      "Epoch [18/2000], Iter [1] Loss: 1.0669 Training Accuracy: 0.51852\n",
      "Epoch [19/2000], Iter [1] Loss: 1.0657 Training Accuracy: 0.51111\n",
      "Epoch [20/2000], Iter [1] Loss: 1.0651 Training Accuracy: 0.51852\n",
      "Epoch [21/2000], Iter [1] Loss: 1.0620 Training Accuracy: 0.51852\n",
      "Epoch [22/2000], Iter [1] Loss: 1.0647 Training Accuracy: 0.52593\n",
      "Epoch [23/2000], Iter [1] Loss: 1.0650 Training Accuracy: 0.51111\n",
      "Epoch [24/2000], Iter [1] Loss: 1.0640 Training Accuracy: 0.51111\n",
      "Epoch [25/2000], Iter [1] Loss: 1.0630 Training Accuracy: 0.50370\n",
      "Epoch [26/2000], Iter [1] Loss: 1.0612 Training Accuracy: 0.48889\n",
      "Epoch [27/2000], Iter [1] Loss: 1.0585 Training Accuracy: 0.51852\n",
      "Epoch [28/2000], Iter [1] Loss: 1.0597 Training Accuracy: 0.48889\n",
      "Epoch [29/2000], Iter [1] Loss: 1.0613 Training Accuracy: 0.50370\n",
      "Epoch [30/2000], Iter [1] Loss: 1.0571 Training Accuracy: 0.51852\n",
      "Epoch [31/2000], Iter [1] Loss: 1.0561 Training Accuracy: 0.51111\n",
      "Epoch [32/2000], Iter [1] Loss: 1.0577 Training Accuracy: 0.50370\n",
      "Epoch [33/2000], Iter [1] Loss: 1.0570 Training Accuracy: 0.51111\n",
      "Epoch [34/2000], Iter [1] Loss: 1.0581 Training Accuracy: 0.50370\n",
      "Epoch [35/2000], Iter [1] Loss: 1.0565 Training Accuracy: 0.51111\n",
      "Epoch [36/2000], Iter [1] Loss: 1.0544 Training Accuracy: 0.49630\n",
      "Epoch [37/2000], Iter [1] Loss: 1.0526 Training Accuracy: 0.51111\n",
      "Epoch [38/2000], Iter [1] Loss: 1.0527 Training Accuracy: 0.49630\n",
      "Epoch [39/2000], Iter [1] Loss: 1.0537 Training Accuracy: 0.48889\n",
      "Epoch [40/2000], Iter [1] Loss: 1.0514 Training Accuracy: 0.51111\n",
      "Epoch [41/2000], Iter [1] Loss: 1.0463 Training Accuracy: 0.51111\n",
      "Epoch [42/2000], Iter [1] Loss: 1.0479 Training Accuracy: 0.50370\n",
      "Epoch [43/2000], Iter [1] Loss: 1.0514 Training Accuracy: 0.50370\n",
      "Epoch [44/2000], Iter [1] Loss: 1.0449 Training Accuracy: 0.51111\n",
      "Epoch [45/2000], Iter [1] Loss: 1.0473 Training Accuracy: 0.49630\n",
      "Epoch [46/2000], Iter [1] Loss: 1.0472 Training Accuracy: 0.50370\n",
      "Epoch [47/2000], Iter [1] Loss: 1.0460 Training Accuracy: 0.51852\n",
      "Epoch [48/2000], Iter [1] Loss: 1.0469 Training Accuracy: 0.51852\n",
      "Epoch [49/2000], Iter [1] Loss: 1.0457 Training Accuracy: 0.50370\n",
      "Epoch [50/2000], Iter [1] Loss: 1.0432 Training Accuracy: 0.49630\n",
      "Epoch [51/2000], Iter [1] Loss: 1.0436 Training Accuracy: 0.50370\n",
      "Epoch [52/2000], Iter [1] Loss: 1.0400 Training Accuracy: 0.51111\n",
      "Epoch [53/2000], Iter [1] Loss: 1.0414 Training Accuracy: 0.52593\n",
      "Epoch [54/2000], Iter [1] Loss: 1.0409 Training Accuracy: 0.49630\n",
      "Epoch [55/2000], Iter [1] Loss: 1.0398 Training Accuracy: 0.50370\n",
      "Epoch [56/2000], Iter [1] Loss: 1.0360 Training Accuracy: 0.51111\n",
      "Epoch [57/2000], Iter [1] Loss: 1.0373 Training Accuracy: 0.51852\n",
      "Epoch [58/2000], Iter [1] Loss: 1.0341 Training Accuracy: 0.50370\n",
      "Epoch [59/2000], Iter [1] Loss: 1.0391 Training Accuracy: 0.50370\n",
      "Epoch [60/2000], Iter [1] Loss: 1.0347 Training Accuracy: 0.51852\n",
      "Epoch [61/2000], Iter [1] Loss: 1.0325 Training Accuracy: 0.51111\n",
      "Epoch [62/2000], Iter [1] Loss: 1.0320 Training Accuracy: 0.51111\n",
      "Epoch [63/2000], Iter [1] Loss: 1.0321 Training Accuracy: 0.51111\n",
      "Epoch [64/2000], Iter [1] Loss: 1.0310 Training Accuracy: 0.51852\n",
      "Epoch [65/2000], Iter [1] Loss: 1.0291 Training Accuracy: 0.50370\n",
      "Epoch [66/2000], Iter [1] Loss: 1.0329 Training Accuracy: 0.51111\n",
      "Epoch [67/2000], Iter [1] Loss: 1.0311 Training Accuracy: 0.51111\n",
      "Epoch [68/2000], Iter [1] Loss: 1.0294 Training Accuracy: 0.51852\n",
      "Epoch [69/2000], Iter [1] Loss: 1.0305 Training Accuracy: 0.51852\n",
      "Epoch [70/2000], Iter [1] Loss: 1.0278 Training Accuracy: 0.51111\n",
      "Epoch [71/2000], Iter [1] Loss: 1.0287 Training Accuracy: 0.51852\n",
      "Epoch [72/2000], Iter [1] Loss: 1.0303 Training Accuracy: 0.49630\n",
      "Epoch [73/2000], Iter [1] Loss: 1.0284 Training Accuracy: 0.50370\n",
      "Epoch [74/2000], Iter [1] Loss: 1.0290 Training Accuracy: 0.51111\n",
      "Epoch [75/2000], Iter [1] Loss: 1.0229 Training Accuracy: 0.51111\n",
      "Epoch [76/2000], Iter [1] Loss: 1.0281 Training Accuracy: 0.50370\n",
      "Epoch [77/2000], Iter [1] Loss: 1.0233 Training Accuracy: 0.51111\n",
      "Epoch [78/2000], Iter [1] Loss: 1.0227 Training Accuracy: 0.51852\n",
      "Epoch [79/2000], Iter [1] Loss: 1.0184 Training Accuracy: 0.50370\n",
      "Epoch [80/2000], Iter [1] Loss: 1.0200 Training Accuracy: 0.52593\n",
      "Epoch [81/2000], Iter [1] Loss: 1.0184 Training Accuracy: 0.51111\n",
      "Epoch [82/2000], Iter [1] Loss: 1.0206 Training Accuracy: 0.52593\n",
      "Epoch [83/2000], Iter [1] Loss: 1.0176 Training Accuracy: 0.51111\n",
      "Epoch [84/2000], Iter [1] Loss: 1.0130 Training Accuracy: 0.51852\n",
      "Epoch [85/2000], Iter [1] Loss: 1.0132 Training Accuracy: 0.51852\n",
      "Epoch [86/2000], Iter [1] Loss: 1.0227 Training Accuracy: 0.50370\n",
      "Epoch [87/2000], Iter [1] Loss: 1.0129 Training Accuracy: 0.51852\n",
      "Epoch [88/2000], Iter [1] Loss: 1.0126 Training Accuracy: 0.51111\n",
      "Epoch [89/2000], Iter [1] Loss: 1.0121 Training Accuracy: 0.51852\n",
      "Epoch [90/2000], Iter [1] Loss: 1.0206 Training Accuracy: 0.51852\n",
      "Epoch [91/2000], Iter [1] Loss: 1.0119 Training Accuracy: 0.50370\n",
      "Epoch [92/2000], Iter [1] Loss: 1.0190 Training Accuracy: 0.53333\n",
      "Epoch [93/2000], Iter [1] Loss: 1.0186 Training Accuracy: 0.51111\n",
      "Epoch [94/2000], Iter [1] Loss: 1.0106 Training Accuracy: 0.51111\n",
      "Epoch [95/2000], Iter [1] Loss: 1.0077 Training Accuracy: 0.51852\n",
      "Epoch [96/2000], Iter [1] Loss: 1.0128 Training Accuracy: 0.52593\n",
      "Epoch [97/2000], Iter [1] Loss: 1.0099 Training Accuracy: 0.51852\n",
      "Epoch [98/2000], Iter [1] Loss: 1.0101 Training Accuracy: 0.51852\n",
      "Epoch [99/2000], Iter [1] Loss: 1.0029 Training Accuracy: 0.52593\n",
      "Epoch [100/2000], Iter [1] Loss: 1.0061 Training Accuracy: 0.52593\n",
      "Epoch [101/2000], Iter [1] Loss: 1.0105 Training Accuracy: 0.51852\n",
      "Epoch [102/2000], Iter [1] Loss: 1.0020 Training Accuracy: 0.51111\n",
      "Epoch [103/2000], Iter [1] Loss: 1.0104 Training Accuracy: 0.51852\n",
      "Epoch [104/2000], Iter [1] Loss: 1.0072 Training Accuracy: 0.52593\n",
      "Epoch [105/2000], Iter [1] Loss: 1.0073 Training Accuracy: 0.51852\n",
      "Epoch [106/2000], Iter [1] Loss: 1.0016 Training Accuracy: 0.51852\n",
      "Epoch [107/2000], Iter [1] Loss: 1.0022 Training Accuracy: 0.53333\n",
      "Epoch [108/2000], Iter [1] Loss: 0.9984 Training Accuracy: 0.53333\n",
      "Epoch [109/2000], Iter [1] Loss: 1.0036 Training Accuracy: 0.51111\n",
      "Epoch [110/2000], Iter [1] Loss: 1.0033 Training Accuracy: 0.52593\n",
      "Epoch [111/2000], Iter [1] Loss: 1.0005 Training Accuracy: 0.51852\n",
      "Epoch [112/2000], Iter [1] Loss: 1.0073 Training Accuracy: 0.51852\n",
      "Epoch [113/2000], Iter [1] Loss: 0.9991 Training Accuracy: 0.52593\n",
      "Epoch [114/2000], Iter [1] Loss: 0.9981 Training Accuracy: 0.54074\n",
      "Epoch [115/2000], Iter [1] Loss: 0.9928 Training Accuracy: 0.53333\n",
      "Epoch [116/2000], Iter [1] Loss: 0.9976 Training Accuracy: 0.51852\n",
      "Epoch [117/2000], Iter [1] Loss: 0.9995 Training Accuracy: 0.54074\n",
      "Epoch [118/2000], Iter [1] Loss: 0.9987 Training Accuracy: 0.54074\n",
      "Epoch [119/2000], Iter [1] Loss: 0.9922 Training Accuracy: 0.53333\n",
      "Epoch [120/2000], Iter [1] Loss: 0.9972 Training Accuracy: 0.52593\n",
      "Epoch [121/2000], Iter [1] Loss: 1.0000 Training Accuracy: 0.53333\n",
      "Epoch [122/2000], Iter [1] Loss: 0.9890 Training Accuracy: 0.54074\n",
      "Epoch [123/2000], Iter [1] Loss: 0.9934 Training Accuracy: 0.53333\n",
      "Epoch [124/2000], Iter [1] Loss: 0.9904 Training Accuracy: 0.52593\n",
      "Epoch [125/2000], Iter [1] Loss: 0.9905 Training Accuracy: 0.53333\n",
      "Epoch [126/2000], Iter [1] Loss: 0.9877 Training Accuracy: 0.53333\n",
      "Epoch [127/2000], Iter [1] Loss: 0.9862 Training Accuracy: 0.53333\n",
      "Epoch [128/2000], Iter [1] Loss: 0.9865 Training Accuracy: 0.52593\n",
      "Epoch [129/2000], Iter [1] Loss: 0.9861 Training Accuracy: 0.55556\n",
      "Epoch [130/2000], Iter [1] Loss: 0.9859 Training Accuracy: 0.53333\n",
      "Epoch [131/2000], Iter [1] Loss: 0.9895 Training Accuracy: 0.54074\n",
      "Epoch [132/2000], Iter [1] Loss: 0.9866 Training Accuracy: 0.52593\n",
      "Epoch [133/2000], Iter [1] Loss: 0.9885 Training Accuracy: 0.54815\n",
      "Epoch [134/2000], Iter [1] Loss: 0.9806 Training Accuracy: 0.54074\n",
      "Epoch [135/2000], Iter [1] Loss: 0.9818 Training Accuracy: 0.53333\n",
      "Epoch [136/2000], Iter [1] Loss: 0.9824 Training Accuracy: 0.54815\n",
      "Epoch [137/2000], Iter [1] Loss: 0.9866 Training Accuracy: 0.54815\n",
      "Epoch [138/2000], Iter [1] Loss: 0.9822 Training Accuracy: 0.54074\n",
      "Epoch [139/2000], Iter [1] Loss: 0.9752 Training Accuracy: 0.55556\n",
      "Epoch [140/2000], Iter [1] Loss: 0.9798 Training Accuracy: 0.54815\n",
      "Epoch [141/2000], Iter [1] Loss: 0.9773 Training Accuracy: 0.55556\n",
      "Epoch [142/2000], Iter [1] Loss: 0.9820 Training Accuracy: 0.54815\n",
      "Epoch [143/2000], Iter [1] Loss: 0.9817 Training Accuracy: 0.55556\n",
      "Epoch [144/2000], Iter [1] Loss: 0.9718 Training Accuracy: 0.56296\n",
      "Epoch [145/2000], Iter [1] Loss: 0.9769 Training Accuracy: 0.54815\n",
      "Epoch [146/2000], Iter [1] Loss: 0.9765 Training Accuracy: 0.54815\n",
      "Epoch [147/2000], Iter [1] Loss: 0.9745 Training Accuracy: 0.55556\n",
      "Epoch [148/2000], Iter [1] Loss: 0.9798 Training Accuracy: 0.54074\n",
      "Epoch [149/2000], Iter [1] Loss: 0.9675 Training Accuracy: 0.57037\n",
      "Epoch [150/2000], Iter [1] Loss: 0.9722 Training Accuracy: 0.57037\n",
      "Epoch [151/2000], Iter [1] Loss: 0.9769 Training Accuracy: 0.55556\n",
      "Epoch [152/2000], Iter [1] Loss: 0.9648 Training Accuracy: 0.56296\n",
      "Epoch [153/2000], Iter [1] Loss: 0.9732 Training Accuracy: 0.57037\n",
      "Epoch [154/2000], Iter [1] Loss: 0.9623 Training Accuracy: 0.56296\n",
      "Epoch [155/2000], Iter [1] Loss: 0.9660 Training Accuracy: 0.55556\n",
      "Epoch [156/2000], Iter [1] Loss: 0.9679 Training Accuracy: 0.57037\n",
      "Epoch [157/2000], Iter [1] Loss: 0.9642 Training Accuracy: 0.55556\n",
      "Epoch [158/2000], Iter [1] Loss: 0.9666 Training Accuracy: 0.57037\n",
      "Epoch [159/2000], Iter [1] Loss: 0.9562 Training Accuracy: 0.57037\n",
      "Epoch [160/2000], Iter [1] Loss: 0.9699 Training Accuracy: 0.58519\n",
      "Epoch [161/2000], Iter [1] Loss: 0.9553 Training Accuracy: 0.57037\n",
      "Epoch [162/2000], Iter [1] Loss: 0.9636 Training Accuracy: 0.56296\n",
      "Epoch [163/2000], Iter [1] Loss: 0.9609 Training Accuracy: 0.57037\n",
      "Epoch [164/2000], Iter [1] Loss: 0.9572 Training Accuracy: 0.57778\n",
      "Epoch [165/2000], Iter [1] Loss: 0.9611 Training Accuracy: 0.57037\n",
      "Epoch [166/2000], Iter [1] Loss: 0.9510 Training Accuracy: 0.58519\n",
      "Epoch [167/2000], Iter [1] Loss: 0.9442 Training Accuracy: 0.57037\n",
      "Epoch [168/2000], Iter [1] Loss: 0.9526 Training Accuracy: 0.56296\n",
      "Epoch [169/2000], Iter [1] Loss: 0.9514 Training Accuracy: 0.57778\n",
      "Epoch [170/2000], Iter [1] Loss: 0.9457 Training Accuracy: 0.59259\n",
      "Epoch [171/2000], Iter [1] Loss: 0.9511 Training Accuracy: 0.59259\n",
      "Epoch [172/2000], Iter [1] Loss: 0.9478 Training Accuracy: 0.57037\n",
      "Epoch [173/2000], Iter [1] Loss: 0.9453 Training Accuracy: 0.57778\n",
      "Epoch [174/2000], Iter [1] Loss: 0.9467 Training Accuracy: 0.59259\n",
      "Epoch [175/2000], Iter [1] Loss: 0.9401 Training Accuracy: 0.59259\n",
      "Epoch [176/2000], Iter [1] Loss: 0.9489 Training Accuracy: 0.59259\n",
      "Epoch [177/2000], Iter [1] Loss: 0.9440 Training Accuracy: 0.57778\n",
      "Epoch [178/2000], Iter [1] Loss: 0.9502 Training Accuracy: 0.59259\n",
      "Epoch [179/2000], Iter [1] Loss: 0.9464 Training Accuracy: 0.58519\n",
      "Epoch [180/2000], Iter [1] Loss: 0.9418 Training Accuracy: 0.58519\n",
      "Epoch [181/2000], Iter [1] Loss: 0.9355 Training Accuracy: 0.60000\n",
      "Epoch [182/2000], Iter [1] Loss: 0.9314 Training Accuracy: 0.60000\n",
      "Epoch [183/2000], Iter [1] Loss: 0.9339 Training Accuracy: 0.60741\n",
      "Epoch [184/2000], Iter [1] Loss: 0.9336 Training Accuracy: 0.59259\n",
      "Epoch [185/2000], Iter [1] Loss: 0.9331 Training Accuracy: 0.59259\n",
      "Epoch [186/2000], Iter [1] Loss: 0.9298 Training Accuracy: 0.60741\n",
      "Epoch [187/2000], Iter [1] Loss: 0.9294 Training Accuracy: 0.59259\n",
      "Epoch [188/2000], Iter [1] Loss: 0.9340 Training Accuracy: 0.59259\n",
      "Epoch [189/2000], Iter [1] Loss: 0.9342 Training Accuracy: 0.60741\n",
      "Epoch [190/2000], Iter [1] Loss: 0.9176 Training Accuracy: 0.60000\n",
      "Epoch [191/2000], Iter [1] Loss: 0.9295 Training Accuracy: 0.61481\n",
      "Epoch [192/2000], Iter [1] Loss: 0.9240 Training Accuracy: 0.60000\n",
      "Epoch [193/2000], Iter [1] Loss: 0.9225 Training Accuracy: 0.59259\n",
      "Epoch [194/2000], Iter [1] Loss: 0.9148 Training Accuracy: 0.62222\n",
      "Epoch [195/2000], Iter [1] Loss: 0.9235 Training Accuracy: 0.60000\n",
      "Epoch [196/2000], Iter [1] Loss: 0.9250 Training Accuracy: 0.60741\n",
      "Epoch [197/2000], Iter [1] Loss: 0.9185 Training Accuracy: 0.60741\n",
      "Epoch [198/2000], Iter [1] Loss: 0.9212 Training Accuracy: 0.61481\n",
      "Epoch [199/2000], Iter [1] Loss: 0.9195 Training Accuracy: 0.61481\n",
      "Epoch [200/2000], Iter [1] Loss: 0.9165 Training Accuracy: 0.61481\n",
      "Epoch [201/2000], Iter [1] Loss: 0.9080 Training Accuracy: 0.60741\n",
      "Epoch [202/2000], Iter [1] Loss: 0.9131 Training Accuracy: 0.60000\n",
      "Epoch [203/2000], Iter [1] Loss: 0.9107 Training Accuracy: 0.60741\n",
      "Epoch [204/2000], Iter [1] Loss: 0.9069 Training Accuracy: 0.60000\n",
      "Epoch [205/2000], Iter [1] Loss: 0.9053 Training Accuracy: 0.62222\n",
      "Epoch [206/2000], Iter [1] Loss: 0.9110 Training Accuracy: 0.62222\n",
      "Epoch [207/2000], Iter [1] Loss: 0.9075 Training Accuracy: 0.60741\n",
      "Epoch [208/2000], Iter [1] Loss: 0.8933 Training Accuracy: 0.60000\n",
      "Epoch [209/2000], Iter [1] Loss: 0.8981 Training Accuracy: 0.60741\n",
      "Epoch [210/2000], Iter [1] Loss: 0.8928 Training Accuracy: 0.60741\n",
      "Epoch [211/2000], Iter [1] Loss: 0.8983 Training Accuracy: 0.62222\n",
      "Epoch [212/2000], Iter [1] Loss: 0.8973 Training Accuracy: 0.62222\n",
      "Epoch [213/2000], Iter [1] Loss: 0.8970 Training Accuracy: 0.60741\n",
      "Epoch [214/2000], Iter [1] Loss: 0.8917 Training Accuracy: 0.62963\n",
      "Epoch [215/2000], Iter [1] Loss: 0.8790 Training Accuracy: 0.61481\n",
      "Epoch [216/2000], Iter [1] Loss: 0.8976 Training Accuracy: 0.62222\n",
      "Epoch [217/2000], Iter [1] Loss: 0.8879 Training Accuracy: 0.62222\n",
      "Epoch [218/2000], Iter [1] Loss: 0.8848 Training Accuracy: 0.62963\n",
      "Epoch [219/2000], Iter [1] Loss: 0.8821 Training Accuracy: 0.62963\n",
      "Epoch [220/2000], Iter [1] Loss: 0.8864 Training Accuracy: 0.62963\n",
      "Epoch [221/2000], Iter [1] Loss: 0.8791 Training Accuracy: 0.62222\n",
      "Epoch [222/2000], Iter [1] Loss: 0.8825 Training Accuracy: 0.62963\n",
      "Epoch [223/2000], Iter [1] Loss: 0.8833 Training Accuracy: 0.62222\n",
      "Epoch [224/2000], Iter [1] Loss: 0.8820 Training Accuracy: 0.64444\n",
      "Epoch [225/2000], Iter [1] Loss: 0.8713 Training Accuracy: 0.63704\n",
      "Epoch [226/2000], Iter [1] Loss: 0.8750 Training Accuracy: 0.63704\n",
      "Epoch [227/2000], Iter [1] Loss: 0.8678 Training Accuracy: 0.62963\n",
      "Epoch [228/2000], Iter [1] Loss: 0.8779 Training Accuracy: 0.62963\n",
      "Epoch [229/2000], Iter [1] Loss: 0.8808 Training Accuracy: 0.62963\n",
      "Epoch [230/2000], Iter [1] Loss: 0.8785 Training Accuracy: 0.63704\n",
      "Epoch [231/2000], Iter [1] Loss: 0.8687 Training Accuracy: 0.64444\n",
      "Epoch [232/2000], Iter [1] Loss: 0.8639 Training Accuracy: 0.62222\n",
      "Epoch [233/2000], Iter [1] Loss: 0.8776 Training Accuracy: 0.62963\n",
      "Epoch [234/2000], Iter [1] Loss: 0.8710 Training Accuracy: 0.64444\n",
      "Epoch [235/2000], Iter [1] Loss: 0.8576 Training Accuracy: 0.65185\n",
      "Epoch [236/2000], Iter [1] Loss: 0.8631 Training Accuracy: 0.65926\n",
      "Epoch [237/2000], Iter [1] Loss: 0.8639 Training Accuracy: 0.63704\n",
      "Epoch [238/2000], Iter [1] Loss: 0.8652 Training Accuracy: 0.65185\n",
      "Epoch [239/2000], Iter [1] Loss: 0.8565 Training Accuracy: 0.65185\n",
      "Epoch [240/2000], Iter [1] Loss: 0.8556 Training Accuracy: 0.65926\n",
      "Epoch [241/2000], Iter [1] Loss: 0.8496 Training Accuracy: 0.64444\n",
      "Epoch [242/2000], Iter [1] Loss: 0.8503 Training Accuracy: 0.65926\n",
      "Epoch [243/2000], Iter [1] Loss: 0.8425 Training Accuracy: 0.64444\n",
      "Epoch [244/2000], Iter [1] Loss: 0.8530 Training Accuracy: 0.66667\n",
      "Epoch [245/2000], Iter [1] Loss: 0.8492 Training Accuracy: 0.65185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [246/2000], Iter [1] Loss: 0.8471 Training Accuracy: 0.65926\n",
      "Epoch [247/2000], Iter [1] Loss: 0.8449 Training Accuracy: 0.65185\n",
      "Epoch [248/2000], Iter [1] Loss: 0.8359 Training Accuracy: 0.65926\n",
      "Epoch [249/2000], Iter [1] Loss: 0.8360 Training Accuracy: 0.65926\n",
      "Epoch [250/2000], Iter [1] Loss: 0.8384 Training Accuracy: 0.65926\n",
      "Epoch [251/2000], Iter [1] Loss: 0.8341 Training Accuracy: 0.65185\n",
      "Epoch [252/2000], Iter [1] Loss: 0.8362 Training Accuracy: 0.65926\n",
      "Epoch [253/2000], Iter [1] Loss: 0.8328 Training Accuracy: 0.64444\n",
      "Epoch [254/2000], Iter [1] Loss: 0.8388 Training Accuracy: 0.66667\n",
      "Epoch [255/2000], Iter [1] Loss: 0.8268 Training Accuracy: 0.65926\n",
      "Epoch [256/2000], Iter [1] Loss: 0.8383 Training Accuracy: 0.65926\n",
      "Epoch [257/2000], Iter [1] Loss: 0.8289 Training Accuracy: 0.65926\n",
      "Epoch [258/2000], Iter [1] Loss: 0.8189 Training Accuracy: 0.65185\n",
      "Epoch [259/2000], Iter [1] Loss: 0.8306 Training Accuracy: 0.66667\n",
      "Epoch [260/2000], Iter [1] Loss: 0.8252 Training Accuracy: 0.67407\n",
      "Epoch [261/2000], Iter [1] Loss: 0.8144 Training Accuracy: 0.65926\n",
      "Epoch [262/2000], Iter [1] Loss: 0.8204 Training Accuracy: 0.67407\n",
      "Epoch [263/2000], Iter [1] Loss: 0.8176 Training Accuracy: 0.66667\n",
      "Epoch [264/2000], Iter [1] Loss: 0.8227 Training Accuracy: 0.66667\n",
      "Epoch [265/2000], Iter [1] Loss: 0.8128 Training Accuracy: 0.67407\n",
      "Epoch [266/2000], Iter [1] Loss: 0.8113 Training Accuracy: 0.66667\n",
      "Epoch [267/2000], Iter [1] Loss: 0.8062 Training Accuracy: 0.67407\n",
      "Epoch [268/2000], Iter [1] Loss: 0.8122 Training Accuracy: 0.68889\n",
      "Epoch [269/2000], Iter [1] Loss: 0.8081 Training Accuracy: 0.67407\n",
      "Epoch [270/2000], Iter [1] Loss: 0.8101 Training Accuracy: 0.68889\n",
      "Epoch [271/2000], Iter [1] Loss: 0.7997 Training Accuracy: 0.68148\n",
      "Epoch [272/2000], Iter [1] Loss: 0.8046 Training Accuracy: 0.67407\n",
      "Epoch [273/2000], Iter [1] Loss: 0.8044 Training Accuracy: 0.68148\n",
      "Epoch [274/2000], Iter [1] Loss: 0.7969 Training Accuracy: 0.68148\n",
      "Epoch [275/2000], Iter [1] Loss: 0.7978 Training Accuracy: 0.68889\n",
      "Epoch [276/2000], Iter [1] Loss: 0.7874 Training Accuracy: 0.66667\n",
      "Epoch [277/2000], Iter [1] Loss: 0.8030 Training Accuracy: 0.68889\n",
      "Epoch [278/2000], Iter [1] Loss: 0.8044 Training Accuracy: 0.68148\n",
      "Epoch [279/2000], Iter [1] Loss: 0.7902 Training Accuracy: 0.68148\n",
      "Epoch [280/2000], Iter [1] Loss: 0.7887 Training Accuracy: 0.68889\n",
      "Epoch [281/2000], Iter [1] Loss: 0.7916 Training Accuracy: 0.69630\n",
      "Epoch [282/2000], Iter [1] Loss: 0.7928 Training Accuracy: 0.69630\n",
      "Epoch [283/2000], Iter [1] Loss: 0.7879 Training Accuracy: 0.68889\n",
      "Epoch [284/2000], Iter [1] Loss: 0.7776 Training Accuracy: 0.70370\n",
      "Epoch [285/2000], Iter [1] Loss: 0.7792 Training Accuracy: 0.67407\n",
      "Epoch [286/2000], Iter [1] Loss: 0.7837 Training Accuracy: 0.69630\n",
      "Epoch [287/2000], Iter [1] Loss: 0.7878 Training Accuracy: 0.68889\n",
      "Epoch [288/2000], Iter [1] Loss: 0.7851 Training Accuracy: 0.68148\n",
      "Epoch [289/2000], Iter [1] Loss: 0.7852 Training Accuracy: 0.69630\n",
      "Epoch [290/2000], Iter [1] Loss: 0.7774 Training Accuracy: 0.71111\n",
      "Epoch [291/2000], Iter [1] Loss: 0.7779 Training Accuracy: 0.69630\n",
      "Epoch [292/2000], Iter [1] Loss: 0.7653 Training Accuracy: 0.68889\n",
      "Epoch [293/2000], Iter [1] Loss: 0.7720 Training Accuracy: 0.68148\n",
      "Epoch [294/2000], Iter [1] Loss: 0.7673 Training Accuracy: 0.67407\n",
      "Epoch [295/2000], Iter [1] Loss: 0.7749 Training Accuracy: 0.68148\n",
      "Epoch [296/2000], Iter [1] Loss: 0.7659 Training Accuracy: 0.70370\n",
      "Epoch [297/2000], Iter [1] Loss: 0.7775 Training Accuracy: 0.68889\n",
      "Epoch [298/2000], Iter [1] Loss: 0.7689 Training Accuracy: 0.70370\n",
      "Epoch [299/2000], Iter [1] Loss: 0.7727 Training Accuracy: 0.68889\n",
      "Epoch [300/2000], Iter [1] Loss: 0.7568 Training Accuracy: 0.71111\n",
      "Epoch [301/2000], Iter [1] Loss: 0.7618 Training Accuracy: 0.69630\n",
      "Epoch [302/2000], Iter [1] Loss: 0.7569 Training Accuracy: 0.70370\n",
      "Epoch [303/2000], Iter [1] Loss: 0.7606 Training Accuracy: 0.69630\n",
      "Epoch [304/2000], Iter [1] Loss: 0.7701 Training Accuracy: 0.71111\n",
      "Epoch [305/2000], Iter [1] Loss: 0.7551 Training Accuracy: 0.69630\n",
      "Epoch [306/2000], Iter [1] Loss: 0.7636 Training Accuracy: 0.69630\n",
      "Epoch [307/2000], Iter [1] Loss: 0.7499 Training Accuracy: 0.70370\n",
      "Epoch [308/2000], Iter [1] Loss: 0.7519 Training Accuracy: 0.71111\n",
      "Epoch [309/2000], Iter [1] Loss: 0.7560 Training Accuracy: 0.71852\n",
      "Epoch [310/2000], Iter [1] Loss: 0.7499 Training Accuracy: 0.71852\n",
      "Epoch [311/2000], Iter [1] Loss: 0.7492 Training Accuracy: 0.71111\n",
      "Epoch [312/2000], Iter [1] Loss: 0.7495 Training Accuracy: 0.70370\n",
      "Epoch [313/2000], Iter [1] Loss: 0.7467 Training Accuracy: 0.70370\n",
      "Epoch [314/2000], Iter [1] Loss: 0.7424 Training Accuracy: 0.70370\n",
      "Epoch [315/2000], Iter [1] Loss: 0.7422 Training Accuracy: 0.69630\n",
      "Epoch [316/2000], Iter [1] Loss: 0.7329 Training Accuracy: 0.71852\n",
      "Epoch [317/2000], Iter [1] Loss: 0.7463 Training Accuracy: 0.71852\n",
      "Epoch [318/2000], Iter [1] Loss: 0.7330 Training Accuracy: 0.70370\n",
      "Epoch [319/2000], Iter [1] Loss: 0.7368 Training Accuracy: 0.71852\n",
      "Epoch [320/2000], Iter [1] Loss: 0.7255 Training Accuracy: 0.71111\n",
      "Epoch [321/2000], Iter [1] Loss: 0.7363 Training Accuracy: 0.71852\n",
      "Epoch [322/2000], Iter [1] Loss: 0.7315 Training Accuracy: 0.70370\n",
      "Epoch [323/2000], Iter [1] Loss: 0.7352 Training Accuracy: 0.70370\n",
      "Epoch [324/2000], Iter [1] Loss: 0.7372 Training Accuracy: 0.72593\n",
      "Epoch [325/2000], Iter [1] Loss: 0.7182 Training Accuracy: 0.68889\n",
      "Epoch [326/2000], Iter [1] Loss: 0.7164 Training Accuracy: 0.71111\n",
      "Epoch [327/2000], Iter [1] Loss: 0.7199 Training Accuracy: 0.71111\n",
      "Epoch [328/2000], Iter [1] Loss: 0.7197 Training Accuracy: 0.71111\n",
      "Epoch [329/2000], Iter [1] Loss: 0.7278 Training Accuracy: 0.71111\n",
      "Epoch [330/2000], Iter [1] Loss: 0.7292 Training Accuracy: 0.71111\n",
      "Epoch [331/2000], Iter [1] Loss: 0.7198 Training Accuracy: 0.71111\n",
      "Epoch [332/2000], Iter [1] Loss: 0.7213 Training Accuracy: 0.71111\n",
      "Epoch [333/2000], Iter [1] Loss: 0.7276 Training Accuracy: 0.70370\n",
      "Epoch [334/2000], Iter [1] Loss: 0.7220 Training Accuracy: 0.71111\n",
      "Epoch [335/2000], Iter [1] Loss: 0.7198 Training Accuracy: 0.71852\n",
      "Epoch [336/2000], Iter [1] Loss: 0.7105 Training Accuracy: 0.71852\n",
      "Epoch [337/2000], Iter [1] Loss: 0.7173 Training Accuracy: 0.71111\n",
      "Epoch [338/2000], Iter [1] Loss: 0.7183 Training Accuracy: 0.72593\n",
      "Epoch [339/2000], Iter [1] Loss: 0.7192 Training Accuracy: 0.71111\n",
      "Epoch [340/2000], Iter [1] Loss: 0.7152 Training Accuracy: 0.72593\n",
      "Epoch [341/2000], Iter [1] Loss: 0.7068 Training Accuracy: 0.71111\n",
      "Epoch [342/2000], Iter [1] Loss: 0.7128 Training Accuracy: 0.71852\n",
      "Epoch [343/2000], Iter [1] Loss: 0.7100 Training Accuracy: 0.71852\n",
      "Epoch [344/2000], Iter [1] Loss: 0.6973 Training Accuracy: 0.72593\n",
      "Epoch [345/2000], Iter [1] Loss: 0.7119 Training Accuracy: 0.71111\n",
      "Epoch [346/2000], Iter [1] Loss: 0.7043 Training Accuracy: 0.71852\n",
      "Epoch [347/2000], Iter [1] Loss: 0.7073 Training Accuracy: 0.74074\n",
      "Epoch [348/2000], Iter [1] Loss: 0.6922 Training Accuracy: 0.72593\n",
      "Epoch [349/2000], Iter [1] Loss: 0.7062 Training Accuracy: 0.72593\n",
      "Epoch [350/2000], Iter [1] Loss: 0.7124 Training Accuracy: 0.72593\n",
      "Epoch [351/2000], Iter [1] Loss: 0.6937 Training Accuracy: 0.71852\n",
      "Epoch [352/2000], Iter [1] Loss: 0.6989 Training Accuracy: 0.71852\n",
      "Epoch [353/2000], Iter [1] Loss: 0.6888 Training Accuracy: 0.73333\n",
      "Epoch [354/2000], Iter [1] Loss: 0.6843 Training Accuracy: 0.73333\n",
      "Epoch [355/2000], Iter [1] Loss: 0.6897 Training Accuracy: 0.72593\n",
      "Epoch [356/2000], Iter [1] Loss: 0.6975 Training Accuracy: 0.71852\n",
      "Epoch [357/2000], Iter [1] Loss: 0.6898 Training Accuracy: 0.73333\n",
      "Epoch [358/2000], Iter [1] Loss: 0.6840 Training Accuracy: 0.72593\n",
      "Epoch [359/2000], Iter [1] Loss: 0.7000 Training Accuracy: 0.72593\n",
      "Epoch [360/2000], Iter [1] Loss: 0.6884 Training Accuracy: 0.73333\n",
      "Epoch [361/2000], Iter [1] Loss: 0.6931 Training Accuracy: 0.71852\n",
      "Epoch [362/2000], Iter [1] Loss: 0.6843 Training Accuracy: 0.72593\n",
      "Epoch [363/2000], Iter [1] Loss: 0.6806 Training Accuracy: 0.74815\n",
      "Epoch [364/2000], Iter [1] Loss: 0.6828 Training Accuracy: 0.73333\n",
      "Epoch [365/2000], Iter [1] Loss: 0.6820 Training Accuracy: 0.71111\n",
      "Epoch [366/2000], Iter [1] Loss: 0.6857 Training Accuracy: 0.73333\n",
      "Epoch [367/2000], Iter [1] Loss: 0.6744 Training Accuracy: 0.74074\n",
      "Epoch [368/2000], Iter [1] Loss: 0.6802 Training Accuracy: 0.72593\n",
      "Epoch [369/2000], Iter [1] Loss: 0.6653 Training Accuracy: 0.73333\n",
      "Epoch [370/2000], Iter [1] Loss: 0.6721 Training Accuracy: 0.74074\n",
      "Epoch [371/2000], Iter [1] Loss: 0.6818 Training Accuracy: 0.73333\n",
      "Epoch [372/2000], Iter [1] Loss: 0.6706 Training Accuracy: 0.73333\n",
      "Epoch [373/2000], Iter [1] Loss: 0.6652 Training Accuracy: 0.73333\n",
      "Epoch [374/2000], Iter [1] Loss: 0.6656 Training Accuracy: 0.73333\n",
      "Epoch [375/2000], Iter [1] Loss: 0.6695 Training Accuracy: 0.74074\n",
      "Epoch [376/2000], Iter [1] Loss: 0.6651 Training Accuracy: 0.73333\n",
      "Epoch [377/2000], Iter [1] Loss: 0.6758 Training Accuracy: 0.74074\n",
      "Epoch [378/2000], Iter [1] Loss: 0.6617 Training Accuracy: 0.74815\n",
      "Epoch [379/2000], Iter [1] Loss: 0.6696 Training Accuracy: 0.74074\n",
      "Epoch [380/2000], Iter [1] Loss: 0.6606 Training Accuracy: 0.72593\n",
      "Epoch [381/2000], Iter [1] Loss: 0.6530 Training Accuracy: 0.74074\n",
      "Epoch [382/2000], Iter [1] Loss: 0.6715 Training Accuracy: 0.74815\n",
      "Epoch [383/2000], Iter [1] Loss: 0.6655 Training Accuracy: 0.74074\n",
      "Epoch [384/2000], Iter [1] Loss: 0.6509 Training Accuracy: 0.73333\n",
      "Epoch [385/2000], Iter [1] Loss: 0.6574 Training Accuracy: 0.74815\n",
      "Epoch [386/2000], Iter [1] Loss: 0.6537 Training Accuracy: 0.73333\n",
      "Epoch [387/2000], Iter [1] Loss: 0.6583 Training Accuracy: 0.73333\n",
      "Epoch [388/2000], Iter [1] Loss: 0.6516 Training Accuracy: 0.71852\n",
      "Epoch [389/2000], Iter [1] Loss: 0.6473 Training Accuracy: 0.74074\n",
      "Epoch [390/2000], Iter [1] Loss: 0.6507 Training Accuracy: 0.74815\n",
      "Epoch [391/2000], Iter [1] Loss: 0.6499 Training Accuracy: 0.72593\n",
      "Epoch [392/2000], Iter [1] Loss: 0.6478 Training Accuracy: 0.74074\n",
      "Epoch [393/2000], Iter [1] Loss: 0.6444 Training Accuracy: 0.74074\n",
      "Epoch [394/2000], Iter [1] Loss: 0.6529 Training Accuracy: 0.74074\n",
      "Epoch [395/2000], Iter [1] Loss: 0.6428 Training Accuracy: 0.74815\n",
      "Epoch [396/2000], Iter [1] Loss: 0.6449 Training Accuracy: 0.74815\n",
      "Epoch [397/2000], Iter [1] Loss: 0.6320 Training Accuracy: 0.74074\n",
      "Epoch [398/2000], Iter [1] Loss: 0.6438 Training Accuracy: 0.73333\n",
      "Epoch [399/2000], Iter [1] Loss: 0.6286 Training Accuracy: 0.73333\n",
      "Epoch [400/2000], Iter [1] Loss: 0.6404 Training Accuracy: 0.75556\n",
      "Epoch [401/2000], Iter [1] Loss: 0.6306 Training Accuracy: 0.73333\n",
      "Epoch [402/2000], Iter [1] Loss: 0.6361 Training Accuracy: 0.74815\n",
      "Epoch [403/2000], Iter [1] Loss: 0.6312 Training Accuracy: 0.75556\n",
      "Epoch [404/2000], Iter [1] Loss: 0.6251 Training Accuracy: 0.74074\n",
      "Epoch [405/2000], Iter [1] Loss: 0.6323 Training Accuracy: 0.75556\n",
      "Epoch [406/2000], Iter [1] Loss: 0.6279 Training Accuracy: 0.74074\n",
      "Epoch [407/2000], Iter [1] Loss: 0.6346 Training Accuracy: 0.73333\n",
      "Epoch [408/2000], Iter [1] Loss: 0.6313 Training Accuracy: 0.76296\n",
      "Epoch [409/2000], Iter [1] Loss: 0.6260 Training Accuracy: 0.74074\n",
      "Epoch [410/2000], Iter [1] Loss: 0.6206 Training Accuracy: 0.75556\n",
      "Epoch [411/2000], Iter [1] Loss: 0.6253 Training Accuracy: 0.74074\n",
      "Epoch [412/2000], Iter [1] Loss: 0.6208 Training Accuracy: 0.73333\n",
      "Epoch [413/2000], Iter [1] Loss: 0.6114 Training Accuracy: 0.73333\n",
      "Epoch [414/2000], Iter [1] Loss: 0.6297 Training Accuracy: 0.75556\n",
      "Epoch [415/2000], Iter [1] Loss: 0.6175 Training Accuracy: 0.74815\n",
      "Epoch [416/2000], Iter [1] Loss: 0.6178 Training Accuracy: 0.74074\n",
      "Epoch [417/2000], Iter [1] Loss: 0.6228 Training Accuracy: 0.75556\n",
      "Epoch [418/2000], Iter [1] Loss: 0.6123 Training Accuracy: 0.75556\n",
      "Epoch [419/2000], Iter [1] Loss: 0.6160 Training Accuracy: 0.76296\n",
      "Epoch [420/2000], Iter [1] Loss: 0.6205 Training Accuracy: 0.76296\n",
      "Epoch [421/2000], Iter [1] Loss: 0.6069 Training Accuracy: 0.76296\n",
      "Epoch [422/2000], Iter [1] Loss: 0.6197 Training Accuracy: 0.75556\n",
      "Epoch [423/2000], Iter [1] Loss: 0.6170 Training Accuracy: 0.74815\n",
      "Epoch [424/2000], Iter [1] Loss: 0.6147 Training Accuracy: 0.75556\n",
      "Epoch [425/2000], Iter [1] Loss: 0.6029 Training Accuracy: 0.76296\n",
      "Epoch [426/2000], Iter [1] Loss: 0.6209 Training Accuracy: 0.75556\n",
      "Epoch [427/2000], Iter [1] Loss: 0.6131 Training Accuracy: 0.75556\n",
      "Epoch [428/2000], Iter [1] Loss: 0.6014 Training Accuracy: 0.76296\n",
      "Epoch [429/2000], Iter [1] Loss: 0.6083 Training Accuracy: 0.75556\n",
      "Epoch [430/2000], Iter [1] Loss: 0.6128 Training Accuracy: 0.76296\n",
      "Epoch [431/2000], Iter [1] Loss: 0.5967 Training Accuracy: 0.74815\n",
      "Epoch [432/2000], Iter [1] Loss: 0.6102 Training Accuracy: 0.76296\n",
      "Epoch [433/2000], Iter [1] Loss: 0.6099 Training Accuracy: 0.75556\n",
      "Epoch [434/2000], Iter [1] Loss: 0.6050 Training Accuracy: 0.76296\n",
      "Epoch [435/2000], Iter [1] Loss: 0.6075 Training Accuracy: 0.75556\n",
      "Epoch [436/2000], Iter [1] Loss: 0.5921 Training Accuracy: 0.75556\n",
      "Epoch [437/2000], Iter [1] Loss: 0.6027 Training Accuracy: 0.76296\n",
      "Epoch [438/2000], Iter [1] Loss: 0.6059 Training Accuracy: 0.77037\n",
      "Epoch [439/2000], Iter [1] Loss: 0.5949 Training Accuracy: 0.75556\n",
      "Epoch [440/2000], Iter [1] Loss: 0.5880 Training Accuracy: 0.76296\n",
      "Epoch [441/2000], Iter [1] Loss: 0.6012 Training Accuracy: 0.76296\n",
      "Epoch [442/2000], Iter [1] Loss: 0.5871 Training Accuracy: 0.75556\n",
      "Epoch [443/2000], Iter [1] Loss: 0.5998 Training Accuracy: 0.77778\n",
      "Epoch [444/2000], Iter [1] Loss: 0.5878 Training Accuracy: 0.77037\n",
      "Epoch [445/2000], Iter [1] Loss: 0.5938 Training Accuracy: 0.76296\n",
      "Epoch [446/2000], Iter [1] Loss: 0.5931 Training Accuracy: 0.76296\n",
      "Epoch [447/2000], Iter [1] Loss: 0.5952 Training Accuracy: 0.76296\n",
      "Epoch [448/2000], Iter [1] Loss: 0.5896 Training Accuracy: 0.74074\n",
      "Epoch [449/2000], Iter [1] Loss: 0.5847 Training Accuracy: 0.75556\n",
      "Epoch [450/2000], Iter [1] Loss: 0.5825 Training Accuracy: 0.75556\n",
      "Epoch [451/2000], Iter [1] Loss: 0.5933 Training Accuracy: 0.77778\n",
      "Epoch [452/2000], Iter [1] Loss: 0.5837 Training Accuracy: 0.77037\n",
      "Epoch [453/2000], Iter [1] Loss: 0.5846 Training Accuracy: 0.77037\n",
      "Epoch [454/2000], Iter [1] Loss: 0.5859 Training Accuracy: 0.76296\n",
      "Epoch [455/2000], Iter [1] Loss: 0.5859 Training Accuracy: 0.75556\n",
      "Epoch [456/2000], Iter [1] Loss: 0.5821 Training Accuracy: 0.76296\n",
      "Epoch [457/2000], Iter [1] Loss: 0.5857 Training Accuracy: 0.77778\n",
      "Epoch [458/2000], Iter [1] Loss: 0.5839 Training Accuracy: 0.74815\n",
      "Epoch [459/2000], Iter [1] Loss: 0.5763 Training Accuracy: 0.76296\n",
      "Epoch [460/2000], Iter [1] Loss: 0.5803 Training Accuracy: 0.76296\n",
      "Epoch [461/2000], Iter [1] Loss: 0.5785 Training Accuracy: 0.77037\n",
      "Epoch [462/2000], Iter [1] Loss: 0.5818 Training Accuracy: 0.75556\n",
      "Epoch [463/2000], Iter [1] Loss: 0.5784 Training Accuracy: 0.76296\n",
      "Epoch [464/2000], Iter [1] Loss: 0.5798 Training Accuracy: 0.76296\n",
      "Epoch [465/2000], Iter [1] Loss: 0.5759 Training Accuracy: 0.75556\n",
      "Epoch [466/2000], Iter [1] Loss: 0.5904 Training Accuracy: 0.77037\n",
      "Epoch [467/2000], Iter [1] Loss: 0.5752 Training Accuracy: 0.77037\n",
      "Epoch [468/2000], Iter [1] Loss: 0.5784 Training Accuracy: 0.76296\n",
      "Epoch [469/2000], Iter [1] Loss: 0.5689 Training Accuracy: 0.78519\n",
      "Epoch [470/2000], Iter [1] Loss: 0.5786 Training Accuracy: 0.77778\n",
      "Epoch [471/2000], Iter [1] Loss: 0.5643 Training Accuracy: 0.77778\n",
      "Epoch [472/2000], Iter [1] Loss: 0.5689 Training Accuracy: 0.77037\n",
      "Epoch [473/2000], Iter [1] Loss: 0.5745 Training Accuracy: 0.76296\n",
      "Epoch [474/2000], Iter [1] Loss: 0.5719 Training Accuracy: 0.77037\n",
      "Epoch [475/2000], Iter [1] Loss: 0.5567 Training Accuracy: 0.77037\n",
      "Epoch [476/2000], Iter [1] Loss: 0.5676 Training Accuracy: 0.77778\n",
      "Epoch [477/2000], Iter [1] Loss: 0.5651 Training Accuracy: 0.76296\n",
      "Epoch [478/2000], Iter [1] Loss: 0.5561 Training Accuracy: 0.77037\n",
      "Epoch [479/2000], Iter [1] Loss: 0.5769 Training Accuracy: 0.77778\n",
      "Epoch [480/2000], Iter [1] Loss: 0.5708 Training Accuracy: 0.76296\n",
      "Epoch [481/2000], Iter [1] Loss: 0.5707 Training Accuracy: 0.75556\n",
      "Epoch [482/2000], Iter [1] Loss: 0.5690 Training Accuracy: 0.77037\n",
      "Epoch [483/2000], Iter [1] Loss: 0.5707 Training Accuracy: 0.77778\n",
      "Epoch [484/2000], Iter [1] Loss: 0.5567 Training Accuracy: 0.79259\n",
      "Epoch [485/2000], Iter [1] Loss: 0.5704 Training Accuracy: 0.75556\n",
      "Epoch [486/2000], Iter [1] Loss: 0.5600 Training Accuracy: 0.78519\n",
      "Epoch [487/2000], Iter [1] Loss: 0.5687 Training Accuracy: 0.77778\n",
      "Epoch [488/2000], Iter [1] Loss: 0.5535 Training Accuracy: 0.78519\n",
      "Epoch [489/2000], Iter [1] Loss: 0.5482 Training Accuracy: 0.77778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [490/2000], Iter [1] Loss: 0.5527 Training Accuracy: 0.78519\n",
      "Epoch [491/2000], Iter [1] Loss: 0.5649 Training Accuracy: 0.78519\n",
      "Epoch [492/2000], Iter [1] Loss: 0.5549 Training Accuracy: 0.77778\n",
      "Epoch [493/2000], Iter [1] Loss: 0.5571 Training Accuracy: 0.78519\n",
      "Epoch [494/2000], Iter [1] Loss: 0.5541 Training Accuracy: 0.79259\n",
      "Epoch [495/2000], Iter [1] Loss: 0.5528 Training Accuracy: 0.77778\n",
      "Epoch [496/2000], Iter [1] Loss: 0.5545 Training Accuracy: 0.78519\n",
      "Epoch [497/2000], Iter [1] Loss: 0.5529 Training Accuracy: 0.77778\n",
      "Epoch [498/2000], Iter [1] Loss: 0.5507 Training Accuracy: 0.78519\n",
      "Epoch [499/2000], Iter [1] Loss: 0.5547 Training Accuracy: 0.77778\n",
      "Epoch [500/2000], Iter [1] Loss: 0.5490 Training Accuracy: 0.77778\n",
      "Epoch [501/2000], Iter [1] Loss: 0.5591 Training Accuracy: 0.77037\n",
      "Epoch [502/2000], Iter [1] Loss: 0.5432 Training Accuracy: 0.77778\n",
      "Epoch [503/2000], Iter [1] Loss: 0.5616 Training Accuracy: 0.78519\n",
      "Epoch [504/2000], Iter [1] Loss: 0.5446 Training Accuracy: 0.78519\n",
      "Epoch [505/2000], Iter [1] Loss: 0.5453 Training Accuracy: 0.77778\n",
      "Epoch [506/2000], Iter [1] Loss: 0.5515 Training Accuracy: 0.77037\n",
      "Epoch [507/2000], Iter [1] Loss: 0.5465 Training Accuracy: 0.77778\n",
      "Epoch [508/2000], Iter [1] Loss: 0.5542 Training Accuracy: 0.77778\n",
      "Epoch [509/2000], Iter [1] Loss: 0.5479 Training Accuracy: 0.77778\n",
      "Epoch [510/2000], Iter [1] Loss: 0.5391 Training Accuracy: 0.77778\n",
      "Epoch [511/2000], Iter [1] Loss: 0.5383 Training Accuracy: 0.77037\n",
      "Epoch [512/2000], Iter [1] Loss: 0.5476 Training Accuracy: 0.77778\n",
      "Epoch [513/2000], Iter [1] Loss: 0.5444 Training Accuracy: 0.78519\n",
      "Epoch [514/2000], Iter [1] Loss: 0.5316 Training Accuracy: 0.77778\n",
      "Epoch [515/2000], Iter [1] Loss: 0.5423 Training Accuracy: 0.78519\n",
      "Epoch [516/2000], Iter [1] Loss: 0.5398 Training Accuracy: 0.77778\n",
      "Epoch [517/2000], Iter [1] Loss: 0.5360 Training Accuracy: 0.79259\n",
      "Epoch [518/2000], Iter [1] Loss: 0.5484 Training Accuracy: 0.77778\n",
      "Epoch [519/2000], Iter [1] Loss: 0.5426 Training Accuracy: 0.77037\n",
      "Epoch [520/2000], Iter [1] Loss: 0.5395 Training Accuracy: 0.79259\n",
      "Epoch [521/2000], Iter [1] Loss: 0.5411 Training Accuracy: 0.77778\n",
      "Epoch [522/2000], Iter [1] Loss: 0.5380 Training Accuracy: 0.77778\n",
      "Epoch [523/2000], Iter [1] Loss: 0.5377 Training Accuracy: 0.79259\n",
      "Epoch [524/2000], Iter [1] Loss: 0.5362 Training Accuracy: 0.77037\n",
      "Epoch [525/2000], Iter [1] Loss: 0.5364 Training Accuracy: 0.77778\n",
      "Epoch [526/2000], Iter [1] Loss: 0.5283 Training Accuracy: 0.79259\n",
      "Epoch [527/2000], Iter [1] Loss: 0.5394 Training Accuracy: 0.77037\n",
      "Epoch [528/2000], Iter [1] Loss: 0.5262 Training Accuracy: 0.77037\n",
      "Epoch [529/2000], Iter [1] Loss: 0.5325 Training Accuracy: 0.77037\n",
      "Epoch [530/2000], Iter [1] Loss: 0.5435 Training Accuracy: 0.77037\n",
      "Epoch [531/2000], Iter [1] Loss: 0.5298 Training Accuracy: 0.79259\n",
      "Epoch [532/2000], Iter [1] Loss: 0.5297 Training Accuracy: 0.77778\n",
      "Epoch [533/2000], Iter [1] Loss: 0.5324 Training Accuracy: 0.77778\n",
      "Epoch [534/2000], Iter [1] Loss: 0.5244 Training Accuracy: 0.78519\n",
      "Epoch [535/2000], Iter [1] Loss: 0.5273 Training Accuracy: 0.77778\n",
      "Epoch [536/2000], Iter [1] Loss: 0.5283 Training Accuracy: 0.77778\n",
      "Epoch [537/2000], Iter [1] Loss: 0.5283 Training Accuracy: 0.78519\n",
      "Epoch [538/2000], Iter [1] Loss: 0.5420 Training Accuracy: 0.77037\n",
      "Epoch [539/2000], Iter [1] Loss: 0.5337 Training Accuracy: 0.77778\n",
      "Epoch [540/2000], Iter [1] Loss: 0.5290 Training Accuracy: 0.76296\n",
      "Epoch [541/2000], Iter [1] Loss: 0.5272 Training Accuracy: 0.77778\n",
      "Epoch [542/2000], Iter [1] Loss: 0.5289 Training Accuracy: 0.78519\n",
      "Epoch [543/2000], Iter [1] Loss: 0.5242 Training Accuracy: 0.78519\n",
      "Epoch [544/2000], Iter [1] Loss: 0.5252 Training Accuracy: 0.79259\n",
      "Epoch [545/2000], Iter [1] Loss: 0.5202 Training Accuracy: 0.79259\n",
      "Epoch [546/2000], Iter [1] Loss: 0.5260 Training Accuracy: 0.79259\n",
      "Epoch [547/2000], Iter [1] Loss: 0.5249 Training Accuracy: 0.78519\n",
      "Epoch [548/2000], Iter [1] Loss: 0.5206 Training Accuracy: 0.77037\n",
      "Epoch [549/2000], Iter [1] Loss: 0.5265 Training Accuracy: 0.77778\n",
      "Epoch [550/2000], Iter [1] Loss: 0.5141 Training Accuracy: 0.80000\n",
      "Epoch [551/2000], Iter [1] Loss: 0.5172 Training Accuracy: 0.80000\n",
      "Epoch [552/2000], Iter [1] Loss: 0.5008 Training Accuracy: 0.78519\n",
      "Epoch [553/2000], Iter [1] Loss: 0.5159 Training Accuracy: 0.78519\n",
      "Epoch [554/2000], Iter [1] Loss: 0.5094 Training Accuracy: 0.77778\n",
      "Epoch [555/2000], Iter [1] Loss: 0.5187 Training Accuracy: 0.79259\n",
      "Epoch [556/2000], Iter [1] Loss: 0.5216 Training Accuracy: 0.78519\n",
      "Epoch [557/2000], Iter [1] Loss: 0.5123 Training Accuracy: 0.79259\n",
      "Epoch [558/2000], Iter [1] Loss: 0.5082 Training Accuracy: 0.78519\n",
      "Epoch [559/2000], Iter [1] Loss: 0.5049 Training Accuracy: 0.78519\n",
      "Epoch [560/2000], Iter [1] Loss: 0.5150 Training Accuracy: 0.79259\n",
      "Epoch [561/2000], Iter [1] Loss: 0.5282 Training Accuracy: 0.79259\n",
      "Epoch [562/2000], Iter [1] Loss: 0.5162 Training Accuracy: 0.80000\n",
      "Epoch [563/2000], Iter [1] Loss: 0.5071 Training Accuracy: 0.77778\n",
      "Epoch [564/2000], Iter [1] Loss: 0.5030 Training Accuracy: 0.79259\n",
      "Epoch [565/2000], Iter [1] Loss: 0.5053 Training Accuracy: 0.78519\n",
      "Epoch [566/2000], Iter [1] Loss: 0.5154 Training Accuracy: 0.79259\n",
      "Epoch [567/2000], Iter [1] Loss: 0.5190 Training Accuracy: 0.78519\n",
      "Epoch [568/2000], Iter [1] Loss: 0.5018 Training Accuracy: 0.79259\n",
      "Epoch [569/2000], Iter [1] Loss: 0.5074 Training Accuracy: 0.78519\n",
      "Epoch [570/2000], Iter [1] Loss: 0.5110 Training Accuracy: 0.78519\n",
      "Epoch [571/2000], Iter [1] Loss: 0.5181 Training Accuracy: 0.78519\n",
      "Epoch [572/2000], Iter [1] Loss: 0.5230 Training Accuracy: 0.80741\n",
      "Epoch [573/2000], Iter [1] Loss: 0.4975 Training Accuracy: 0.78519\n",
      "Epoch [574/2000], Iter [1] Loss: 0.5003 Training Accuracy: 0.79259\n",
      "Epoch [575/2000], Iter [1] Loss: 0.5199 Training Accuracy: 0.79259\n",
      "Epoch [576/2000], Iter [1] Loss: 0.5031 Training Accuracy: 0.77778\n",
      "Epoch [577/2000], Iter [1] Loss: 0.5040 Training Accuracy: 0.80000\n",
      "Epoch [578/2000], Iter [1] Loss: 0.5080 Training Accuracy: 0.79259\n",
      "Epoch [579/2000], Iter [1] Loss: 0.4997 Training Accuracy: 0.79259\n",
      "Epoch [580/2000], Iter [1] Loss: 0.5070 Training Accuracy: 0.79259\n",
      "Epoch [581/2000], Iter [1] Loss: 0.4999 Training Accuracy: 0.79259\n",
      "Epoch [582/2000], Iter [1] Loss: 0.5023 Training Accuracy: 0.79259\n",
      "Epoch [583/2000], Iter [1] Loss: 0.5062 Training Accuracy: 0.78519\n",
      "Epoch [584/2000], Iter [1] Loss: 0.4957 Training Accuracy: 0.79259\n",
      "Epoch [585/2000], Iter [1] Loss: 0.4901 Training Accuracy: 0.79259\n",
      "Epoch [586/2000], Iter [1] Loss: 0.4948 Training Accuracy: 0.79259\n",
      "Epoch [587/2000], Iter [1] Loss: 0.5010 Training Accuracy: 0.78519\n",
      "Epoch [588/2000], Iter [1] Loss: 0.4991 Training Accuracy: 0.78519\n",
      "Epoch [589/2000], Iter [1] Loss: 0.4896 Training Accuracy: 0.77778\n",
      "Epoch [590/2000], Iter [1] Loss: 0.5041 Training Accuracy: 0.80000\n",
      "Epoch [591/2000], Iter [1] Loss: 0.4827 Training Accuracy: 0.78519\n",
      "Epoch [592/2000], Iter [1] Loss: 0.4940 Training Accuracy: 0.80000\n",
      "Epoch [593/2000], Iter [1] Loss: 0.4912 Training Accuracy: 0.78519\n",
      "Epoch [594/2000], Iter [1] Loss: 0.4955 Training Accuracy: 0.79259\n",
      "Epoch [595/2000], Iter [1] Loss: 0.4986 Training Accuracy: 0.80000\n",
      "Epoch [596/2000], Iter [1] Loss: 0.4959 Training Accuracy: 0.78519\n",
      "Epoch [597/2000], Iter [1] Loss: 0.5001 Training Accuracy: 0.79259\n",
      "Epoch [598/2000], Iter [1] Loss: 0.5005 Training Accuracy: 0.77778\n",
      "Epoch [599/2000], Iter [1] Loss: 0.4880 Training Accuracy: 0.79259\n",
      "Epoch [600/2000], Iter [1] Loss: 0.4909 Training Accuracy: 0.79259\n",
      "Epoch [601/2000], Iter [1] Loss: 0.4899 Training Accuracy: 0.80000\n",
      "Epoch [602/2000], Iter [1] Loss: 0.4948 Training Accuracy: 0.77778\n",
      "Epoch [603/2000], Iter [1] Loss: 0.4967 Training Accuracy: 0.80000\n",
      "Epoch [604/2000], Iter [1] Loss: 0.4901 Training Accuracy: 0.79259\n",
      "Epoch [605/2000], Iter [1] Loss: 0.5086 Training Accuracy: 0.80000\n",
      "Epoch [606/2000], Iter [1] Loss: 0.4920 Training Accuracy: 0.78519\n",
      "Epoch [607/2000], Iter [1] Loss: 0.4971 Training Accuracy: 0.79259\n",
      "Epoch [608/2000], Iter [1] Loss: 0.4937 Training Accuracy: 0.80000\n",
      "Epoch [609/2000], Iter [1] Loss: 0.4895 Training Accuracy: 0.80000\n",
      "Epoch [610/2000], Iter [1] Loss: 0.4917 Training Accuracy: 0.79259\n",
      "Epoch [611/2000], Iter [1] Loss: 0.4826 Training Accuracy: 0.80000\n",
      "Epoch [612/2000], Iter [1] Loss: 0.4988 Training Accuracy: 0.79259\n",
      "Epoch [613/2000], Iter [1] Loss: 0.4831 Training Accuracy: 0.78519\n",
      "Epoch [614/2000], Iter [1] Loss: 0.4866 Training Accuracy: 0.79259\n",
      "Epoch [615/2000], Iter [1] Loss: 0.4839 Training Accuracy: 0.79259\n",
      "Epoch [616/2000], Iter [1] Loss: 0.4846 Training Accuracy: 0.79259\n",
      "Epoch [617/2000], Iter [1] Loss: 0.4843 Training Accuracy: 0.78519\n",
      "Epoch [618/2000], Iter [1] Loss: 0.5014 Training Accuracy: 0.81481\n",
      "Epoch [619/2000], Iter [1] Loss: 0.4876 Training Accuracy: 0.79259\n",
      "Epoch [620/2000], Iter [1] Loss: 0.4822 Training Accuracy: 0.77778\n",
      "Epoch [621/2000], Iter [1] Loss: 0.4894 Training Accuracy: 0.80000\n",
      "Epoch [622/2000], Iter [1] Loss: 0.4812 Training Accuracy: 0.78519\n",
      "Epoch [623/2000], Iter [1] Loss: 0.4774 Training Accuracy: 0.79259\n",
      "Epoch [624/2000], Iter [1] Loss: 0.4791 Training Accuracy: 0.80741\n",
      "Epoch [625/2000], Iter [1] Loss: 0.4831 Training Accuracy: 0.80000\n",
      "Epoch [626/2000], Iter [1] Loss: 0.4766 Training Accuracy: 0.80000\n",
      "Epoch [627/2000], Iter [1] Loss: 0.4834 Training Accuracy: 0.79259\n",
      "Epoch [628/2000], Iter [1] Loss: 0.4886 Training Accuracy: 0.78519\n",
      "Epoch [629/2000], Iter [1] Loss: 0.4864 Training Accuracy: 0.79259\n",
      "Epoch [630/2000], Iter [1] Loss: 0.4687 Training Accuracy: 0.79259\n",
      "Epoch [631/2000], Iter [1] Loss: 0.4883 Training Accuracy: 0.80000\n",
      "Epoch [632/2000], Iter [1] Loss: 0.4723 Training Accuracy: 0.77778\n",
      "Epoch [633/2000], Iter [1] Loss: 0.4738 Training Accuracy: 0.79259\n",
      "Epoch [634/2000], Iter [1] Loss: 0.4810 Training Accuracy: 0.79259\n",
      "Epoch [635/2000], Iter [1] Loss: 0.4732 Training Accuracy: 0.80000\n",
      "Epoch [636/2000], Iter [1] Loss: 0.4809 Training Accuracy: 0.80000\n",
      "Epoch [637/2000], Iter [1] Loss: 0.4829 Training Accuracy: 0.79259\n",
      "Epoch [638/2000], Iter [1] Loss: 0.4818 Training Accuracy: 0.79259\n",
      "Epoch [639/2000], Iter [1] Loss: 0.4742 Training Accuracy: 0.79259\n",
      "Epoch [640/2000], Iter [1] Loss: 0.4793 Training Accuracy: 0.80000\n",
      "Epoch [641/2000], Iter [1] Loss: 0.4721 Training Accuracy: 0.77778\n",
      "Epoch [642/2000], Iter [1] Loss: 0.4767 Training Accuracy: 0.80741\n",
      "Epoch [643/2000], Iter [1] Loss: 0.4709 Training Accuracy: 0.80741\n",
      "Epoch [644/2000], Iter [1] Loss: 0.4879 Training Accuracy: 0.80000\n",
      "Epoch [645/2000], Iter [1] Loss: 0.4769 Training Accuracy: 0.80000\n",
      "Epoch [646/2000], Iter [1] Loss: 0.4573 Training Accuracy: 0.79259\n",
      "Epoch [647/2000], Iter [1] Loss: 0.4653 Training Accuracy: 0.79259\n",
      "Epoch [648/2000], Iter [1] Loss: 0.4785 Training Accuracy: 0.80741\n",
      "Epoch [649/2000], Iter [1] Loss: 0.4660 Training Accuracy: 0.79259\n",
      "Epoch [650/2000], Iter [1] Loss: 0.4729 Training Accuracy: 0.78519\n",
      "Epoch [651/2000], Iter [1] Loss: 0.4749 Training Accuracy: 0.80741\n",
      "Epoch [652/2000], Iter [1] Loss: 0.4767 Training Accuracy: 0.80000\n",
      "Epoch [653/2000], Iter [1] Loss: 0.4715 Training Accuracy: 0.80000\n",
      "Epoch [654/2000], Iter [1] Loss: 0.4706 Training Accuracy: 0.80000\n",
      "Epoch [655/2000], Iter [1] Loss: 0.4820 Training Accuracy: 0.79259\n",
      "Epoch [656/2000], Iter [1] Loss: 0.4717 Training Accuracy: 0.79259\n",
      "Epoch [657/2000], Iter [1] Loss: 0.4709 Training Accuracy: 0.80741\n",
      "Epoch [658/2000], Iter [1] Loss: 0.4653 Training Accuracy: 0.80741\n",
      "Epoch [659/2000], Iter [1] Loss: 0.4556 Training Accuracy: 0.80000\n",
      "Epoch [660/2000], Iter [1] Loss: 0.4676 Training Accuracy: 0.80741\n",
      "Epoch [661/2000], Iter [1] Loss: 0.4572 Training Accuracy: 0.80000\n",
      "Epoch [662/2000], Iter [1] Loss: 0.4707 Training Accuracy: 0.78519\n",
      "Epoch [663/2000], Iter [1] Loss: 0.4512 Training Accuracy: 0.80000\n",
      "Epoch [664/2000], Iter [1] Loss: 0.4740 Training Accuracy: 0.80000\n",
      "Epoch [665/2000], Iter [1] Loss: 0.4720 Training Accuracy: 0.79259\n",
      "Epoch [666/2000], Iter [1] Loss: 0.4740 Training Accuracy: 0.79259\n",
      "Epoch [667/2000], Iter [1] Loss: 0.4662 Training Accuracy: 0.80741\n",
      "Epoch [668/2000], Iter [1] Loss: 0.4745 Training Accuracy: 0.81481\n",
      "Epoch [669/2000], Iter [1] Loss: 0.4651 Training Accuracy: 0.81481\n",
      "Epoch [670/2000], Iter [1] Loss: 0.4747 Training Accuracy: 0.79259\n",
      "Epoch [671/2000], Iter [1] Loss: 0.4631 Training Accuracy: 0.80741\n",
      "Epoch [672/2000], Iter [1] Loss: 0.4640 Training Accuracy: 0.80741\n",
      "Epoch [673/2000], Iter [1] Loss: 0.4708 Training Accuracy: 0.80741\n",
      "Epoch [674/2000], Iter [1] Loss: 0.4550 Training Accuracy: 0.80000\n",
      "Epoch [675/2000], Iter [1] Loss: 0.4595 Training Accuracy: 0.80741\n",
      "Epoch [676/2000], Iter [1] Loss: 0.4617 Training Accuracy: 0.80000\n",
      "Epoch [677/2000], Iter [1] Loss: 0.4663 Training Accuracy: 0.80000\n",
      "Epoch [678/2000], Iter [1] Loss: 0.4710 Training Accuracy: 0.80741\n",
      "Epoch [679/2000], Iter [1] Loss: 0.4604 Training Accuracy: 0.80000\n",
      "Epoch [680/2000], Iter [1] Loss: 0.4530 Training Accuracy: 0.80741\n",
      "Epoch [681/2000], Iter [1] Loss: 0.4644 Training Accuracy: 0.80000\n",
      "Epoch [682/2000], Iter [1] Loss: 0.4681 Training Accuracy: 0.80741\n",
      "Epoch [683/2000], Iter [1] Loss: 0.4569 Training Accuracy: 0.82222\n",
      "Epoch [684/2000], Iter [1] Loss: 0.4564 Training Accuracy: 0.80000\n",
      "Epoch [685/2000], Iter [1] Loss: 0.4533 Training Accuracy: 0.81481\n",
      "Epoch [686/2000], Iter [1] Loss: 0.4631 Training Accuracy: 0.79259\n",
      "Epoch [687/2000], Iter [1] Loss: 0.4599 Training Accuracy: 0.81481\n",
      "Epoch [688/2000], Iter [1] Loss: 0.4529 Training Accuracy: 0.80000\n",
      "Epoch [689/2000], Iter [1] Loss: 0.4607 Training Accuracy: 0.80741\n",
      "Epoch [690/2000], Iter [1] Loss: 0.4615 Training Accuracy: 0.80000\n",
      "Epoch [691/2000], Iter [1] Loss: 0.4479 Training Accuracy: 0.80741\n",
      "Epoch [692/2000], Iter [1] Loss: 0.4632 Training Accuracy: 0.80741\n",
      "Epoch [693/2000], Iter [1] Loss: 0.4435 Training Accuracy: 0.80741\n",
      "Epoch [694/2000], Iter [1] Loss: 0.4695 Training Accuracy: 0.80000\n",
      "Epoch [695/2000], Iter [1] Loss: 0.4605 Training Accuracy: 0.80000\n",
      "Epoch [696/2000], Iter [1] Loss: 0.4596 Training Accuracy: 0.81481\n",
      "Epoch [697/2000], Iter [1] Loss: 0.4520 Training Accuracy: 0.80741\n",
      "Epoch [698/2000], Iter [1] Loss: 0.4392 Training Accuracy: 0.80741\n",
      "Epoch [699/2000], Iter [1] Loss: 0.4575 Training Accuracy: 0.80741\n",
      "Epoch [700/2000], Iter [1] Loss: 0.4609 Training Accuracy: 0.80741\n",
      "Epoch [701/2000], Iter [1] Loss: 0.4551 Training Accuracy: 0.80741\n",
      "Epoch [702/2000], Iter [1] Loss: 0.4487 Training Accuracy: 0.81481\n",
      "Epoch [703/2000], Iter [1] Loss: 0.4662 Training Accuracy: 0.80741\n",
      "Epoch [704/2000], Iter [1] Loss: 0.4444 Training Accuracy: 0.79259\n",
      "Epoch [705/2000], Iter [1] Loss: 0.4520 Training Accuracy: 0.81481\n",
      "Epoch [706/2000], Iter [1] Loss: 0.4431 Training Accuracy: 0.79259\n",
      "Epoch [707/2000], Iter [1] Loss: 0.4494 Training Accuracy: 0.81481\n",
      "Epoch [708/2000], Iter [1] Loss: 0.4541 Training Accuracy: 0.80741\n",
      "Epoch [709/2000], Iter [1] Loss: 0.4513 Training Accuracy: 0.80000\n",
      "Epoch [710/2000], Iter [1] Loss: 0.4573 Training Accuracy: 0.80741\n",
      "Epoch [711/2000], Iter [1] Loss: 0.4419 Training Accuracy: 0.80000\n",
      "Epoch [712/2000], Iter [1] Loss: 0.4589 Training Accuracy: 0.80741\n",
      "Epoch [713/2000], Iter [1] Loss: 0.4462 Training Accuracy: 0.81481\n",
      "Epoch [714/2000], Iter [1] Loss: 0.4450 Training Accuracy: 0.80000\n",
      "Epoch [715/2000], Iter [1] Loss: 0.4458 Training Accuracy: 0.80741\n",
      "Epoch [716/2000], Iter [1] Loss: 0.4546 Training Accuracy: 0.80741\n",
      "Epoch [717/2000], Iter [1] Loss: 0.4412 Training Accuracy: 0.80000\n",
      "Epoch [718/2000], Iter [1] Loss: 0.4516 Training Accuracy: 0.80000\n",
      "Epoch [719/2000], Iter [1] Loss: 0.4441 Training Accuracy: 0.79259\n",
      "Epoch [720/2000], Iter [1] Loss: 0.4535 Training Accuracy: 0.80741\n",
      "Epoch [721/2000], Iter [1] Loss: 0.4539 Training Accuracy: 0.81481\n",
      "Epoch [722/2000], Iter [1] Loss: 0.4541 Training Accuracy: 0.81481\n",
      "Epoch [723/2000], Iter [1] Loss: 0.4491 Training Accuracy: 0.81481\n",
      "Epoch [724/2000], Iter [1] Loss: 0.4537 Training Accuracy: 0.80000\n",
      "Epoch [725/2000], Iter [1] Loss: 0.4328 Training Accuracy: 0.80741\n",
      "Epoch [726/2000], Iter [1] Loss: 0.4442 Training Accuracy: 0.79259\n",
      "Epoch [727/2000], Iter [1] Loss: 0.4448 Training Accuracy: 0.80741\n",
      "Epoch [728/2000], Iter [1] Loss: 0.4420 Training Accuracy: 0.80000\n",
      "Epoch [729/2000], Iter [1] Loss: 0.4477 Training Accuracy: 0.80741\n",
      "Epoch [730/2000], Iter [1] Loss: 0.4466 Training Accuracy: 0.80000\n",
      "Epoch [731/2000], Iter [1] Loss: 0.4463 Training Accuracy: 0.81481\n",
      "Epoch [732/2000], Iter [1] Loss: 0.4495 Training Accuracy: 0.80000\n",
      "Epoch [733/2000], Iter [1] Loss: 0.4494 Training Accuracy: 0.81481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [734/2000], Iter [1] Loss: 0.4347 Training Accuracy: 0.80000\n",
      "Epoch [735/2000], Iter [1] Loss: 0.4497 Training Accuracy: 0.80741\n",
      "Epoch [736/2000], Iter [1] Loss: 0.4438 Training Accuracy: 0.80000\n",
      "Epoch [737/2000], Iter [1] Loss: 0.4338 Training Accuracy: 0.80741\n",
      "Epoch [738/2000], Iter [1] Loss: 0.4449 Training Accuracy: 0.80741\n",
      "Epoch [739/2000], Iter [1] Loss: 0.4516 Training Accuracy: 0.81481\n",
      "Epoch [740/2000], Iter [1] Loss: 0.4454 Training Accuracy: 0.79259\n",
      "Epoch [741/2000], Iter [1] Loss: 0.4293 Training Accuracy: 0.80741\n",
      "Epoch [742/2000], Iter [1] Loss: 0.4488 Training Accuracy: 0.81481\n",
      "Epoch [743/2000], Iter [1] Loss: 0.4426 Training Accuracy: 0.80000\n",
      "Epoch [744/2000], Iter [1] Loss: 0.4345 Training Accuracy: 0.81481\n",
      "Epoch [745/2000], Iter [1] Loss: 0.4435 Training Accuracy: 0.80741\n",
      "Epoch [746/2000], Iter [1] Loss: 0.4406 Training Accuracy: 0.80000\n",
      "Epoch [747/2000], Iter [1] Loss: 0.4276 Training Accuracy: 0.80741\n",
      "Epoch [748/2000], Iter [1] Loss: 0.4278 Training Accuracy: 0.79259\n",
      "Epoch [749/2000], Iter [1] Loss: 0.4439 Training Accuracy: 0.79259\n",
      "Epoch [750/2000], Iter [1] Loss: 0.4327 Training Accuracy: 0.80000\n",
      "Epoch [751/2000], Iter [1] Loss: 0.4503 Training Accuracy: 0.80741\n",
      "Epoch [752/2000], Iter [1] Loss: 0.4422 Training Accuracy: 0.80000\n",
      "Epoch [753/2000], Iter [1] Loss: 0.4412 Training Accuracy: 0.79259\n",
      "Epoch [754/2000], Iter [1] Loss: 0.4352 Training Accuracy: 0.80741\n",
      "Epoch [755/2000], Iter [1] Loss: 0.4287 Training Accuracy: 0.80741\n",
      "Epoch [756/2000], Iter [1] Loss: 0.4379 Training Accuracy: 0.80741\n",
      "Epoch [757/2000], Iter [1] Loss: 0.4348 Training Accuracy: 0.81481\n",
      "Epoch [758/2000], Iter [1] Loss: 0.4464 Training Accuracy: 0.80741\n",
      "Epoch [759/2000], Iter [1] Loss: 0.4375 Training Accuracy: 0.81481\n",
      "Epoch [760/2000], Iter [1] Loss: 0.4407 Training Accuracy: 0.80741\n",
      "Epoch [761/2000], Iter [1] Loss: 0.4380 Training Accuracy: 0.81481\n",
      "Epoch [762/2000], Iter [1] Loss: 0.4329 Training Accuracy: 0.80741\n",
      "Epoch [763/2000], Iter [1] Loss: 0.4468 Training Accuracy: 0.80000\n",
      "Epoch [764/2000], Iter [1] Loss: 0.4366 Training Accuracy: 0.81481\n",
      "Epoch [765/2000], Iter [1] Loss: 0.4315 Training Accuracy: 0.81481\n",
      "Epoch [766/2000], Iter [1] Loss: 0.4340 Training Accuracy: 0.80741\n",
      "Epoch [767/2000], Iter [1] Loss: 0.4246 Training Accuracy: 0.80741\n",
      "Epoch [768/2000], Iter [1] Loss: 0.4416 Training Accuracy: 0.80000\n",
      "Epoch [769/2000], Iter [1] Loss: 0.4348 Training Accuracy: 0.81481\n",
      "Epoch [770/2000], Iter [1] Loss: 0.4336 Training Accuracy: 0.80000\n",
      "Epoch [771/2000], Iter [1] Loss: 0.4360 Training Accuracy: 0.81481\n",
      "Epoch [772/2000], Iter [1] Loss: 0.4337 Training Accuracy: 0.80741\n",
      "Epoch [773/2000], Iter [1] Loss: 0.4402 Training Accuracy: 0.80741\n",
      "Epoch [774/2000], Iter [1] Loss: 0.4319 Training Accuracy: 0.81481\n",
      "Epoch [775/2000], Iter [1] Loss: 0.4330 Training Accuracy: 0.79259\n",
      "Epoch [776/2000], Iter [1] Loss: 0.4481 Training Accuracy: 0.80741\n",
      "Epoch [777/2000], Iter [1] Loss: 0.4341 Training Accuracy: 0.79259\n",
      "Epoch [778/2000], Iter [1] Loss: 0.4324 Training Accuracy: 0.80741\n",
      "Epoch [779/2000], Iter [1] Loss: 0.4311 Training Accuracy: 0.81481\n",
      "Epoch [780/2000], Iter [1] Loss: 0.4460 Training Accuracy: 0.80000\n",
      "Epoch [781/2000], Iter [1] Loss: 0.4309 Training Accuracy: 0.80000\n",
      "Epoch [782/2000], Iter [1] Loss: 0.4309 Training Accuracy: 0.81481\n",
      "Epoch [783/2000], Iter [1] Loss: 0.4242 Training Accuracy: 0.80741\n",
      "Epoch [784/2000], Iter [1] Loss: 0.4292 Training Accuracy: 0.80000\n",
      "Epoch [785/2000], Iter [1] Loss: 0.4088 Training Accuracy: 0.80741\n",
      "Epoch [786/2000], Iter [1] Loss: 0.4401 Training Accuracy: 0.80741\n",
      "Epoch [787/2000], Iter [1] Loss: 0.4286 Training Accuracy: 0.80741\n",
      "Epoch [788/2000], Iter [1] Loss: 0.4262 Training Accuracy: 0.80000\n",
      "Epoch [789/2000], Iter [1] Loss: 0.4307 Training Accuracy: 0.79259\n",
      "Epoch [790/2000], Iter [1] Loss: 0.4360 Training Accuracy: 0.80000\n",
      "Epoch [791/2000], Iter [1] Loss: 0.4368 Training Accuracy: 0.81481\n",
      "Epoch [792/2000], Iter [1] Loss: 0.4456 Training Accuracy: 0.80741\n",
      "Epoch [793/2000], Iter [1] Loss: 0.4225 Training Accuracy: 0.80741\n",
      "Epoch [794/2000], Iter [1] Loss: 0.4215 Training Accuracy: 0.80741\n",
      "Epoch [795/2000], Iter [1] Loss: 0.4222 Training Accuracy: 0.80741\n",
      "Epoch [796/2000], Iter [1] Loss: 0.4259 Training Accuracy: 0.80000\n",
      "Epoch [797/2000], Iter [1] Loss: 0.4304 Training Accuracy: 0.80741\n",
      "Epoch [798/2000], Iter [1] Loss: 0.4217 Training Accuracy: 0.81481\n",
      "Epoch [799/2000], Iter [1] Loss: 0.4346 Training Accuracy: 0.81481\n",
      "Epoch [800/2000], Iter [1] Loss: 0.4140 Training Accuracy: 0.80741\n",
      "Epoch [801/2000], Iter [1] Loss: 0.4383 Training Accuracy: 0.80741\n",
      "Epoch [802/2000], Iter [1] Loss: 0.4116 Training Accuracy: 0.80741\n",
      "Epoch [803/2000], Iter [1] Loss: 0.4300 Training Accuracy: 0.80741\n",
      "Epoch [804/2000], Iter [1] Loss: 0.4196 Training Accuracy: 0.81481\n",
      "Epoch [805/2000], Iter [1] Loss: 0.4167 Training Accuracy: 0.80000\n",
      "Epoch [806/2000], Iter [1] Loss: 0.4276 Training Accuracy: 0.80741\n",
      "Epoch [807/2000], Iter [1] Loss: 0.4255 Training Accuracy: 0.79259\n",
      "Epoch [808/2000], Iter [1] Loss: 0.4173 Training Accuracy: 0.79259\n",
      "Epoch [809/2000], Iter [1] Loss: 0.4262 Training Accuracy: 0.80741\n",
      "Epoch [810/2000], Iter [1] Loss: 0.4064 Training Accuracy: 0.81481\n",
      "Epoch [811/2000], Iter [1] Loss: 0.4237 Training Accuracy: 0.80741\n",
      "Epoch [812/2000], Iter [1] Loss: 0.4083 Training Accuracy: 0.80741\n",
      "Epoch [813/2000], Iter [1] Loss: 0.4288 Training Accuracy: 0.80000\n",
      "Epoch [814/2000], Iter [1] Loss: 0.4176 Training Accuracy: 0.80741\n",
      "Epoch [815/2000], Iter [1] Loss: 0.4181 Training Accuracy: 0.80741\n",
      "Epoch [816/2000], Iter [1] Loss: 0.4239 Training Accuracy: 0.80000\n",
      "Epoch [817/2000], Iter [1] Loss: 0.4288 Training Accuracy: 0.80741\n",
      "Epoch [818/2000], Iter [1] Loss: 0.4229 Training Accuracy: 0.80741\n",
      "Epoch [819/2000], Iter [1] Loss: 0.4141 Training Accuracy: 0.80000\n",
      "Epoch [820/2000], Iter [1] Loss: 0.4146 Training Accuracy: 0.80741\n",
      "Epoch [821/2000], Iter [1] Loss: 0.4200 Training Accuracy: 0.80741\n",
      "Epoch [822/2000], Iter [1] Loss: 0.4234 Training Accuracy: 0.80000\n",
      "Epoch [823/2000], Iter [1] Loss: 0.4223 Training Accuracy: 0.81481\n",
      "Epoch [824/2000], Iter [1] Loss: 0.4149 Training Accuracy: 0.81481\n",
      "Epoch [825/2000], Iter [1] Loss: 0.4180 Training Accuracy: 0.80000\n",
      "Epoch [826/2000], Iter [1] Loss: 0.4119 Training Accuracy: 0.80000\n",
      "Epoch [827/2000], Iter [1] Loss: 0.4278 Training Accuracy: 0.80000\n",
      "Epoch [828/2000], Iter [1] Loss: 0.4136 Training Accuracy: 0.81481\n",
      "Epoch [829/2000], Iter [1] Loss: 0.4142 Training Accuracy: 0.80741\n",
      "Epoch [830/2000], Iter [1] Loss: 0.4336 Training Accuracy: 0.78519\n",
      "Epoch [831/2000], Iter [1] Loss: 0.4092 Training Accuracy: 0.81481\n",
      "Epoch [832/2000], Iter [1] Loss: 0.4254 Training Accuracy: 0.81481\n",
      "Epoch [833/2000], Iter [1] Loss: 0.4174 Training Accuracy: 0.80741\n",
      "Epoch [834/2000], Iter [1] Loss: 0.4229 Training Accuracy: 0.80000\n",
      "Epoch [835/2000], Iter [1] Loss: 0.4070 Training Accuracy: 0.81481\n",
      "Epoch [836/2000], Iter [1] Loss: 0.4186 Training Accuracy: 0.81481\n",
      "Epoch [837/2000], Iter [1] Loss: 0.4110 Training Accuracy: 0.81481\n",
      "Epoch [838/2000], Iter [1] Loss: 0.4318 Training Accuracy: 0.80741\n",
      "Epoch [839/2000], Iter [1] Loss: 0.4038 Training Accuracy: 0.80741\n",
      "Epoch [840/2000], Iter [1] Loss: 0.4256 Training Accuracy: 0.81481\n",
      "Epoch [841/2000], Iter [1] Loss: 0.4220 Training Accuracy: 0.81481\n",
      "Epoch [842/2000], Iter [1] Loss: 0.4130 Training Accuracy: 0.80000\n",
      "Epoch [843/2000], Iter [1] Loss: 0.4172 Training Accuracy: 0.81481\n",
      "Epoch [844/2000], Iter [1] Loss: 0.4111 Training Accuracy: 0.81481\n",
      "Epoch [845/2000], Iter [1] Loss: 0.4072 Training Accuracy: 0.79259\n",
      "Epoch [846/2000], Iter [1] Loss: 0.4170 Training Accuracy: 0.80000\n",
      "Epoch [847/2000], Iter [1] Loss: 0.4131 Training Accuracy: 0.81481\n",
      "Epoch [848/2000], Iter [1] Loss: 0.4111 Training Accuracy: 0.80741\n",
      "Epoch [849/2000], Iter [1] Loss: 0.4296 Training Accuracy: 0.81481\n",
      "Epoch [850/2000], Iter [1] Loss: 0.4013 Training Accuracy: 0.80000\n",
      "Epoch [851/2000], Iter [1] Loss: 0.4093 Training Accuracy: 0.79259\n",
      "Epoch [852/2000], Iter [1] Loss: 0.4102 Training Accuracy: 0.80741\n",
      "Epoch [853/2000], Iter [1] Loss: 0.4188 Training Accuracy: 0.80741\n",
      "Epoch [854/2000], Iter [1] Loss: 0.4194 Training Accuracy: 0.80741\n",
      "Epoch [855/2000], Iter [1] Loss: 0.4135 Training Accuracy: 0.81481\n",
      "Epoch [856/2000], Iter [1] Loss: 0.4115 Training Accuracy: 0.80000\n",
      "Epoch [857/2000], Iter [1] Loss: 0.4037 Training Accuracy: 0.81481\n",
      "Epoch [858/2000], Iter [1] Loss: 0.4100 Training Accuracy: 0.81481\n",
      "Epoch [859/2000], Iter [1] Loss: 0.4216 Training Accuracy: 0.80741\n",
      "Epoch [860/2000], Iter [1] Loss: 0.4108 Training Accuracy: 0.80000\n",
      "Epoch [861/2000], Iter [1] Loss: 0.4023 Training Accuracy: 0.81481\n",
      "Epoch [862/2000], Iter [1] Loss: 0.4219 Training Accuracy: 0.80741\n",
      "Epoch [863/2000], Iter [1] Loss: 0.4104 Training Accuracy: 0.80741\n",
      "Epoch [864/2000], Iter [1] Loss: 0.4082 Training Accuracy: 0.80741\n",
      "Epoch [865/2000], Iter [1] Loss: 0.4023 Training Accuracy: 0.80741\n",
      "Epoch [866/2000], Iter [1] Loss: 0.4081 Training Accuracy: 0.79259\n",
      "Epoch [867/2000], Iter [1] Loss: 0.4143 Training Accuracy: 0.80741\n",
      "Epoch [868/2000], Iter [1] Loss: 0.4126 Training Accuracy: 0.81481\n",
      "Epoch [869/2000], Iter [1] Loss: 0.4102 Training Accuracy: 0.81481\n",
      "Epoch [870/2000], Iter [1] Loss: 0.4090 Training Accuracy: 0.80741\n",
      "Epoch [871/2000], Iter [1] Loss: 0.4116 Training Accuracy: 0.80741\n",
      "Epoch [872/2000], Iter [1] Loss: 0.4048 Training Accuracy: 0.81481\n",
      "Epoch [873/2000], Iter [1] Loss: 0.4104 Training Accuracy: 0.81481\n",
      "Epoch [874/2000], Iter [1] Loss: 0.4233 Training Accuracy: 0.81481\n",
      "Epoch [875/2000], Iter [1] Loss: 0.4160 Training Accuracy: 0.81481\n",
      "Epoch [876/2000], Iter [1] Loss: 0.4199 Training Accuracy: 0.80000\n",
      "Epoch [877/2000], Iter [1] Loss: 0.4065 Training Accuracy: 0.81481\n",
      "Epoch [878/2000], Iter [1] Loss: 0.4098 Training Accuracy: 0.80741\n",
      "Epoch [879/2000], Iter [1] Loss: 0.4045 Training Accuracy: 0.80741\n",
      "Epoch [880/2000], Iter [1] Loss: 0.4127 Training Accuracy: 0.81481\n",
      "Epoch [881/2000], Iter [1] Loss: 0.3936 Training Accuracy: 0.80000\n",
      "Epoch [882/2000], Iter [1] Loss: 0.3966 Training Accuracy: 0.80000\n",
      "Epoch [883/2000], Iter [1] Loss: 0.4128 Training Accuracy: 0.80000\n",
      "Epoch [884/2000], Iter [1] Loss: 0.4074 Training Accuracy: 0.81481\n",
      "Epoch [885/2000], Iter [1] Loss: 0.3977 Training Accuracy: 0.80741\n",
      "Epoch [886/2000], Iter [1] Loss: 0.3926 Training Accuracy: 0.80741\n",
      "Epoch [887/2000], Iter [1] Loss: 0.3976 Training Accuracy: 0.80000\n",
      "Epoch [888/2000], Iter [1] Loss: 0.4024 Training Accuracy: 0.81481\n",
      "Epoch [889/2000], Iter [1] Loss: 0.4074 Training Accuracy: 0.81481\n",
      "Epoch [890/2000], Iter [1] Loss: 0.3932 Training Accuracy: 0.80741\n",
      "Epoch [891/2000], Iter [1] Loss: 0.4075 Training Accuracy: 0.80000\n",
      "Epoch [892/2000], Iter [1] Loss: 0.3997 Training Accuracy: 0.80741\n",
      "Epoch [893/2000], Iter [1] Loss: 0.4174 Training Accuracy: 0.80741\n",
      "Epoch [894/2000], Iter [1] Loss: 0.4058 Training Accuracy: 0.80000\n",
      "Epoch [895/2000], Iter [1] Loss: 0.4095 Training Accuracy: 0.80741\n",
      "Epoch [896/2000], Iter [1] Loss: 0.3942 Training Accuracy: 0.80000\n",
      "Epoch [897/2000], Iter [1] Loss: 0.4082 Training Accuracy: 0.80741\n",
      "Epoch [898/2000], Iter [1] Loss: 0.4199 Training Accuracy: 0.80000\n",
      "Epoch [899/2000], Iter [1] Loss: 0.4008 Training Accuracy: 0.81481\n",
      "Epoch [900/2000], Iter [1] Loss: 0.4053 Training Accuracy: 0.81481\n",
      "Epoch [901/2000], Iter [1] Loss: 0.4060 Training Accuracy: 0.80741\n",
      "Epoch [902/2000], Iter [1] Loss: 0.4026 Training Accuracy: 0.81481\n",
      "Epoch [903/2000], Iter [1] Loss: 0.4001 Training Accuracy: 0.80000\n",
      "Epoch [904/2000], Iter [1] Loss: 0.3900 Training Accuracy: 0.80000\n",
      "Epoch [905/2000], Iter [1] Loss: 0.4036 Training Accuracy: 0.81481\n",
      "Epoch [906/2000], Iter [1] Loss: 0.4024 Training Accuracy: 0.79259\n",
      "Epoch [907/2000], Iter [1] Loss: 0.4019 Training Accuracy: 0.80000\n",
      "Epoch [908/2000], Iter [1] Loss: 0.4069 Training Accuracy: 0.80741\n",
      "Epoch [909/2000], Iter [1] Loss: 0.4112 Training Accuracy: 0.80000\n",
      "Epoch [910/2000], Iter [1] Loss: 0.4010 Training Accuracy: 0.80741\n",
      "Epoch [911/2000], Iter [1] Loss: 0.4033 Training Accuracy: 0.80741\n",
      "Epoch [912/2000], Iter [1] Loss: 0.4043 Training Accuracy: 0.82222\n",
      "Epoch [913/2000], Iter [1] Loss: 0.3874 Training Accuracy: 0.81481\n",
      "Epoch [914/2000], Iter [1] Loss: 0.3980 Training Accuracy: 0.81481\n",
      "Epoch [915/2000], Iter [1] Loss: 0.4052 Training Accuracy: 0.80000\n",
      "Epoch [916/2000], Iter [1] Loss: 0.3898 Training Accuracy: 0.81481\n",
      "Epoch [917/2000], Iter [1] Loss: 0.4127 Training Accuracy: 0.80000\n",
      "Epoch [918/2000], Iter [1] Loss: 0.4050 Training Accuracy: 0.81481\n",
      "Epoch [919/2000], Iter [1] Loss: 0.4035 Training Accuracy: 0.81481\n",
      "Epoch [920/2000], Iter [1] Loss: 0.4076 Training Accuracy: 0.80000\n",
      "Epoch [921/2000], Iter [1] Loss: 0.3947 Training Accuracy: 0.81481\n",
      "Epoch [922/2000], Iter [1] Loss: 0.3939 Training Accuracy: 0.81481\n",
      "Epoch [923/2000], Iter [1] Loss: 0.4113 Training Accuracy: 0.80741\n",
      "Epoch [924/2000], Iter [1] Loss: 0.3994 Training Accuracy: 0.80741\n",
      "Epoch [925/2000], Iter [1] Loss: 0.3994 Training Accuracy: 0.81481\n",
      "Epoch [926/2000], Iter [1] Loss: 0.4028 Training Accuracy: 0.80741\n",
      "Epoch [927/2000], Iter [1] Loss: 0.4021 Training Accuracy: 0.81481\n",
      "Epoch [928/2000], Iter [1] Loss: 0.3889 Training Accuracy: 0.82222\n",
      "Epoch [929/2000], Iter [1] Loss: 0.3941 Training Accuracy: 0.80741\n",
      "Epoch [930/2000], Iter [1] Loss: 0.3919 Training Accuracy: 0.81481\n",
      "Epoch [931/2000], Iter [1] Loss: 0.3949 Training Accuracy: 0.80000\n",
      "Epoch [932/2000], Iter [1] Loss: 0.4055 Training Accuracy: 0.80741\n",
      "Epoch [933/2000], Iter [1] Loss: 0.3959 Training Accuracy: 0.81481\n",
      "Epoch [934/2000], Iter [1] Loss: 0.4026 Training Accuracy: 0.80741\n",
      "Epoch [935/2000], Iter [1] Loss: 0.3992 Training Accuracy: 0.81481\n",
      "Epoch [936/2000], Iter [1] Loss: 0.3980 Training Accuracy: 0.81481\n",
      "Epoch [937/2000], Iter [1] Loss: 0.3904 Training Accuracy: 0.80741\n",
      "Epoch [938/2000], Iter [1] Loss: 0.4007 Training Accuracy: 0.80741\n",
      "Epoch [939/2000], Iter [1] Loss: 0.3892 Training Accuracy: 0.80741\n",
      "Epoch [940/2000], Iter [1] Loss: 0.3900 Training Accuracy: 0.81481\n",
      "Epoch [941/2000], Iter [1] Loss: 0.4082 Training Accuracy: 0.81481\n",
      "Epoch [942/2000], Iter [1] Loss: 0.3902 Training Accuracy: 0.80741\n",
      "Epoch [943/2000], Iter [1] Loss: 0.3949 Training Accuracy: 0.81481\n",
      "Epoch [944/2000], Iter [1] Loss: 0.3846 Training Accuracy: 0.80741\n",
      "Epoch [945/2000], Iter [1] Loss: 0.4111 Training Accuracy: 0.80000\n",
      "Epoch [946/2000], Iter [1] Loss: 0.4055 Training Accuracy: 0.81481\n",
      "Epoch [947/2000], Iter [1] Loss: 0.3873 Training Accuracy: 0.80741\n",
      "Epoch [948/2000], Iter [1] Loss: 0.3857 Training Accuracy: 0.81481\n",
      "Epoch [949/2000], Iter [1] Loss: 0.3975 Training Accuracy: 0.82222\n",
      "Epoch [950/2000], Iter [1] Loss: 0.4083 Training Accuracy: 0.81481\n",
      "Epoch [951/2000], Iter [1] Loss: 0.3769 Training Accuracy: 0.80741\n",
      "Epoch [952/2000], Iter [1] Loss: 0.3981 Training Accuracy: 0.81481\n",
      "Epoch [953/2000], Iter [1] Loss: 0.3884 Training Accuracy: 0.81481\n",
      "Epoch [954/2000], Iter [1] Loss: 0.4005 Training Accuracy: 0.80000\n",
      "Epoch [955/2000], Iter [1] Loss: 0.3924 Training Accuracy: 0.80000\n",
      "Epoch [956/2000], Iter [1] Loss: 0.3895 Training Accuracy: 0.82222\n",
      "Epoch [957/2000], Iter [1] Loss: 0.3973 Training Accuracy: 0.80741\n",
      "Epoch [958/2000], Iter [1] Loss: 0.3869 Training Accuracy: 0.81481\n",
      "Epoch [959/2000], Iter [1] Loss: 0.4014 Training Accuracy: 0.81481\n",
      "Epoch [960/2000], Iter [1] Loss: 0.3920 Training Accuracy: 0.81481\n",
      "Epoch [961/2000], Iter [1] Loss: 0.4046 Training Accuracy: 0.81481\n",
      "Epoch [962/2000], Iter [1] Loss: 0.3836 Training Accuracy: 0.80741\n",
      "Epoch [963/2000], Iter [1] Loss: 0.3898 Training Accuracy: 0.80741\n",
      "Epoch [964/2000], Iter [1] Loss: 0.3926 Training Accuracy: 0.80741\n",
      "Epoch [965/2000], Iter [1] Loss: 0.3925 Training Accuracy: 0.80741\n",
      "Epoch [966/2000], Iter [1] Loss: 0.3908 Training Accuracy: 0.82222\n",
      "Epoch [967/2000], Iter [1] Loss: 0.3813 Training Accuracy: 0.82222\n",
      "Epoch [968/2000], Iter [1] Loss: 0.3833 Training Accuracy: 0.81481\n",
      "Epoch [969/2000], Iter [1] Loss: 0.3815 Training Accuracy: 0.80741\n",
      "Epoch [970/2000], Iter [1] Loss: 0.3896 Training Accuracy: 0.80741\n",
      "Epoch [971/2000], Iter [1] Loss: 0.3900 Training Accuracy: 0.81481\n",
      "Epoch [972/2000], Iter [1] Loss: 0.4002 Training Accuracy: 0.80741\n",
      "Epoch [973/2000], Iter [1] Loss: 0.3833 Training Accuracy: 0.80000\n",
      "Epoch [974/2000], Iter [1] Loss: 0.3833 Training Accuracy: 0.80000\n",
      "Epoch [975/2000], Iter [1] Loss: 0.4016 Training Accuracy: 0.81481\n",
      "Epoch [976/2000], Iter [1] Loss: 0.3756 Training Accuracy: 0.80741\n",
      "Epoch [977/2000], Iter [1] Loss: 0.3840 Training Accuracy: 0.81481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [978/2000], Iter [1] Loss: 0.3912 Training Accuracy: 0.80741\n",
      "Epoch [979/2000], Iter [1] Loss: 0.3859 Training Accuracy: 0.81481\n",
      "Epoch [980/2000], Iter [1] Loss: 0.3806 Training Accuracy: 0.80741\n",
      "Epoch [981/2000], Iter [1] Loss: 0.3903 Training Accuracy: 0.80741\n",
      "Epoch [982/2000], Iter [1] Loss: 0.3825 Training Accuracy: 0.81481\n",
      "Epoch [983/2000], Iter [1] Loss: 0.3984 Training Accuracy: 0.81481\n",
      "Epoch [984/2000], Iter [1] Loss: 0.3913 Training Accuracy: 0.80741\n",
      "Epoch [985/2000], Iter [1] Loss: 0.3857 Training Accuracy: 0.80741\n",
      "Epoch [986/2000], Iter [1] Loss: 0.4027 Training Accuracy: 0.81481\n",
      "Epoch [987/2000], Iter [1] Loss: 0.3811 Training Accuracy: 0.81481\n",
      "Epoch [988/2000], Iter [1] Loss: 0.3736 Training Accuracy: 0.80741\n",
      "Epoch [989/2000], Iter [1] Loss: 0.3923 Training Accuracy: 0.81481\n",
      "Epoch [990/2000], Iter [1] Loss: 0.3879 Training Accuracy: 0.80741\n",
      "Epoch [991/2000], Iter [1] Loss: 0.3804 Training Accuracy: 0.79259\n",
      "Epoch [992/2000], Iter [1] Loss: 0.3886 Training Accuracy: 0.80741\n",
      "Epoch [993/2000], Iter [1] Loss: 0.3944 Training Accuracy: 0.81481\n",
      "Epoch [994/2000], Iter [1] Loss: 0.3723 Training Accuracy: 0.81481\n",
      "Epoch [995/2000], Iter [1] Loss: 0.3935 Training Accuracy: 0.79259\n",
      "Epoch [996/2000], Iter [1] Loss: 0.3999 Training Accuracy: 0.80741\n",
      "Epoch [997/2000], Iter [1] Loss: 0.3928 Training Accuracy: 0.82222\n",
      "Epoch [998/2000], Iter [1] Loss: 0.3932 Training Accuracy: 0.80741\n",
      "Epoch [999/2000], Iter [1] Loss: 0.3889 Training Accuracy: 0.81481\n",
      "Epoch [1000/2000], Iter [1] Loss: 0.3750 Training Accuracy: 0.80000\n",
      "Epoch [1001/2000], Iter [1] Loss: 0.3714 Training Accuracy: 0.80000\n",
      "Epoch [1002/2000], Iter [1] Loss: 0.3917 Training Accuracy: 0.79259\n",
      "Epoch [1003/2000], Iter [1] Loss: 0.3874 Training Accuracy: 0.80000\n",
      "Epoch [1004/2000], Iter [1] Loss: 0.3860 Training Accuracy: 0.80741\n",
      "Epoch [1005/2000], Iter [1] Loss: 0.3808 Training Accuracy: 0.80741\n",
      "Epoch [1006/2000], Iter [1] Loss: 0.3785 Training Accuracy: 0.80741\n",
      "Epoch [1007/2000], Iter [1] Loss: 0.3864 Training Accuracy: 0.81481\n",
      "Epoch [1008/2000], Iter [1] Loss: 0.3789 Training Accuracy: 0.81481\n",
      "Epoch [1009/2000], Iter [1] Loss: 0.3721 Training Accuracy: 0.82222\n",
      "Epoch [1010/2000], Iter [1] Loss: 0.3753 Training Accuracy: 0.82963\n",
      "Epoch [1011/2000], Iter [1] Loss: 0.3649 Training Accuracy: 0.80741\n",
      "Epoch [1012/2000], Iter [1] Loss: 0.3794 Training Accuracy: 0.80741\n",
      "Epoch [1013/2000], Iter [1] Loss: 0.3824 Training Accuracy: 0.80741\n",
      "Epoch [1014/2000], Iter [1] Loss: 0.3784 Training Accuracy: 0.82222\n",
      "Epoch [1015/2000], Iter [1] Loss: 0.3741 Training Accuracy: 0.81481\n",
      "Epoch [1016/2000], Iter [1] Loss: 0.3749 Training Accuracy: 0.80741\n",
      "Epoch [1017/2000], Iter [1] Loss: 0.3639 Training Accuracy: 0.81481\n",
      "Epoch [1018/2000], Iter [1] Loss: 0.3737 Training Accuracy: 0.81481\n",
      "Epoch [1019/2000], Iter [1] Loss: 0.3806 Training Accuracy: 0.80741\n",
      "Epoch [1020/2000], Iter [1] Loss: 0.3835 Training Accuracy: 0.80741\n",
      "Epoch [1021/2000], Iter [1] Loss: 0.3933 Training Accuracy: 0.80741\n",
      "Epoch [1022/2000], Iter [1] Loss: 0.3834 Training Accuracy: 0.80000\n",
      "Epoch [1023/2000], Iter [1] Loss: 0.3850 Training Accuracy: 0.79259\n",
      "Epoch [1024/2000], Iter [1] Loss: 0.3875 Training Accuracy: 0.80741\n",
      "Epoch [1025/2000], Iter [1] Loss: 0.3897 Training Accuracy: 0.82222\n",
      "Epoch [1026/2000], Iter [1] Loss: 0.3694 Training Accuracy: 0.81481\n",
      "Epoch [1027/2000], Iter [1] Loss: 0.3595 Training Accuracy: 0.80000\n",
      "Epoch [1028/2000], Iter [1] Loss: 0.3914 Training Accuracy: 0.81481\n",
      "Epoch [1029/2000], Iter [1] Loss: 0.3828 Training Accuracy: 0.80741\n",
      "Epoch [1030/2000], Iter [1] Loss: 0.3814 Training Accuracy: 0.81481\n",
      "Epoch [1031/2000], Iter [1] Loss: 0.3912 Training Accuracy: 0.81481\n",
      "Epoch [1032/2000], Iter [1] Loss: 0.3679 Training Accuracy: 0.80741\n",
      "Epoch [1033/2000], Iter [1] Loss: 0.3898 Training Accuracy: 0.80741\n",
      "Epoch [1034/2000], Iter [1] Loss: 0.3695 Training Accuracy: 0.80741\n",
      "Epoch [1035/2000], Iter [1] Loss: 0.3677 Training Accuracy: 0.82222\n",
      "Epoch [1036/2000], Iter [1] Loss: 0.3890 Training Accuracy: 0.81481\n",
      "Epoch [1037/2000], Iter [1] Loss: 0.3579 Training Accuracy: 0.80741\n",
      "Epoch [1038/2000], Iter [1] Loss: 0.3802 Training Accuracy: 0.82222\n",
      "Epoch [1039/2000], Iter [1] Loss: 0.3788 Training Accuracy: 0.80000\n",
      "Epoch [1040/2000], Iter [1] Loss: 0.3830 Training Accuracy: 0.81481\n",
      "Epoch [1041/2000], Iter [1] Loss: 0.3934 Training Accuracy: 0.80741\n",
      "Epoch [1042/2000], Iter [1] Loss: 0.3826 Training Accuracy: 0.80741\n",
      "Epoch [1043/2000], Iter [1] Loss: 0.3909 Training Accuracy: 0.80741\n",
      "Epoch [1044/2000], Iter [1] Loss: 0.3918 Training Accuracy: 0.81481\n",
      "Epoch [1045/2000], Iter [1] Loss: 0.3813 Training Accuracy: 0.79259\n",
      "Epoch [1046/2000], Iter [1] Loss: 0.3687 Training Accuracy: 0.82222\n",
      "Epoch [1047/2000], Iter [1] Loss: 0.3802 Training Accuracy: 0.80000\n",
      "Epoch [1048/2000], Iter [1] Loss: 0.3867 Training Accuracy: 0.81481\n",
      "Epoch [1049/2000], Iter [1] Loss: 0.3755 Training Accuracy: 0.81481\n",
      "Epoch [1050/2000], Iter [1] Loss: 0.3727 Training Accuracy: 0.80741\n",
      "Epoch [1051/2000], Iter [1] Loss: 0.3699 Training Accuracy: 0.81481\n",
      "Epoch [1052/2000], Iter [1] Loss: 0.3855 Training Accuracy: 0.81481\n",
      "Epoch [1053/2000], Iter [1] Loss: 0.3714 Training Accuracy: 0.81481\n",
      "Epoch [1054/2000], Iter [1] Loss: 0.3651 Training Accuracy: 0.80741\n",
      "Epoch [1055/2000], Iter [1] Loss: 0.3625 Training Accuracy: 0.80741\n",
      "Epoch [1056/2000], Iter [1] Loss: 0.3650 Training Accuracy: 0.80741\n",
      "Epoch [1057/2000], Iter [1] Loss: 0.3710 Training Accuracy: 0.81481\n",
      "Epoch [1058/2000], Iter [1] Loss: 0.3639 Training Accuracy: 0.81481\n",
      "Epoch [1059/2000], Iter [1] Loss: 0.3767 Training Accuracy: 0.80000\n",
      "Epoch [1060/2000], Iter [1] Loss: 0.3750 Training Accuracy: 0.80741\n",
      "Epoch [1061/2000], Iter [1] Loss: 0.3635 Training Accuracy: 0.80000\n",
      "Epoch [1062/2000], Iter [1] Loss: 0.3632 Training Accuracy: 0.81481\n",
      "Epoch [1063/2000], Iter [1] Loss: 0.3818 Training Accuracy: 0.80741\n",
      "Epoch [1064/2000], Iter [1] Loss: 0.3679 Training Accuracy: 0.82222\n",
      "Epoch [1065/2000], Iter [1] Loss: 0.3770 Training Accuracy: 0.81481\n",
      "Epoch [1066/2000], Iter [1] Loss: 0.3718 Training Accuracy: 0.81481\n",
      "Epoch [1067/2000], Iter [1] Loss: 0.3752 Training Accuracy: 0.81481\n",
      "Epoch [1068/2000], Iter [1] Loss: 0.3665 Training Accuracy: 0.80741\n",
      "Epoch [1069/2000], Iter [1] Loss: 0.3782 Training Accuracy: 0.80741\n",
      "Epoch [1070/2000], Iter [1] Loss: 0.3782 Training Accuracy: 0.82222\n",
      "Epoch [1071/2000], Iter [1] Loss: 0.3610 Training Accuracy: 0.80741\n",
      "Epoch [1072/2000], Iter [1] Loss: 0.3802 Training Accuracy: 0.82222\n",
      "Epoch [1073/2000], Iter [1] Loss: 0.3608 Training Accuracy: 0.80741\n",
      "Epoch [1074/2000], Iter [1] Loss: 0.3711 Training Accuracy: 0.81481\n",
      "Epoch [1075/2000], Iter [1] Loss: 0.3783 Training Accuracy: 0.80741\n",
      "Epoch [1076/2000], Iter [1] Loss: 0.3604 Training Accuracy: 0.82222\n",
      "Epoch [1077/2000], Iter [1] Loss: 0.3636 Training Accuracy: 0.80741\n",
      "Epoch [1078/2000], Iter [1] Loss: 0.3771 Training Accuracy: 0.82222\n",
      "Epoch [1079/2000], Iter [1] Loss: 0.3618 Training Accuracy: 0.80741\n",
      "Epoch [1080/2000], Iter [1] Loss: 0.3754 Training Accuracy: 0.80741\n",
      "Epoch [1081/2000], Iter [1] Loss: 0.3681 Training Accuracy: 0.81481\n",
      "Epoch [1082/2000], Iter [1] Loss: 0.3719 Training Accuracy: 0.80000\n",
      "Epoch [1083/2000], Iter [1] Loss: 0.3731 Training Accuracy: 0.81481\n",
      "Epoch [1084/2000], Iter [1] Loss: 0.3788 Training Accuracy: 0.81481\n",
      "Epoch [1085/2000], Iter [1] Loss: 0.3722 Training Accuracy: 0.81481\n",
      "Epoch [1086/2000], Iter [1] Loss: 0.3765 Training Accuracy: 0.82222\n",
      "Epoch [1087/2000], Iter [1] Loss: 0.3881 Training Accuracy: 0.80741\n",
      "Epoch [1088/2000], Iter [1] Loss: 0.3599 Training Accuracy: 0.81481\n",
      "Epoch [1089/2000], Iter [1] Loss: 0.3699 Training Accuracy: 0.80741\n",
      "Epoch [1090/2000], Iter [1] Loss: 0.3691 Training Accuracy: 0.82222\n",
      "Epoch [1091/2000], Iter [1] Loss: 0.3817 Training Accuracy: 0.81481\n",
      "Epoch [1092/2000], Iter [1] Loss: 0.3722 Training Accuracy: 0.81481\n",
      "Epoch [1093/2000], Iter [1] Loss: 0.3719 Training Accuracy: 0.80741\n",
      "Epoch [1094/2000], Iter [1] Loss: 0.3685 Training Accuracy: 0.82222\n",
      "Epoch [1095/2000], Iter [1] Loss: 0.3609 Training Accuracy: 0.80741\n",
      "Epoch [1096/2000], Iter [1] Loss: 0.3518 Training Accuracy: 0.81481\n",
      "Epoch [1097/2000], Iter [1] Loss: 0.3717 Training Accuracy: 0.80741\n",
      "Epoch [1098/2000], Iter [1] Loss: 0.3711 Training Accuracy: 0.80741\n",
      "Epoch [1099/2000], Iter [1] Loss: 0.3647 Training Accuracy: 0.81481\n",
      "Epoch [1100/2000], Iter [1] Loss: 0.3710 Training Accuracy: 0.84444\n",
      "Epoch [1101/2000], Iter [1] Loss: 0.3792 Training Accuracy: 0.80741\n",
      "Epoch [1102/2000], Iter [1] Loss: 0.3780 Training Accuracy: 0.81481\n",
      "Epoch [1103/2000], Iter [1] Loss: 0.3645 Training Accuracy: 0.81481\n",
      "Epoch [1104/2000], Iter [1] Loss: 0.3688 Training Accuracy: 0.81481\n",
      "Epoch [1105/2000], Iter [1] Loss: 0.3674 Training Accuracy: 0.80000\n",
      "Epoch [1106/2000], Iter [1] Loss: 0.3706 Training Accuracy: 0.80741\n",
      "Epoch [1107/2000], Iter [1] Loss: 0.3586 Training Accuracy: 0.81481\n",
      "Epoch [1108/2000], Iter [1] Loss: 0.3701 Training Accuracy: 0.81481\n",
      "Epoch [1109/2000], Iter [1] Loss: 0.3734 Training Accuracy: 0.80741\n",
      "Epoch [1110/2000], Iter [1] Loss: 0.3671 Training Accuracy: 0.80741\n",
      "Epoch [1111/2000], Iter [1] Loss: 0.3778 Training Accuracy: 0.80741\n",
      "Epoch [1112/2000], Iter [1] Loss: 0.3590 Training Accuracy: 0.80741\n",
      "Epoch [1113/2000], Iter [1] Loss: 0.3581 Training Accuracy: 0.80741\n",
      "Epoch [1114/2000], Iter [1] Loss: 0.3711 Training Accuracy: 0.80741\n",
      "Epoch [1115/2000], Iter [1] Loss: 0.3760 Training Accuracy: 0.80000\n",
      "Epoch [1116/2000], Iter [1] Loss: 0.3823 Training Accuracy: 0.80000\n",
      "Epoch [1117/2000], Iter [1] Loss: 0.3771 Training Accuracy: 0.81481\n",
      "Epoch [1118/2000], Iter [1] Loss: 0.3664 Training Accuracy: 0.80741\n",
      "Epoch [1119/2000], Iter [1] Loss: 0.3683 Training Accuracy: 0.80741\n",
      "Epoch [1120/2000], Iter [1] Loss: 0.3498 Training Accuracy: 0.80000\n",
      "Epoch [1121/2000], Iter [1] Loss: 0.3663 Training Accuracy: 0.81481\n",
      "Epoch [1122/2000], Iter [1] Loss: 0.3604 Training Accuracy: 0.80000\n",
      "Epoch [1123/2000], Iter [1] Loss: 0.3605 Training Accuracy: 0.80000\n",
      "Epoch [1124/2000], Iter [1] Loss: 0.3649 Training Accuracy: 0.81481\n",
      "Epoch [1125/2000], Iter [1] Loss: 0.3763 Training Accuracy: 0.83704\n",
      "Epoch [1126/2000], Iter [1] Loss: 0.3662 Training Accuracy: 0.83704\n",
      "Epoch [1127/2000], Iter [1] Loss: 0.3629 Training Accuracy: 0.81481\n",
      "Epoch [1128/2000], Iter [1] Loss: 0.3651 Training Accuracy: 0.80741\n",
      "Epoch [1129/2000], Iter [1] Loss: 0.3754 Training Accuracy: 0.80000\n",
      "Epoch [1130/2000], Iter [1] Loss: 0.3695 Training Accuracy: 0.80741\n",
      "Epoch [1131/2000], Iter [1] Loss: 0.3586 Training Accuracy: 0.81481\n",
      "Epoch [1132/2000], Iter [1] Loss: 0.3584 Training Accuracy: 0.81481\n",
      "Epoch [1133/2000], Iter [1] Loss: 0.3597 Training Accuracy: 0.81481\n",
      "Epoch [1134/2000], Iter [1] Loss: 0.3582 Training Accuracy: 0.80000\n",
      "Epoch [1135/2000], Iter [1] Loss: 0.3545 Training Accuracy: 0.82222\n",
      "Epoch [1136/2000], Iter [1] Loss: 0.3595 Training Accuracy: 0.81481\n",
      "Epoch [1137/2000], Iter [1] Loss: 0.3541 Training Accuracy: 0.80000\n",
      "Epoch [1138/2000], Iter [1] Loss: 0.3650 Training Accuracy: 0.80741\n",
      "Epoch [1139/2000], Iter [1] Loss: 0.3605 Training Accuracy: 0.80741\n",
      "Epoch [1140/2000], Iter [1] Loss: 0.3537 Training Accuracy: 0.81481\n",
      "Epoch [1141/2000], Iter [1] Loss: 0.3557 Training Accuracy: 0.80741\n",
      "Epoch [1142/2000], Iter [1] Loss: 0.3716 Training Accuracy: 0.80000\n",
      "Epoch [1143/2000], Iter [1] Loss: 0.3746 Training Accuracy: 0.80000\n",
      "Epoch [1144/2000], Iter [1] Loss: 0.3478 Training Accuracy: 0.78519\n",
      "Epoch [1145/2000], Iter [1] Loss: 0.3573 Training Accuracy: 0.81481\n",
      "Epoch [1146/2000], Iter [1] Loss: 0.3523 Training Accuracy: 0.82963\n",
      "Epoch [1147/2000], Iter [1] Loss: 0.3660 Training Accuracy: 0.82222\n",
      "Epoch [1148/2000], Iter [1] Loss: 0.3511 Training Accuracy: 0.81481\n",
      "Epoch [1149/2000], Iter [1] Loss: 0.3623 Training Accuracy: 0.81481\n",
      "Epoch [1150/2000], Iter [1] Loss: 0.3728 Training Accuracy: 0.80000\n",
      "Epoch [1151/2000], Iter [1] Loss: 0.3636 Training Accuracy: 0.80000\n",
      "Epoch [1152/2000], Iter [1] Loss: 0.3820 Training Accuracy: 0.80741\n",
      "Epoch [1153/2000], Iter [1] Loss: 0.3585 Training Accuracy: 0.81481\n",
      "Epoch [1154/2000], Iter [1] Loss: 0.3668 Training Accuracy: 0.81481\n",
      "Epoch [1155/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.82222\n",
      "Epoch [1156/2000], Iter [1] Loss: 0.3561 Training Accuracy: 0.80000\n",
      "Epoch [1157/2000], Iter [1] Loss: 0.3514 Training Accuracy: 0.82222\n",
      "Epoch [1158/2000], Iter [1] Loss: 0.3653 Training Accuracy: 0.81481\n",
      "Epoch [1159/2000], Iter [1] Loss: 0.3522 Training Accuracy: 0.80741\n",
      "Epoch [1160/2000], Iter [1] Loss: 0.3676 Training Accuracy: 0.81481\n",
      "Epoch [1161/2000], Iter [1] Loss: 0.3492 Training Accuracy: 0.80000\n",
      "Epoch [1162/2000], Iter [1] Loss: 0.3469 Training Accuracy: 0.80000\n",
      "Epoch [1163/2000], Iter [1] Loss: 0.3537 Training Accuracy: 0.82222\n",
      "Epoch [1164/2000], Iter [1] Loss: 0.3438 Training Accuracy: 0.80741\n",
      "Epoch [1165/2000], Iter [1] Loss: 0.3554 Training Accuracy: 0.81481\n",
      "Epoch [1166/2000], Iter [1] Loss: 0.3610 Training Accuracy: 0.82963\n",
      "Epoch [1167/2000], Iter [1] Loss: 0.3691 Training Accuracy: 0.80741\n",
      "Epoch [1168/2000], Iter [1] Loss: 0.3603 Training Accuracy: 0.82222\n",
      "Epoch [1169/2000], Iter [1] Loss: 0.3475 Training Accuracy: 0.82963\n",
      "Epoch [1170/2000], Iter [1] Loss: 0.3533 Training Accuracy: 0.81481\n",
      "Epoch [1171/2000], Iter [1] Loss: 0.3522 Training Accuracy: 0.82963\n",
      "Epoch [1172/2000], Iter [1] Loss: 0.3497 Training Accuracy: 0.84444\n",
      "Epoch [1173/2000], Iter [1] Loss: 0.3626 Training Accuracy: 0.82963\n",
      "Epoch [1174/2000], Iter [1] Loss: 0.3547 Training Accuracy: 0.82963\n",
      "Epoch [1175/2000], Iter [1] Loss: 0.3721 Training Accuracy: 0.80741\n",
      "Epoch [1176/2000], Iter [1] Loss: 0.3598 Training Accuracy: 0.80741\n",
      "Epoch [1177/2000], Iter [1] Loss: 0.3594 Training Accuracy: 0.80741\n",
      "Epoch [1178/2000], Iter [1] Loss: 0.3457 Training Accuracy: 0.80000\n",
      "Epoch [1179/2000], Iter [1] Loss: 0.3687 Training Accuracy: 0.79259\n",
      "Epoch [1180/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.81481\n",
      "Epoch [1181/2000], Iter [1] Loss: 0.3606 Training Accuracy: 0.81481\n",
      "Epoch [1182/2000], Iter [1] Loss: 0.3588 Training Accuracy: 0.81481\n",
      "Epoch [1183/2000], Iter [1] Loss: 0.3584 Training Accuracy: 0.81481\n",
      "Epoch [1184/2000], Iter [1] Loss: 0.3584 Training Accuracy: 0.82222\n",
      "Epoch [1185/2000], Iter [1] Loss: 0.3670 Training Accuracy: 0.80741\n",
      "Epoch [1186/2000], Iter [1] Loss: 0.3592 Training Accuracy: 0.83704\n",
      "Epoch [1187/2000], Iter [1] Loss: 0.3517 Training Accuracy: 0.82222\n",
      "Epoch [1188/2000], Iter [1] Loss: 0.3520 Training Accuracy: 0.79259\n",
      "Epoch [1189/2000], Iter [1] Loss: 0.3681 Training Accuracy: 0.81481\n",
      "Epoch [1190/2000], Iter [1] Loss: 0.3603 Training Accuracy: 0.81481\n",
      "Epoch [1191/2000], Iter [1] Loss: 0.3598 Training Accuracy: 0.80741\n",
      "Epoch [1192/2000], Iter [1] Loss: 0.3507 Training Accuracy: 0.80000\n",
      "Epoch [1193/2000], Iter [1] Loss: 0.3638 Training Accuracy: 0.81481\n",
      "Epoch [1194/2000], Iter [1] Loss: 0.3560 Training Accuracy: 0.82222\n",
      "Epoch [1195/2000], Iter [1] Loss: 0.3571 Training Accuracy: 0.80741\n",
      "Epoch [1196/2000], Iter [1] Loss: 0.3572 Training Accuracy: 0.82222\n",
      "Epoch [1197/2000], Iter [1] Loss: 0.3495 Training Accuracy: 0.80741\n",
      "Epoch [1198/2000], Iter [1] Loss: 0.3472 Training Accuracy: 0.80741\n",
      "Epoch [1199/2000], Iter [1] Loss: 0.3525 Training Accuracy: 0.82963\n",
      "Epoch [1200/2000], Iter [1] Loss: 0.3616 Training Accuracy: 0.81481\n",
      "Epoch [1201/2000], Iter [1] Loss: 0.3498 Training Accuracy: 0.80741\n",
      "Epoch [1202/2000], Iter [1] Loss: 0.3572 Training Accuracy: 0.80741\n",
      "Epoch [1203/2000], Iter [1] Loss: 0.3609 Training Accuracy: 0.80741\n",
      "Epoch [1204/2000], Iter [1] Loss: 0.3465 Training Accuracy: 0.80741\n",
      "Epoch [1205/2000], Iter [1] Loss: 0.3609 Training Accuracy: 0.82222\n",
      "Epoch [1206/2000], Iter [1] Loss: 0.3395 Training Accuracy: 0.80741\n",
      "Epoch [1207/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.81481\n",
      "Epoch [1208/2000], Iter [1] Loss: 0.3556 Training Accuracy: 0.80741\n",
      "Epoch [1209/2000], Iter [1] Loss: 0.3466 Training Accuracy: 0.79259\n",
      "Epoch [1210/2000], Iter [1] Loss: 0.3461 Training Accuracy: 0.81481\n",
      "Epoch [1211/2000], Iter [1] Loss: 0.3670 Training Accuracy: 0.80000\n",
      "Epoch [1212/2000], Iter [1] Loss: 0.3451 Training Accuracy: 0.81481\n",
      "Epoch [1213/2000], Iter [1] Loss: 0.3636 Training Accuracy: 0.82222\n",
      "Epoch [1214/2000], Iter [1] Loss: 0.3494 Training Accuracy: 0.82963\n",
      "Epoch [1215/2000], Iter [1] Loss: 0.3522 Training Accuracy: 0.80741\n",
      "Epoch [1216/2000], Iter [1] Loss: 0.3552 Training Accuracy: 0.80000\n",
      "Epoch [1217/2000], Iter [1] Loss: 0.3477 Training Accuracy: 0.80741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1218/2000], Iter [1] Loss: 0.3438 Training Accuracy: 0.82222\n",
      "Epoch [1219/2000], Iter [1] Loss: 0.3612 Training Accuracy: 0.80741\n",
      "Epoch [1220/2000], Iter [1] Loss: 0.3530 Training Accuracy: 0.81481\n",
      "Epoch [1221/2000], Iter [1] Loss: 0.3548 Training Accuracy: 0.82222\n",
      "Epoch [1222/2000], Iter [1] Loss: 0.3462 Training Accuracy: 0.80000\n",
      "Epoch [1223/2000], Iter [1] Loss: 0.3405 Training Accuracy: 0.82222\n",
      "Epoch [1224/2000], Iter [1] Loss: 0.3473 Training Accuracy: 0.81481\n",
      "Epoch [1225/2000], Iter [1] Loss: 0.3526 Training Accuracy: 0.79259\n",
      "Epoch [1226/2000], Iter [1] Loss: 0.3516 Training Accuracy: 0.80741\n",
      "Epoch [1227/2000], Iter [1] Loss: 0.3491 Training Accuracy: 0.80741\n",
      "Epoch [1228/2000], Iter [1] Loss: 0.3513 Training Accuracy: 0.82222\n",
      "Epoch [1229/2000], Iter [1] Loss: 0.3472 Training Accuracy: 0.80000\n",
      "Epoch [1230/2000], Iter [1] Loss: 0.3562 Training Accuracy: 0.80741\n",
      "Epoch [1231/2000], Iter [1] Loss: 0.3376 Training Accuracy: 0.83704\n",
      "Epoch [1232/2000], Iter [1] Loss: 0.3598 Training Accuracy: 0.82963\n",
      "Epoch [1233/2000], Iter [1] Loss: 0.3495 Training Accuracy: 0.81481\n",
      "Epoch [1234/2000], Iter [1] Loss: 0.3580 Training Accuracy: 0.80741\n",
      "Epoch [1235/2000], Iter [1] Loss: 0.3486 Training Accuracy: 0.81481\n",
      "Epoch [1236/2000], Iter [1] Loss: 0.3511 Training Accuracy: 0.79259\n",
      "Epoch [1237/2000], Iter [1] Loss: 0.3696 Training Accuracy: 0.78519\n",
      "Epoch [1238/2000], Iter [1] Loss: 0.3556 Training Accuracy: 0.82222\n",
      "Epoch [1239/2000], Iter [1] Loss: 0.3637 Training Accuracy: 0.80000\n",
      "Epoch [1240/2000], Iter [1] Loss: 0.3502 Training Accuracy: 0.81481\n",
      "Epoch [1241/2000], Iter [1] Loss: 0.3492 Training Accuracy: 0.80741\n",
      "Epoch [1242/2000], Iter [1] Loss: 0.3574 Training Accuracy: 0.80000\n",
      "Epoch [1243/2000], Iter [1] Loss: 0.3404 Training Accuracy: 0.81481\n",
      "Epoch [1244/2000], Iter [1] Loss: 0.3612 Training Accuracy: 0.80000\n",
      "Epoch [1245/2000], Iter [1] Loss: 0.3528 Training Accuracy: 0.84444\n",
      "Epoch [1246/2000], Iter [1] Loss: 0.3788 Training Accuracy: 0.81481\n",
      "Epoch [1247/2000], Iter [1] Loss: 0.3658 Training Accuracy: 0.80741\n",
      "Epoch [1248/2000], Iter [1] Loss: 0.3462 Training Accuracy: 0.80000\n",
      "Epoch [1249/2000], Iter [1] Loss: 0.3427 Training Accuracy: 0.81481\n",
      "Epoch [1250/2000], Iter [1] Loss: 0.3606 Training Accuracy: 0.82222\n",
      "Epoch [1251/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.80741\n",
      "Epoch [1252/2000], Iter [1] Loss: 0.3454 Training Accuracy: 0.79259\n",
      "Epoch [1253/2000], Iter [1] Loss: 0.3465 Training Accuracy: 0.79259\n",
      "Epoch [1254/2000], Iter [1] Loss: 0.3350 Training Accuracy: 0.79259\n",
      "Epoch [1255/2000], Iter [1] Loss: 0.3427 Training Accuracy: 0.82222\n",
      "Epoch [1256/2000], Iter [1] Loss: 0.3544 Training Accuracy: 0.81481\n",
      "Epoch [1257/2000], Iter [1] Loss: 0.3589 Training Accuracy: 0.82222\n",
      "Epoch [1258/2000], Iter [1] Loss: 0.3434 Training Accuracy: 0.80000\n",
      "Epoch [1259/2000], Iter [1] Loss: 0.3444 Training Accuracy: 0.80741\n",
      "Epoch [1260/2000], Iter [1] Loss: 0.3371 Training Accuracy: 0.80741\n",
      "Epoch [1261/2000], Iter [1] Loss: 0.3534 Training Accuracy: 0.81481\n",
      "Epoch [1262/2000], Iter [1] Loss: 0.3383 Training Accuracy: 0.80741\n",
      "Epoch [1263/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.82222\n",
      "Epoch [1264/2000], Iter [1] Loss: 0.3503 Training Accuracy: 0.81481\n",
      "Epoch [1265/2000], Iter [1] Loss: 0.3471 Training Accuracy: 0.82222\n",
      "Epoch [1266/2000], Iter [1] Loss: 0.3520 Training Accuracy: 0.81481\n",
      "Epoch [1267/2000], Iter [1] Loss: 0.3424 Training Accuracy: 0.80741\n",
      "Epoch [1268/2000], Iter [1] Loss: 0.3335 Training Accuracy: 0.81481\n",
      "Epoch [1269/2000], Iter [1] Loss: 0.3527 Training Accuracy: 0.80000\n",
      "Epoch [1270/2000], Iter [1] Loss: 0.3380 Training Accuracy: 0.80741\n",
      "Epoch [1271/2000], Iter [1] Loss: 0.3267 Training Accuracy: 0.80000\n",
      "Epoch [1272/2000], Iter [1] Loss: 0.3388 Training Accuracy: 0.80741\n",
      "Epoch [1273/2000], Iter [1] Loss: 0.3525 Training Accuracy: 0.80000\n",
      "Epoch [1274/2000], Iter [1] Loss: 0.3516 Training Accuracy: 0.80741\n",
      "Epoch [1275/2000], Iter [1] Loss: 0.3324 Training Accuracy: 0.80000\n",
      "Epoch [1276/2000], Iter [1] Loss: 0.3418 Training Accuracy: 0.80000\n",
      "Epoch [1277/2000], Iter [1] Loss: 0.3466 Training Accuracy: 0.80741\n",
      "Epoch [1278/2000], Iter [1] Loss: 0.3394 Training Accuracy: 0.80000\n",
      "Epoch [1279/2000], Iter [1] Loss: 0.3470 Training Accuracy: 0.82222\n",
      "Epoch [1280/2000], Iter [1] Loss: 0.3538 Training Accuracy: 0.82963\n",
      "Epoch [1281/2000], Iter [1] Loss: 0.3464 Training Accuracy: 0.80000\n",
      "Epoch [1282/2000], Iter [1] Loss: 0.3462 Training Accuracy: 0.81481\n",
      "Epoch [1283/2000], Iter [1] Loss: 0.3410 Training Accuracy: 0.80000\n",
      "Epoch [1284/2000], Iter [1] Loss: 0.3296 Training Accuracy: 0.80741\n",
      "Epoch [1285/2000], Iter [1] Loss: 0.3381 Training Accuracy: 0.82963\n",
      "Epoch [1286/2000], Iter [1] Loss: 0.3537 Training Accuracy: 0.83704\n",
      "Epoch [1287/2000], Iter [1] Loss: 0.3402 Training Accuracy: 0.82963\n",
      "Epoch [1288/2000], Iter [1] Loss: 0.3258 Training Accuracy: 0.81481\n",
      "Epoch [1289/2000], Iter [1] Loss: 0.3481 Training Accuracy: 0.82963\n",
      "Epoch [1290/2000], Iter [1] Loss: 0.3453 Training Accuracy: 0.80741\n",
      "Epoch [1291/2000], Iter [1] Loss: 0.3528 Training Accuracy: 0.81481\n",
      "Epoch [1292/2000], Iter [1] Loss: 0.3451 Training Accuracy: 0.80741\n",
      "Epoch [1293/2000], Iter [1] Loss: 0.3447 Training Accuracy: 0.78519\n",
      "Epoch [1294/2000], Iter [1] Loss: 0.3371 Training Accuracy: 0.80741\n",
      "Epoch [1295/2000], Iter [1] Loss: 0.3354 Training Accuracy: 0.83704\n",
      "Epoch [1296/2000], Iter [1] Loss: 0.3405 Training Accuracy: 0.80741\n",
      "Epoch [1297/2000], Iter [1] Loss: 0.3434 Training Accuracy: 0.79259\n",
      "Epoch [1298/2000], Iter [1] Loss: 0.3468 Training Accuracy: 0.80741\n",
      "Epoch [1299/2000], Iter [1] Loss: 0.3565 Training Accuracy: 0.80000\n",
      "Epoch [1300/2000], Iter [1] Loss: 0.3458 Training Accuracy: 0.81481\n",
      "Epoch [1301/2000], Iter [1] Loss: 0.3494 Training Accuracy: 0.82222\n",
      "Epoch [1302/2000], Iter [1] Loss: 0.3433 Training Accuracy: 0.80741\n",
      "Epoch [1303/2000], Iter [1] Loss: 0.3497 Training Accuracy: 0.82222\n",
      "Epoch [1304/2000], Iter [1] Loss: 0.3540 Training Accuracy: 0.80741\n",
      "Epoch [1305/2000], Iter [1] Loss: 0.3341 Training Accuracy: 0.82222\n",
      "Epoch [1306/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.80741\n",
      "Epoch [1307/2000], Iter [1] Loss: 0.3382 Training Accuracy: 0.81481\n",
      "Epoch [1308/2000], Iter [1] Loss: 0.3432 Training Accuracy: 0.80000\n",
      "Epoch [1309/2000], Iter [1] Loss: 0.3471 Training Accuracy: 0.81481\n",
      "Epoch [1310/2000], Iter [1] Loss: 0.3596 Training Accuracy: 0.79259\n",
      "Epoch [1311/2000], Iter [1] Loss: 0.3334 Training Accuracy: 0.79259\n",
      "Epoch [1312/2000], Iter [1] Loss: 0.3348 Training Accuracy: 0.82222\n",
      "Epoch [1313/2000], Iter [1] Loss: 0.3289 Training Accuracy: 0.80741\n",
      "Epoch [1314/2000], Iter [1] Loss: 0.3377 Training Accuracy: 0.83704\n",
      "Epoch [1315/2000], Iter [1] Loss: 0.3370 Training Accuracy: 0.82222\n",
      "Epoch [1316/2000], Iter [1] Loss: 0.3497 Training Accuracy: 0.81481\n",
      "Epoch [1317/2000], Iter [1] Loss: 0.3339 Training Accuracy: 0.80741\n",
      "Epoch [1318/2000], Iter [1] Loss: 0.3479 Training Accuracy: 0.82963\n",
      "Epoch [1319/2000], Iter [1] Loss: 0.3381 Training Accuracy: 0.82963\n",
      "Epoch [1320/2000], Iter [1] Loss: 0.3424 Training Accuracy: 0.81481\n",
      "Epoch [1321/2000], Iter [1] Loss: 0.3484 Training Accuracy: 0.82222\n",
      "Epoch [1322/2000], Iter [1] Loss: 0.3584 Training Accuracy: 0.80000\n",
      "Epoch [1323/2000], Iter [1] Loss: 0.3421 Training Accuracy: 0.80741\n",
      "Epoch [1324/2000], Iter [1] Loss: 0.3303 Training Accuracy: 0.83704\n",
      "Epoch [1325/2000], Iter [1] Loss: 0.3370 Training Accuracy: 0.82222\n",
      "Epoch [1326/2000], Iter [1] Loss: 0.3415 Training Accuracy: 0.81481\n",
      "Epoch [1327/2000], Iter [1] Loss: 0.3390 Training Accuracy: 0.83704\n",
      "Epoch [1328/2000], Iter [1] Loss: 0.3324 Training Accuracy: 0.81481\n",
      "Epoch [1329/2000], Iter [1] Loss: 0.3503 Training Accuracy: 0.78519\n",
      "Epoch [1330/2000], Iter [1] Loss: 0.3434 Training Accuracy: 0.81481\n",
      "Epoch [1331/2000], Iter [1] Loss: 0.3372 Training Accuracy: 0.80741\n",
      "Epoch [1332/2000], Iter [1] Loss: 0.3297 Training Accuracy: 0.85185\n",
      "Epoch [1333/2000], Iter [1] Loss: 0.3404 Training Accuracy: 0.82222\n",
      "Epoch [1334/2000], Iter [1] Loss: 0.3353 Training Accuracy: 0.82222\n",
      "Epoch [1335/2000], Iter [1] Loss: 0.3351 Training Accuracy: 0.81481\n",
      "Epoch [1336/2000], Iter [1] Loss: 0.3435 Training Accuracy: 0.80741\n",
      "Epoch [1337/2000], Iter [1] Loss: 0.3464 Training Accuracy: 0.80741\n",
      "Epoch [1338/2000], Iter [1] Loss: 0.3415 Training Accuracy: 0.79259\n",
      "Epoch [1339/2000], Iter [1] Loss: 0.3366 Training Accuracy: 0.80741\n",
      "Epoch [1340/2000], Iter [1] Loss: 0.3516 Training Accuracy: 0.82222\n",
      "Epoch [1341/2000], Iter [1] Loss: 0.3516 Training Accuracy: 0.81481\n",
      "Epoch [1342/2000], Iter [1] Loss: 0.3394 Training Accuracy: 0.82222\n",
      "Epoch [1343/2000], Iter [1] Loss: 0.3435 Training Accuracy: 0.82963\n",
      "Epoch [1344/2000], Iter [1] Loss: 0.3393 Training Accuracy: 0.80741\n",
      "Epoch [1345/2000], Iter [1] Loss: 0.3489 Training Accuracy: 0.80741\n",
      "Epoch [1346/2000], Iter [1] Loss: 0.3478 Training Accuracy: 0.80741\n",
      "Epoch [1347/2000], Iter [1] Loss: 0.3369 Training Accuracy: 0.82222\n",
      "Epoch [1348/2000], Iter [1] Loss: 0.3430 Training Accuracy: 0.80000\n",
      "Epoch [1349/2000], Iter [1] Loss: 0.3385 Training Accuracy: 0.82222\n",
      "Epoch [1350/2000], Iter [1] Loss: 0.3364 Training Accuracy: 0.80741\n",
      "Epoch [1351/2000], Iter [1] Loss: 0.3368 Training Accuracy: 0.82222\n",
      "Epoch [1352/2000], Iter [1] Loss: 0.3511 Training Accuracy: 0.82222\n",
      "Epoch [1353/2000], Iter [1] Loss: 0.3410 Training Accuracy: 0.82963\n",
      "Epoch [1354/2000], Iter [1] Loss: 0.3319 Training Accuracy: 0.81481\n",
      "Epoch [1355/2000], Iter [1] Loss: 0.3402 Training Accuracy: 0.80000\n",
      "Epoch [1356/2000], Iter [1] Loss: 0.3432 Training Accuracy: 0.83704\n",
      "Epoch [1357/2000], Iter [1] Loss: 0.3495 Training Accuracy: 0.81481\n",
      "Epoch [1358/2000], Iter [1] Loss: 0.3468 Training Accuracy: 0.80741\n",
      "Epoch [1359/2000], Iter [1] Loss: 0.3345 Training Accuracy: 0.80741\n",
      "Epoch [1360/2000], Iter [1] Loss: 0.3451 Training Accuracy: 0.83704\n",
      "Epoch [1361/2000], Iter [1] Loss: 0.3522 Training Accuracy: 0.82963\n",
      "Epoch [1362/2000], Iter [1] Loss: 0.3444 Training Accuracy: 0.79259\n",
      "Epoch [1363/2000], Iter [1] Loss: 0.3371 Training Accuracy: 0.82222\n",
      "Epoch [1364/2000], Iter [1] Loss: 0.3427 Training Accuracy: 0.80741\n",
      "Epoch [1365/2000], Iter [1] Loss: 0.3487 Training Accuracy: 0.80741\n",
      "Epoch [1366/2000], Iter [1] Loss: 0.3470 Training Accuracy: 0.81481\n",
      "Epoch [1367/2000], Iter [1] Loss: 0.3360 Training Accuracy: 0.80000\n",
      "Epoch [1368/2000], Iter [1] Loss: 0.3395 Training Accuracy: 0.80000\n",
      "Epoch [1369/2000], Iter [1] Loss: 0.3405 Training Accuracy: 0.81481\n",
      "Epoch [1370/2000], Iter [1] Loss: 0.3481 Training Accuracy: 0.80741\n",
      "Epoch [1371/2000], Iter [1] Loss: 0.3349 Training Accuracy: 0.81481\n",
      "Epoch [1372/2000], Iter [1] Loss: 0.3378 Training Accuracy: 0.80000\n",
      "Epoch [1373/2000], Iter [1] Loss: 0.3347 Training Accuracy: 0.80000\n",
      "Epoch [1374/2000], Iter [1] Loss: 0.3394 Training Accuracy: 0.81481\n",
      "Epoch [1375/2000], Iter [1] Loss: 0.3473 Training Accuracy: 0.80741\n",
      "Epoch [1376/2000], Iter [1] Loss: 0.3518 Training Accuracy: 0.79259\n",
      "Epoch [1377/2000], Iter [1] Loss: 0.3518 Training Accuracy: 0.82222\n",
      "Epoch [1378/2000], Iter [1] Loss: 0.3332 Training Accuracy: 0.81481\n",
      "Epoch [1379/2000], Iter [1] Loss: 0.3364 Training Accuracy: 0.82222\n",
      "Epoch [1380/2000], Iter [1] Loss: 0.3487 Training Accuracy: 0.78519\n",
      "Epoch [1381/2000], Iter [1] Loss: 0.3304 Training Accuracy: 0.77778\n",
      "Epoch [1382/2000], Iter [1] Loss: 0.3362 Training Accuracy: 0.81481\n",
      "Epoch [1383/2000], Iter [1] Loss: 0.3348 Training Accuracy: 0.78519\n",
      "Epoch [1384/2000], Iter [1] Loss: 0.3299 Training Accuracy: 0.82963\n",
      "Epoch [1385/2000], Iter [1] Loss: 0.3536 Training Accuracy: 0.78519\n",
      "Epoch [1386/2000], Iter [1] Loss: 0.3391 Training Accuracy: 0.80741\n",
      "Epoch [1387/2000], Iter [1] Loss: 0.3275 Training Accuracy: 0.80741\n",
      "Epoch [1388/2000], Iter [1] Loss: 0.3412 Training Accuracy: 0.80741\n",
      "Epoch [1389/2000], Iter [1] Loss: 0.3317 Training Accuracy: 0.82222\n",
      "Epoch [1390/2000], Iter [1] Loss: 0.3437 Training Accuracy: 0.85185\n",
      "Epoch [1391/2000], Iter [1] Loss: 0.3241 Training Accuracy: 0.82222\n",
      "Epoch [1392/2000], Iter [1] Loss: 0.3388 Training Accuracy: 0.80000\n",
      "Epoch [1393/2000], Iter [1] Loss: 0.3257 Training Accuracy: 0.80741\n",
      "Epoch [1394/2000], Iter [1] Loss: 0.3509 Training Accuracy: 0.80000\n",
      "Epoch [1395/2000], Iter [1] Loss: 0.3439 Training Accuracy: 0.81481\n",
      "Epoch [1396/2000], Iter [1] Loss: 0.3267 Training Accuracy: 0.81481\n",
      "Epoch [1397/2000], Iter [1] Loss: 0.3292 Training Accuracy: 0.80741\n",
      "Epoch [1398/2000], Iter [1] Loss: 0.3312 Training Accuracy: 0.80000\n",
      "Epoch [1399/2000], Iter [1] Loss: 0.3325 Training Accuracy: 0.80741\n",
      "Epoch [1400/2000], Iter [1] Loss: 0.3363 Training Accuracy: 0.80000\n",
      "Epoch [1401/2000], Iter [1] Loss: 0.3382 Training Accuracy: 0.80741\n",
      "Epoch [1402/2000], Iter [1] Loss: 0.3181 Training Accuracy: 0.81481\n",
      "Epoch [1403/2000], Iter [1] Loss: 0.3375 Training Accuracy: 0.81481\n",
      "Epoch [1404/2000], Iter [1] Loss: 0.3366 Training Accuracy: 0.81481\n",
      "Epoch [1405/2000], Iter [1] Loss: 0.3265 Training Accuracy: 0.82222\n",
      "Epoch [1406/2000], Iter [1] Loss: 0.3361 Training Accuracy: 0.83704\n",
      "Epoch [1407/2000], Iter [1] Loss: 0.3237 Training Accuracy: 0.82222\n",
      "Epoch [1408/2000], Iter [1] Loss: 0.3392 Training Accuracy: 0.83704\n",
      "Epoch [1409/2000], Iter [1] Loss: 0.3332 Training Accuracy: 0.78519\n",
      "Epoch [1410/2000], Iter [1] Loss: 0.3337 Training Accuracy: 0.82963\n",
      "Epoch [1411/2000], Iter [1] Loss: 0.3365 Training Accuracy: 0.80741\n",
      "Epoch [1412/2000], Iter [1] Loss: 0.3299 Training Accuracy: 0.79259\n",
      "Epoch [1413/2000], Iter [1] Loss: 0.3288 Training Accuracy: 0.82963\n",
      "Epoch [1414/2000], Iter [1] Loss: 0.3315 Training Accuracy: 0.80000\n",
      "Epoch [1415/2000], Iter [1] Loss: 0.3407 Training Accuracy: 0.81481\n",
      "Epoch [1416/2000], Iter [1] Loss: 0.3268 Training Accuracy: 0.80741\n",
      "Epoch [1417/2000], Iter [1] Loss: 0.3310 Training Accuracy: 0.80741\n",
      "Epoch [1418/2000], Iter [1] Loss: 0.3322 Training Accuracy: 0.80741\n",
      "Epoch [1419/2000], Iter [1] Loss: 0.3274 Training Accuracy: 0.83704\n",
      "Epoch [1420/2000], Iter [1] Loss: 0.3271 Training Accuracy: 0.80000\n",
      "Epoch [1421/2000], Iter [1] Loss: 0.3407 Training Accuracy: 0.81481\n",
      "Epoch [1422/2000], Iter [1] Loss: 0.3379 Training Accuracy: 0.80741\n",
      "Epoch [1423/2000], Iter [1] Loss: 0.3205 Training Accuracy: 0.81481\n",
      "Epoch [1424/2000], Iter [1] Loss: 0.3268 Training Accuracy: 0.80000\n",
      "Epoch [1425/2000], Iter [1] Loss: 0.3392 Training Accuracy: 0.80000\n",
      "Epoch [1426/2000], Iter [1] Loss: 0.3259 Training Accuracy: 0.82222\n",
      "Epoch [1427/2000], Iter [1] Loss: 0.3276 Training Accuracy: 0.83704\n",
      "Epoch [1428/2000], Iter [1] Loss: 0.3377 Training Accuracy: 0.80000\n",
      "Epoch [1429/2000], Iter [1] Loss: 0.3362 Training Accuracy: 0.77778\n",
      "Epoch [1430/2000], Iter [1] Loss: 0.3374 Training Accuracy: 0.78519\n",
      "Epoch [1431/2000], Iter [1] Loss: 0.3317 Training Accuracy: 0.80741\n",
      "Epoch [1432/2000], Iter [1] Loss: 0.3266 Training Accuracy: 0.79259\n",
      "Epoch [1433/2000], Iter [1] Loss: 0.3288 Training Accuracy: 0.82963\n",
      "Epoch [1434/2000], Iter [1] Loss: 0.3388 Training Accuracy: 0.85185\n",
      "Epoch [1435/2000], Iter [1] Loss: 0.3247 Training Accuracy: 0.82963\n",
      "Epoch [1436/2000], Iter [1] Loss: 0.3295 Training Accuracy: 0.82222\n",
      "Epoch [1437/2000], Iter [1] Loss: 0.3352 Training Accuracy: 0.82963\n",
      "Epoch [1438/2000], Iter [1] Loss: 0.3299 Training Accuracy: 0.82222\n",
      "Epoch [1439/2000], Iter [1] Loss: 0.3257 Training Accuracy: 0.80741\n",
      "Epoch [1440/2000], Iter [1] Loss: 0.3536 Training Accuracy: 0.82963\n",
      "Epoch [1441/2000], Iter [1] Loss: 0.3259 Training Accuracy: 0.80000\n",
      "Epoch [1442/2000], Iter [1] Loss: 0.3407 Training Accuracy: 0.81481\n",
      "Epoch [1443/2000], Iter [1] Loss: 0.3274 Training Accuracy: 0.80000\n",
      "Epoch [1444/2000], Iter [1] Loss: 0.3264 Training Accuracy: 0.83704\n",
      "Epoch [1445/2000], Iter [1] Loss: 0.3273 Training Accuracy: 0.81481\n",
      "Epoch [1446/2000], Iter [1] Loss: 0.3374 Training Accuracy: 0.77778\n",
      "Epoch [1447/2000], Iter [1] Loss: 0.3390 Training Accuracy: 0.80741\n",
      "Epoch [1448/2000], Iter [1] Loss: 0.3308 Training Accuracy: 0.78519\n",
      "Epoch [1449/2000], Iter [1] Loss: 0.3307 Training Accuracy: 0.84444\n",
      "Epoch [1450/2000], Iter [1] Loss: 0.3396 Training Accuracy: 0.80741\n",
      "Epoch [1451/2000], Iter [1] Loss: 0.3237 Training Accuracy: 0.82963\n",
      "Epoch [1452/2000], Iter [1] Loss: 0.3434 Training Accuracy: 0.81481\n",
      "Epoch [1453/2000], Iter [1] Loss: 0.3417 Training Accuracy: 0.80000\n",
      "Epoch [1454/2000], Iter [1] Loss: 0.3269 Training Accuracy: 0.80000\n",
      "Epoch [1455/2000], Iter [1] Loss: 0.3253 Training Accuracy: 0.81481\n",
      "Epoch [1456/2000], Iter [1] Loss: 0.3381 Training Accuracy: 0.82222\n",
      "Epoch [1457/2000], Iter [1] Loss: 0.3249 Training Accuracy: 0.80741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1458/2000], Iter [1] Loss: 0.3412 Training Accuracy: 0.81481\n",
      "Epoch [1459/2000], Iter [1] Loss: 0.3255 Training Accuracy: 0.83704\n",
      "Epoch [1460/2000], Iter [1] Loss: 0.3340 Training Accuracy: 0.78519\n",
      "Epoch [1461/2000], Iter [1] Loss: 0.3162 Training Accuracy: 0.80741\n",
      "Epoch [1462/2000], Iter [1] Loss: 0.3401 Training Accuracy: 0.80741\n",
      "Epoch [1463/2000], Iter [1] Loss: 0.3321 Training Accuracy: 0.85926\n",
      "Epoch [1464/2000], Iter [1] Loss: 0.3247 Training Accuracy: 0.83704\n",
      "Epoch [1465/2000], Iter [1] Loss: 0.3329 Training Accuracy: 0.81481\n",
      "Epoch [1466/2000], Iter [1] Loss: 0.3300 Training Accuracy: 0.79259\n",
      "Epoch [1467/2000], Iter [1] Loss: 0.3210 Training Accuracy: 0.82963\n",
      "Epoch [1468/2000], Iter [1] Loss: 0.3202 Training Accuracy: 0.80741\n",
      "Epoch [1469/2000], Iter [1] Loss: 0.3293 Training Accuracy: 0.82963\n",
      "Epoch [1470/2000], Iter [1] Loss: 0.3413 Training Accuracy: 0.82963\n",
      "Epoch [1471/2000], Iter [1] Loss: 0.3269 Training Accuracy: 0.80741\n",
      "Epoch [1472/2000], Iter [1] Loss: 0.3181 Training Accuracy: 0.80741\n",
      "Epoch [1473/2000], Iter [1] Loss: 0.3279 Training Accuracy: 0.81481\n",
      "Epoch [1474/2000], Iter [1] Loss: 0.3382 Training Accuracy: 0.80000\n",
      "Epoch [1475/2000], Iter [1] Loss: 0.3398 Training Accuracy: 0.81481\n",
      "Epoch [1476/2000], Iter [1] Loss: 0.3318 Training Accuracy: 0.81481\n",
      "Epoch [1477/2000], Iter [1] Loss: 0.3265 Training Accuracy: 0.83704\n",
      "Epoch [1478/2000], Iter [1] Loss: 0.3247 Training Accuracy: 0.82222\n",
      "Epoch [1479/2000], Iter [1] Loss: 0.3297 Training Accuracy: 0.83704\n",
      "Epoch [1480/2000], Iter [1] Loss: 0.3430 Training Accuracy: 0.81481\n",
      "Epoch [1481/2000], Iter [1] Loss: 0.3220 Training Accuracy: 0.82222\n",
      "Epoch [1482/2000], Iter [1] Loss: 0.3227 Training Accuracy: 0.82222\n",
      "Epoch [1483/2000], Iter [1] Loss: 0.3229 Training Accuracy: 0.80000\n",
      "Epoch [1484/2000], Iter [1] Loss: 0.3369 Training Accuracy: 0.82222\n",
      "Epoch [1485/2000], Iter [1] Loss: 0.3342 Training Accuracy: 0.82222\n",
      "Epoch [1486/2000], Iter [1] Loss: 0.3293 Training Accuracy: 0.83704\n",
      "Epoch [1487/2000], Iter [1] Loss: 0.3247 Training Accuracy: 0.82222\n",
      "Epoch [1488/2000], Iter [1] Loss: 0.3292 Training Accuracy: 0.78519\n",
      "Epoch [1489/2000], Iter [1] Loss: 0.3187 Training Accuracy: 0.81481\n",
      "Epoch [1490/2000], Iter [1] Loss: 0.3361 Training Accuracy: 0.80000\n",
      "Epoch [1491/2000], Iter [1] Loss: 0.3189 Training Accuracy: 0.82222\n",
      "Epoch [1492/2000], Iter [1] Loss: 0.3254 Training Accuracy: 0.82963\n",
      "Epoch [1493/2000], Iter [1] Loss: 0.3363 Training Accuracy: 0.78519\n",
      "Epoch [1494/2000], Iter [1] Loss: 0.3283 Training Accuracy: 0.82222\n",
      "Epoch [1495/2000], Iter [1] Loss: 0.3253 Training Accuracy: 0.81481\n",
      "Epoch [1496/2000], Iter [1] Loss: 0.3204 Training Accuracy: 0.82222\n",
      "Epoch [1497/2000], Iter [1] Loss: 0.3258 Training Accuracy: 0.80000\n",
      "Epoch [1498/2000], Iter [1] Loss: 0.3190 Training Accuracy: 0.80000\n",
      "Epoch [1499/2000], Iter [1] Loss: 0.3413 Training Accuracy: 0.82222\n",
      "Epoch [1500/2000], Iter [1] Loss: 0.3443 Training Accuracy: 0.81481\n",
      "Epoch [1501/2000], Iter [1] Loss: 0.3360 Training Accuracy: 0.82222\n",
      "Epoch [1502/2000], Iter [1] Loss: 0.3320 Training Accuracy: 0.80741\n",
      "Epoch [1503/2000], Iter [1] Loss: 0.3121 Training Accuracy: 0.82222\n",
      "Epoch [1504/2000], Iter [1] Loss: 0.3432 Training Accuracy: 0.81481\n",
      "Epoch [1505/2000], Iter [1] Loss: 0.3268 Training Accuracy: 0.85185\n",
      "Epoch [1506/2000], Iter [1] Loss: 0.3327 Training Accuracy: 0.82963\n",
      "Epoch [1507/2000], Iter [1] Loss: 0.3293 Training Accuracy: 0.82963\n",
      "Epoch [1508/2000], Iter [1] Loss: 0.3212 Training Accuracy: 0.81481\n",
      "Epoch [1509/2000], Iter [1] Loss: 0.3313 Training Accuracy: 0.79259\n",
      "Epoch [1510/2000], Iter [1] Loss: 0.3198 Training Accuracy: 0.84444\n",
      "Epoch [1511/2000], Iter [1] Loss: 0.3282 Training Accuracy: 0.77778\n",
      "Epoch [1512/2000], Iter [1] Loss: 0.3375 Training Accuracy: 0.80000\n",
      "Epoch [1513/2000], Iter [1] Loss: 0.3262 Training Accuracy: 0.83704\n",
      "Epoch [1514/2000], Iter [1] Loss: 0.3216 Training Accuracy: 0.81481\n",
      "Epoch [1515/2000], Iter [1] Loss: 0.3355 Training Accuracy: 0.85185\n",
      "Epoch [1516/2000], Iter [1] Loss: 0.3278 Training Accuracy: 0.82963\n",
      "Epoch [1517/2000], Iter [1] Loss: 0.3239 Training Accuracy: 0.81481\n",
      "Epoch [1518/2000], Iter [1] Loss: 0.3218 Training Accuracy: 0.80741\n",
      "Epoch [1519/2000], Iter [1] Loss: 0.3308 Training Accuracy: 0.81481\n",
      "Epoch [1520/2000], Iter [1] Loss: 0.3255 Training Accuracy: 0.80741\n",
      "Epoch [1521/2000], Iter [1] Loss: 0.3296 Training Accuracy: 0.86667\n",
      "Epoch [1522/2000], Iter [1] Loss: 0.3189 Training Accuracy: 0.82963\n",
      "Epoch [1523/2000], Iter [1] Loss: 0.3187 Training Accuracy: 0.80000\n",
      "Epoch [1524/2000], Iter [1] Loss: 0.3201 Training Accuracy: 0.82222\n",
      "Epoch [1525/2000], Iter [1] Loss: 0.3370 Training Accuracy: 0.82963\n",
      "Epoch [1526/2000], Iter [1] Loss: 0.3349 Training Accuracy: 0.80741\n",
      "Epoch [1527/2000], Iter [1] Loss: 0.3201 Training Accuracy: 0.80000\n",
      "Epoch [1528/2000], Iter [1] Loss: 0.3175 Training Accuracy: 0.80000\n",
      "Epoch [1529/2000], Iter [1] Loss: 0.3254 Training Accuracy: 0.82963\n",
      "Epoch [1530/2000], Iter [1] Loss: 0.3267 Training Accuracy: 0.82963\n",
      "Epoch [1531/2000], Iter [1] Loss: 0.3263 Training Accuracy: 0.81481\n",
      "Epoch [1532/2000], Iter [1] Loss: 0.3150 Training Accuracy: 0.80000\n",
      "Epoch [1533/2000], Iter [1] Loss: 0.3260 Training Accuracy: 0.82963\n",
      "Epoch [1534/2000], Iter [1] Loss: 0.3226 Training Accuracy: 0.81481\n",
      "Epoch [1535/2000], Iter [1] Loss: 0.3310 Training Accuracy: 0.82963\n",
      "Epoch [1536/2000], Iter [1] Loss: 0.3211 Training Accuracy: 0.81481\n",
      "Epoch [1537/2000], Iter [1] Loss: 0.3303 Training Accuracy: 0.81481\n",
      "Epoch [1538/2000], Iter [1] Loss: 0.3225 Training Accuracy: 0.83704\n",
      "Epoch [1539/2000], Iter [1] Loss: 0.3218 Training Accuracy: 0.80741\n",
      "Epoch [1540/2000], Iter [1] Loss: 0.3223 Training Accuracy: 0.82222\n",
      "Epoch [1541/2000], Iter [1] Loss: 0.3278 Training Accuracy: 0.82222\n",
      "Epoch [1542/2000], Iter [1] Loss: 0.3240 Training Accuracy: 0.83704\n",
      "Epoch [1543/2000], Iter [1] Loss: 0.3307 Training Accuracy: 0.82222\n",
      "Epoch [1544/2000], Iter [1] Loss: 0.3319 Training Accuracy: 0.80741\n",
      "Epoch [1545/2000], Iter [1] Loss: 0.3205 Training Accuracy: 0.81481\n",
      "Epoch [1546/2000], Iter [1] Loss: 0.3186 Training Accuracy: 0.80741\n",
      "Epoch [1547/2000], Iter [1] Loss: 0.3348 Training Accuracy: 0.82963\n",
      "Epoch [1548/2000], Iter [1] Loss: 0.3201 Training Accuracy: 0.81481\n",
      "Epoch [1549/2000], Iter [1] Loss: 0.3118 Training Accuracy: 0.79259\n",
      "Epoch [1550/2000], Iter [1] Loss: 0.3233 Training Accuracy: 0.82963\n",
      "Epoch [1551/2000], Iter [1] Loss: 0.3101 Training Accuracy: 0.81481\n",
      "Epoch [1552/2000], Iter [1] Loss: 0.3238 Training Accuracy: 0.84444\n",
      "Epoch [1553/2000], Iter [1] Loss: 0.3328 Training Accuracy: 0.82963\n",
      "Epoch [1554/2000], Iter [1] Loss: 0.3183 Training Accuracy: 0.82222\n",
      "Epoch [1555/2000], Iter [1] Loss: 0.3177 Training Accuracy: 0.82222\n",
      "Epoch [1556/2000], Iter [1] Loss: 0.3314 Training Accuracy: 0.79259\n",
      "Epoch [1557/2000], Iter [1] Loss: 0.3129 Training Accuracy: 0.83704\n",
      "Epoch [1558/2000], Iter [1] Loss: 0.3079 Training Accuracy: 0.82963\n",
      "Epoch [1559/2000], Iter [1] Loss: 0.3236 Training Accuracy: 0.80741\n",
      "Epoch [1560/2000], Iter [1] Loss: 0.3279 Training Accuracy: 0.82222\n",
      "Epoch [1561/2000], Iter [1] Loss: 0.3334 Training Accuracy: 0.83704\n",
      "Epoch [1562/2000], Iter [1] Loss: 0.3194 Training Accuracy: 0.80741\n",
      "Epoch [1563/2000], Iter [1] Loss: 0.3245 Training Accuracy: 0.80000\n",
      "Epoch [1564/2000], Iter [1] Loss: 0.3266 Training Accuracy: 0.78519\n",
      "Epoch [1565/2000], Iter [1] Loss: 0.3168 Training Accuracy: 0.82222\n",
      "Epoch [1566/2000], Iter [1] Loss: 0.3334 Training Accuracy: 0.82963\n",
      "Epoch [1567/2000], Iter [1] Loss: 0.3336 Training Accuracy: 0.80741\n",
      "Epoch [1568/2000], Iter [1] Loss: 0.3178 Training Accuracy: 0.83704\n",
      "Epoch [1569/2000], Iter [1] Loss: 0.3300 Training Accuracy: 0.78519\n",
      "Epoch [1570/2000], Iter [1] Loss: 0.3249 Training Accuracy: 0.83704\n",
      "Epoch [1571/2000], Iter [1] Loss: 0.3378 Training Accuracy: 0.80741\n",
      "Epoch [1572/2000], Iter [1] Loss: 0.3087 Training Accuracy: 0.82222\n",
      "Epoch [1573/2000], Iter [1] Loss: 0.3156 Training Accuracy: 0.83704\n",
      "Epoch [1574/2000], Iter [1] Loss: 0.3247 Training Accuracy: 0.83704\n",
      "Epoch [1575/2000], Iter [1] Loss: 0.3186 Training Accuracy: 0.84444\n",
      "Epoch [1576/2000], Iter [1] Loss: 0.3100 Training Accuracy: 0.81481\n",
      "Epoch [1577/2000], Iter [1] Loss: 0.3247 Training Accuracy: 0.80741\n",
      "Epoch [1578/2000], Iter [1] Loss: 0.3235 Training Accuracy: 0.78519\n",
      "Epoch [1579/2000], Iter [1] Loss: 0.3213 Training Accuracy: 0.80741\n",
      "Epoch [1580/2000], Iter [1] Loss: 0.3143 Training Accuracy: 0.82222\n",
      "Epoch [1581/2000], Iter [1] Loss: 0.3209 Training Accuracy: 0.80000\n",
      "Epoch [1582/2000], Iter [1] Loss: 0.3253 Training Accuracy: 0.80000\n",
      "Epoch [1583/2000], Iter [1] Loss: 0.3223 Training Accuracy: 0.82222\n",
      "Epoch [1584/2000], Iter [1] Loss: 0.3245 Training Accuracy: 0.79259\n",
      "Epoch [1585/2000], Iter [1] Loss: 0.3235 Training Accuracy: 0.82222\n",
      "Epoch [1586/2000], Iter [1] Loss: 0.3248 Training Accuracy: 0.83704\n",
      "Epoch [1587/2000], Iter [1] Loss: 0.3205 Training Accuracy: 0.80741\n",
      "Epoch [1588/2000], Iter [1] Loss: 0.3207 Training Accuracy: 0.85926\n",
      "Epoch [1589/2000], Iter [1] Loss: 0.3163 Training Accuracy: 0.80741\n",
      "Epoch [1590/2000], Iter [1] Loss: 0.3191 Training Accuracy: 0.80741\n",
      "Epoch [1591/2000], Iter [1] Loss: 0.3233 Training Accuracy: 0.80000\n",
      "Epoch [1592/2000], Iter [1] Loss: 0.3110 Training Accuracy: 0.82963\n",
      "Epoch [1593/2000], Iter [1] Loss: 0.3137 Training Accuracy: 0.82963\n",
      "Epoch [1594/2000], Iter [1] Loss: 0.3207 Training Accuracy: 0.80741\n",
      "Epoch [1595/2000], Iter [1] Loss: 0.3173 Training Accuracy: 0.79259\n",
      "Epoch [1596/2000], Iter [1] Loss: 0.3201 Training Accuracy: 0.82222\n",
      "Epoch [1597/2000], Iter [1] Loss: 0.3321 Training Accuracy: 0.79259\n",
      "Epoch [1598/2000], Iter [1] Loss: 0.3105 Training Accuracy: 0.82963\n",
      "Epoch [1599/2000], Iter [1] Loss: 0.3135 Training Accuracy: 0.76296\n",
      "Epoch [1600/2000], Iter [1] Loss: 0.3218 Training Accuracy: 0.80741\n",
      "Epoch [1601/2000], Iter [1] Loss: 0.3272 Training Accuracy: 0.82222\n",
      "Epoch [1602/2000], Iter [1] Loss: 0.3195 Training Accuracy: 0.80000\n",
      "Epoch [1603/2000], Iter [1] Loss: 0.3263 Training Accuracy: 0.80000\n",
      "Epoch [1604/2000], Iter [1] Loss: 0.3316 Training Accuracy: 0.82963\n",
      "Epoch [1605/2000], Iter [1] Loss: 0.3163 Training Accuracy: 0.80741\n",
      "Epoch [1606/2000], Iter [1] Loss: 0.3144 Training Accuracy: 0.80741\n",
      "Epoch [1607/2000], Iter [1] Loss: 0.3185 Training Accuracy: 0.80000\n",
      "Epoch [1608/2000], Iter [1] Loss: 0.3101 Training Accuracy: 0.80741\n",
      "Epoch [1609/2000], Iter [1] Loss: 0.3094 Training Accuracy: 0.79259\n",
      "Epoch [1610/2000], Iter [1] Loss: 0.3204 Training Accuracy: 0.80741\n",
      "Epoch [1611/2000], Iter [1] Loss: 0.3288 Training Accuracy: 0.85185\n",
      "Epoch [1612/2000], Iter [1] Loss: 0.3197 Training Accuracy: 0.85926\n",
      "Epoch [1613/2000], Iter [1] Loss: 0.3216 Training Accuracy: 0.78519\n",
      "Epoch [1614/2000], Iter [1] Loss: 0.3133 Training Accuracy: 0.82222\n",
      "Epoch [1615/2000], Iter [1] Loss: 0.3284 Training Accuracy: 0.81481\n",
      "Epoch [1616/2000], Iter [1] Loss: 0.3263 Training Accuracy: 0.83704\n",
      "Epoch [1617/2000], Iter [1] Loss: 0.3138 Training Accuracy: 0.82963\n",
      "Epoch [1618/2000], Iter [1] Loss: 0.3116 Training Accuracy: 0.81481\n",
      "Epoch [1619/2000], Iter [1] Loss: 0.3223 Training Accuracy: 0.82963\n",
      "Epoch [1620/2000], Iter [1] Loss: 0.3100 Training Accuracy: 0.83704\n",
      "Epoch [1621/2000], Iter [1] Loss: 0.3192 Training Accuracy: 0.81481\n",
      "Epoch [1622/2000], Iter [1] Loss: 0.3165 Training Accuracy: 0.82963\n",
      "Epoch [1623/2000], Iter [1] Loss: 0.3253 Training Accuracy: 0.82222\n",
      "Epoch [1624/2000], Iter [1] Loss: 0.3124 Training Accuracy: 0.82222\n",
      "Epoch [1625/2000], Iter [1] Loss: 0.3195 Training Accuracy: 0.83704\n",
      "Epoch [1626/2000], Iter [1] Loss: 0.3301 Training Accuracy: 0.82963\n",
      "Epoch [1627/2000], Iter [1] Loss: 0.3021 Training Accuracy: 0.79259\n",
      "Epoch [1628/2000], Iter [1] Loss: 0.3145 Training Accuracy: 0.81481\n",
      "Epoch [1629/2000], Iter [1] Loss: 0.3278 Training Accuracy: 0.82222\n",
      "Epoch [1630/2000], Iter [1] Loss: 0.3194 Training Accuracy: 0.83704\n",
      "Epoch [1631/2000], Iter [1] Loss: 0.3299 Training Accuracy: 0.80000\n",
      "Epoch [1632/2000], Iter [1] Loss: 0.3183 Training Accuracy: 0.80741\n",
      "Epoch [1633/2000], Iter [1] Loss: 0.3243 Training Accuracy: 0.82222\n",
      "Epoch [1634/2000], Iter [1] Loss: 0.3323 Training Accuracy: 0.82222\n",
      "Epoch [1635/2000], Iter [1] Loss: 0.3246 Training Accuracy: 0.82222\n",
      "Epoch [1636/2000], Iter [1] Loss: 0.3253 Training Accuracy: 0.80741\n",
      "Epoch [1637/2000], Iter [1] Loss: 0.3283 Training Accuracy: 0.82963\n",
      "Epoch [1638/2000], Iter [1] Loss: 0.3187 Training Accuracy: 0.81481\n",
      "Epoch [1639/2000], Iter [1] Loss: 0.3106 Training Accuracy: 0.80741\n",
      "Epoch [1640/2000], Iter [1] Loss: 0.3047 Training Accuracy: 0.81481\n",
      "Epoch [1641/2000], Iter [1] Loss: 0.3225 Training Accuracy: 0.80741\n",
      "Epoch [1642/2000], Iter [1] Loss: 0.3185 Training Accuracy: 0.77778\n",
      "Epoch [1643/2000], Iter [1] Loss: 0.3178 Training Accuracy: 0.80741\n",
      "Epoch [1644/2000], Iter [1] Loss: 0.3114 Training Accuracy: 0.78519\n",
      "Epoch [1645/2000], Iter [1] Loss: 0.3131 Training Accuracy: 0.81481\n",
      "Epoch [1646/2000], Iter [1] Loss: 0.3319 Training Accuracy: 0.82963\n",
      "Epoch [1647/2000], Iter [1] Loss: 0.3212 Training Accuracy: 0.84444\n",
      "Epoch [1648/2000], Iter [1] Loss: 0.3078 Training Accuracy: 0.78519\n",
      "Epoch [1649/2000], Iter [1] Loss: 0.3148 Training Accuracy: 0.84444\n",
      "Epoch [1650/2000], Iter [1] Loss: 0.3191 Training Accuracy: 0.80000\n",
      "Epoch [1651/2000], Iter [1] Loss: 0.3168 Training Accuracy: 0.82222\n",
      "Epoch [1652/2000], Iter [1] Loss: 0.3093 Training Accuracy: 0.81481\n",
      "Epoch [1653/2000], Iter [1] Loss: 0.3122 Training Accuracy: 0.80741\n",
      "Epoch [1654/2000], Iter [1] Loss: 0.3218 Training Accuracy: 0.81481\n",
      "Epoch [1655/2000], Iter [1] Loss: 0.3039 Training Accuracy: 0.82222\n",
      "Epoch [1656/2000], Iter [1] Loss: 0.3160 Training Accuracy: 0.80741\n",
      "Epoch [1657/2000], Iter [1] Loss: 0.3138 Training Accuracy: 0.84444\n",
      "Epoch [1658/2000], Iter [1] Loss: 0.3044 Training Accuracy: 0.80000\n",
      "Epoch [1659/2000], Iter [1] Loss: 0.3206 Training Accuracy: 0.82222\n",
      "Epoch [1660/2000], Iter [1] Loss: 0.3153 Training Accuracy: 0.83704\n",
      "Epoch [1661/2000], Iter [1] Loss: 0.3101 Training Accuracy: 0.82963\n",
      "Epoch [1662/2000], Iter [1] Loss: 0.3247 Training Accuracy: 0.82222\n",
      "Epoch [1663/2000], Iter [1] Loss: 0.3325 Training Accuracy: 0.80741\n",
      "Epoch [1664/2000], Iter [1] Loss: 0.3180 Training Accuracy: 0.80000\n",
      "Epoch [1665/2000], Iter [1] Loss: 0.3145 Training Accuracy: 0.82222\n",
      "Epoch [1666/2000], Iter [1] Loss: 0.3164 Training Accuracy: 0.82222\n",
      "Epoch [1667/2000], Iter [1] Loss: 0.3098 Training Accuracy: 0.80000\n",
      "Epoch [1668/2000], Iter [1] Loss: 0.3153 Training Accuracy: 0.82963\n",
      "Epoch [1669/2000], Iter [1] Loss: 0.3172 Training Accuracy: 0.79259\n",
      "Epoch [1670/2000], Iter [1] Loss: 0.3202 Training Accuracy: 0.85185\n",
      "Epoch [1671/2000], Iter [1] Loss: 0.3239 Training Accuracy: 0.82222\n",
      "Epoch [1672/2000], Iter [1] Loss: 0.3126 Training Accuracy: 0.80741\n",
      "Epoch [1673/2000], Iter [1] Loss: 0.3252 Training Accuracy: 0.80741\n",
      "Epoch [1674/2000], Iter [1] Loss: 0.3110 Training Accuracy: 0.80741\n",
      "Epoch [1675/2000], Iter [1] Loss: 0.3131 Training Accuracy: 0.81481\n",
      "Epoch [1676/2000], Iter [1] Loss: 0.3161 Training Accuracy: 0.80741\n",
      "Epoch [1677/2000], Iter [1] Loss: 0.3199 Training Accuracy: 0.81481\n",
      "Epoch [1678/2000], Iter [1] Loss: 0.3159 Training Accuracy: 0.82222\n",
      "Epoch [1679/2000], Iter [1] Loss: 0.3182 Training Accuracy: 0.82222\n",
      "Epoch [1680/2000], Iter [1] Loss: 0.3185 Training Accuracy: 0.82963\n",
      "Epoch [1681/2000], Iter [1] Loss: 0.3185 Training Accuracy: 0.82963\n",
      "Epoch [1682/2000], Iter [1] Loss: 0.3183 Training Accuracy: 0.82222\n",
      "Epoch [1683/2000], Iter [1] Loss: 0.3279 Training Accuracy: 0.80741\n",
      "Epoch [1684/2000], Iter [1] Loss: 0.3178 Training Accuracy: 0.82963\n",
      "Epoch [1685/2000], Iter [1] Loss: 0.3100 Training Accuracy: 0.82963\n",
      "Epoch [1686/2000], Iter [1] Loss: 0.3087 Training Accuracy: 0.80000\n",
      "Epoch [1687/2000], Iter [1] Loss: 0.3106 Training Accuracy: 0.80741\n",
      "Epoch [1688/2000], Iter [1] Loss: 0.3219 Training Accuracy: 0.80000\n",
      "Epoch [1689/2000], Iter [1] Loss: 0.3303 Training Accuracy: 0.84444\n",
      "Epoch [1690/2000], Iter [1] Loss: 0.3038 Training Accuracy: 0.81481\n",
      "Epoch [1691/2000], Iter [1] Loss: 0.3077 Training Accuracy: 0.84444\n",
      "Epoch [1692/2000], Iter [1] Loss: 0.3167 Training Accuracy: 0.80741\n",
      "Epoch [1693/2000], Iter [1] Loss: 0.3229 Training Accuracy: 0.79259\n",
      "Epoch [1694/2000], Iter [1] Loss: 0.3063 Training Accuracy: 0.84444\n",
      "Epoch [1695/2000], Iter [1] Loss: 0.3086 Training Accuracy: 0.82222\n",
      "Epoch [1696/2000], Iter [1] Loss: 0.3105 Training Accuracy: 0.83704\n",
      "Epoch [1697/2000], Iter [1] Loss: 0.3126 Training Accuracy: 0.82963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1698/2000], Iter [1] Loss: 0.3062 Training Accuracy: 0.82222\n",
      "Epoch [1699/2000], Iter [1] Loss: 0.3178 Training Accuracy: 0.84444\n",
      "Epoch [1700/2000], Iter [1] Loss: 0.3048 Training Accuracy: 0.81481\n",
      "Epoch [1701/2000], Iter [1] Loss: 0.3104 Training Accuracy: 0.83704\n",
      "Epoch [1702/2000], Iter [1] Loss: 0.3124 Training Accuracy: 0.81481\n",
      "Epoch [1703/2000], Iter [1] Loss: 0.3130 Training Accuracy: 0.80000\n",
      "Epoch [1704/2000], Iter [1] Loss: 0.3136 Training Accuracy: 0.77778\n",
      "Epoch [1705/2000], Iter [1] Loss: 0.3230 Training Accuracy: 0.83704\n",
      "Epoch [1706/2000], Iter [1] Loss: 0.3159 Training Accuracy: 0.80000\n",
      "Epoch [1707/2000], Iter [1] Loss: 0.3139 Training Accuracy: 0.83704\n",
      "Epoch [1708/2000], Iter [1] Loss: 0.3116 Training Accuracy: 0.79259\n",
      "Epoch [1709/2000], Iter [1] Loss: 0.3181 Training Accuracy: 0.82222\n",
      "Epoch [1710/2000], Iter [1] Loss: 0.3083 Training Accuracy: 0.82222\n",
      "Epoch [1711/2000], Iter [1] Loss: 0.3160 Training Accuracy: 0.82963\n",
      "Epoch [1712/2000], Iter [1] Loss: 0.3065 Training Accuracy: 0.82222\n",
      "Epoch [1713/2000], Iter [1] Loss: 0.3198 Training Accuracy: 0.82222\n",
      "Epoch [1714/2000], Iter [1] Loss: 0.3127 Training Accuracy: 0.80000\n",
      "Epoch [1715/2000], Iter [1] Loss: 0.3097 Training Accuracy: 0.81481\n",
      "Epoch [1716/2000], Iter [1] Loss: 0.3184 Training Accuracy: 0.80000\n",
      "Epoch [1717/2000], Iter [1] Loss: 0.3031 Training Accuracy: 0.84444\n",
      "Epoch [1718/2000], Iter [1] Loss: 0.3190 Training Accuracy: 0.85185\n",
      "Epoch [1719/2000], Iter [1] Loss: 0.3135 Training Accuracy: 0.82963\n",
      "Epoch [1720/2000], Iter [1] Loss: 0.3196 Training Accuracy: 0.78519\n",
      "Epoch [1721/2000], Iter [1] Loss: 0.3128 Training Accuracy: 0.82963\n",
      "Epoch [1722/2000], Iter [1] Loss: 0.3109 Training Accuracy: 0.80741\n",
      "Epoch [1723/2000], Iter [1] Loss: 0.3239 Training Accuracy: 0.80000\n",
      "Epoch [1724/2000], Iter [1] Loss: 0.3109 Training Accuracy: 0.80000\n",
      "Epoch [1725/2000], Iter [1] Loss: 0.3124 Training Accuracy: 0.82963\n",
      "Epoch [1726/2000], Iter [1] Loss: 0.3045 Training Accuracy: 0.80741\n",
      "Epoch [1727/2000], Iter [1] Loss: 0.3137 Training Accuracy: 0.81481\n",
      "Epoch [1728/2000], Iter [1] Loss: 0.3141 Training Accuracy: 0.77778\n",
      "Epoch [1729/2000], Iter [1] Loss: 0.3184 Training Accuracy: 0.80000\n",
      "Epoch [1730/2000], Iter [1] Loss: 0.3191 Training Accuracy: 0.84444\n",
      "Epoch [1731/2000], Iter [1] Loss: 0.3122 Training Accuracy: 0.80000\n",
      "Epoch [1732/2000], Iter [1] Loss: 0.3120 Training Accuracy: 0.81481\n",
      "Epoch [1733/2000], Iter [1] Loss: 0.3208 Training Accuracy: 0.85185\n",
      "Epoch [1734/2000], Iter [1] Loss: 0.3005 Training Accuracy: 0.83704\n",
      "Epoch [1735/2000], Iter [1] Loss: 0.3085 Training Accuracy: 0.82222\n",
      "Epoch [1736/2000], Iter [1] Loss: 0.3157 Training Accuracy: 0.81481\n",
      "Epoch [1737/2000], Iter [1] Loss: 0.3092 Training Accuracy: 0.83704\n",
      "Epoch [1738/2000], Iter [1] Loss: 0.3068 Training Accuracy: 0.81481\n",
      "Epoch [1739/2000], Iter [1] Loss: 0.3197 Training Accuracy: 0.82222\n",
      "Epoch [1740/2000], Iter [1] Loss: 0.3233 Training Accuracy: 0.79259\n",
      "Epoch [1741/2000], Iter [1] Loss: 0.3135 Training Accuracy: 0.82222\n",
      "Epoch [1742/2000], Iter [1] Loss: 0.3142 Training Accuracy: 0.80741\n",
      "Epoch [1743/2000], Iter [1] Loss: 0.3044 Training Accuracy: 0.80741\n",
      "Epoch [1744/2000], Iter [1] Loss: 0.3134 Training Accuracy: 0.82963\n",
      "Epoch [1745/2000], Iter [1] Loss: 0.3073 Training Accuracy: 0.80000\n",
      "Epoch [1746/2000], Iter [1] Loss: 0.3207 Training Accuracy: 0.80741\n",
      "Epoch [1747/2000], Iter [1] Loss: 0.3089 Training Accuracy: 0.82222\n",
      "Epoch [1748/2000], Iter [1] Loss: 0.3179 Training Accuracy: 0.83704\n",
      "Epoch [1749/2000], Iter [1] Loss: 0.3206 Training Accuracy: 0.80000\n",
      "Epoch [1750/2000], Iter [1] Loss: 0.3039 Training Accuracy: 0.85185\n",
      "Epoch [1751/2000], Iter [1] Loss: 0.3226 Training Accuracy: 0.82963\n",
      "Epoch [1752/2000], Iter [1] Loss: 0.3076 Training Accuracy: 0.82963\n",
      "Epoch [1753/2000], Iter [1] Loss: 0.3018 Training Accuracy: 0.82222\n",
      "Epoch [1754/2000], Iter [1] Loss: 0.3060 Training Accuracy: 0.83704\n",
      "Epoch [1755/2000], Iter [1] Loss: 0.3214 Training Accuracy: 0.77778\n",
      "Epoch [1756/2000], Iter [1] Loss: 0.3195 Training Accuracy: 0.81481\n",
      "Epoch [1757/2000], Iter [1] Loss: 0.3162 Training Accuracy: 0.83704\n",
      "Epoch [1758/2000], Iter [1] Loss: 0.3091 Training Accuracy: 0.84444\n",
      "Epoch [1759/2000], Iter [1] Loss: 0.3231 Training Accuracy: 0.82222\n",
      "Epoch [1760/2000], Iter [1] Loss: 0.3093 Training Accuracy: 0.77778\n",
      "Epoch [1761/2000], Iter [1] Loss: 0.3059 Training Accuracy: 0.79259\n",
      "Epoch [1762/2000], Iter [1] Loss: 0.3185 Training Accuracy: 0.81481\n",
      "Epoch [1763/2000], Iter [1] Loss: 0.3151 Training Accuracy: 0.82222\n",
      "Epoch [1764/2000], Iter [1] Loss: 0.3072 Training Accuracy: 0.82963\n",
      "Epoch [1765/2000], Iter [1] Loss: 0.3014 Training Accuracy: 0.82963\n",
      "Epoch [1766/2000], Iter [1] Loss: 0.3161 Training Accuracy: 0.82222\n",
      "Epoch [1767/2000], Iter [1] Loss: 0.3000 Training Accuracy: 0.82222\n",
      "Epoch [1768/2000], Iter [1] Loss: 0.2993 Training Accuracy: 0.81481\n",
      "Epoch [1769/2000], Iter [1] Loss: 0.3122 Training Accuracy: 0.81481\n",
      "Epoch [1770/2000], Iter [1] Loss: 0.3003 Training Accuracy: 0.83704\n",
      "Epoch [1771/2000], Iter [1] Loss: 0.3166 Training Accuracy: 0.83704\n",
      "Epoch [1772/2000], Iter [1] Loss: 0.2975 Training Accuracy: 0.84444\n",
      "Epoch [1773/2000], Iter [1] Loss: 0.3163 Training Accuracy: 0.82222\n",
      "Epoch [1774/2000], Iter [1] Loss: 0.3091 Training Accuracy: 0.82222\n",
      "Epoch [1775/2000], Iter [1] Loss: 0.3038 Training Accuracy: 0.82963\n",
      "Epoch [1776/2000], Iter [1] Loss: 0.3088 Training Accuracy: 0.83704\n",
      "Epoch [1777/2000], Iter [1] Loss: 0.3032 Training Accuracy: 0.80741\n",
      "Epoch [1778/2000], Iter [1] Loss: 0.3128 Training Accuracy: 0.81481\n",
      "Epoch [1779/2000], Iter [1] Loss: 0.3097 Training Accuracy: 0.84444\n",
      "Epoch [1780/2000], Iter [1] Loss: 0.3124 Training Accuracy: 0.82963\n",
      "Epoch [1781/2000], Iter [1] Loss: 0.2904 Training Accuracy: 0.81481\n",
      "Epoch [1782/2000], Iter [1] Loss: 0.3017 Training Accuracy: 0.82222\n",
      "Epoch [1783/2000], Iter [1] Loss: 0.3014 Training Accuracy: 0.81481\n",
      "Epoch [1784/2000], Iter [1] Loss: 0.3135 Training Accuracy: 0.82963\n",
      "Epoch [1785/2000], Iter [1] Loss: 0.3104 Training Accuracy: 0.78519\n",
      "Epoch [1786/2000], Iter [1] Loss: 0.3069 Training Accuracy: 0.82222\n",
      "Epoch [1787/2000], Iter [1] Loss: 0.3145 Training Accuracy: 0.80000\n",
      "Epoch [1788/2000], Iter [1] Loss: 0.3163 Training Accuracy: 0.81481\n",
      "Epoch [1789/2000], Iter [1] Loss: 0.3059 Training Accuracy: 0.81481\n",
      "Epoch [1790/2000], Iter [1] Loss: 0.2979 Training Accuracy: 0.80000\n",
      "Epoch [1791/2000], Iter [1] Loss: 0.3166 Training Accuracy: 0.81481\n",
      "Epoch [1792/2000], Iter [1] Loss: 0.3109 Training Accuracy: 0.80741\n",
      "Epoch [1793/2000], Iter [1] Loss: 0.3133 Training Accuracy: 0.81481\n",
      "Epoch [1794/2000], Iter [1] Loss: 0.3037 Training Accuracy: 0.83704\n",
      "Epoch [1795/2000], Iter [1] Loss: 0.3011 Training Accuracy: 0.81481\n",
      "Epoch [1796/2000], Iter [1] Loss: 0.2948 Training Accuracy: 0.83704\n",
      "Epoch [1797/2000], Iter [1] Loss: 0.3105 Training Accuracy: 0.81481\n",
      "Epoch [1798/2000], Iter [1] Loss: 0.3104 Training Accuracy: 0.80741\n",
      "Epoch [1799/2000], Iter [1] Loss: 0.2990 Training Accuracy: 0.79259\n",
      "Epoch [1800/2000], Iter [1] Loss: 0.2982 Training Accuracy: 0.81481\n",
      "Epoch [1801/2000], Iter [1] Loss: 0.3043 Training Accuracy: 0.82222\n",
      "Epoch [1802/2000], Iter [1] Loss: 0.3165 Training Accuracy: 0.78519\n",
      "Epoch [1803/2000], Iter [1] Loss: 0.3102 Training Accuracy: 0.82963\n",
      "Epoch [1804/2000], Iter [1] Loss: 0.3088 Training Accuracy: 0.82963\n",
      "Epoch [1805/2000], Iter [1] Loss: 0.3063 Training Accuracy: 0.78519\n",
      "Epoch [1806/2000], Iter [1] Loss: 0.3137 Training Accuracy: 0.84444\n",
      "Epoch [1807/2000], Iter [1] Loss: 0.3060 Training Accuracy: 0.85185\n",
      "Epoch [1808/2000], Iter [1] Loss: 0.3169 Training Accuracy: 0.80741\n",
      "Epoch [1809/2000], Iter [1] Loss: 0.3155 Training Accuracy: 0.82963\n",
      "Epoch [1810/2000], Iter [1] Loss: 0.3117 Training Accuracy: 0.83704\n",
      "Epoch [1811/2000], Iter [1] Loss: 0.3068 Training Accuracy: 0.78519\n",
      "Epoch [1812/2000], Iter [1] Loss: 0.3112 Training Accuracy: 0.82963\n",
      "Epoch [1813/2000], Iter [1] Loss: 0.3043 Training Accuracy: 0.81481\n",
      "Epoch [1814/2000], Iter [1] Loss: 0.3152 Training Accuracy: 0.85185\n",
      "Epoch [1815/2000], Iter [1] Loss: 0.3095 Training Accuracy: 0.80000\n",
      "Epoch [1816/2000], Iter [1] Loss: 0.3023 Training Accuracy: 0.82222\n",
      "Epoch [1817/2000], Iter [1] Loss: 0.3059 Training Accuracy: 0.80000\n",
      "Epoch [1818/2000], Iter [1] Loss: 0.3028 Training Accuracy: 0.80000\n",
      "Epoch [1819/2000], Iter [1] Loss: 0.3207 Training Accuracy: 0.81481\n",
      "Epoch [1820/2000], Iter [1] Loss: 0.2998 Training Accuracy: 0.81481\n",
      "Epoch [1821/2000], Iter [1] Loss: 0.3022 Training Accuracy: 0.83704\n",
      "Epoch [1822/2000], Iter [1] Loss: 0.3095 Training Accuracy: 0.85926\n",
      "Epoch [1823/2000], Iter [1] Loss: 0.3061 Training Accuracy: 0.82963\n",
      "Epoch [1824/2000], Iter [1] Loss: 0.3117 Training Accuracy: 0.83704\n",
      "Epoch [1825/2000], Iter [1] Loss: 0.3101 Training Accuracy: 0.80000\n",
      "Epoch [1826/2000], Iter [1] Loss: 0.2907 Training Accuracy: 0.80000\n",
      "Epoch [1827/2000], Iter [1] Loss: 0.2929 Training Accuracy: 0.82222\n",
      "Epoch [1828/2000], Iter [1] Loss: 0.3188 Training Accuracy: 0.83704\n",
      "Epoch [1829/2000], Iter [1] Loss: 0.3232 Training Accuracy: 0.81481\n",
      "Epoch [1830/2000], Iter [1] Loss: 0.3165 Training Accuracy: 0.80741\n",
      "Epoch [1831/2000], Iter [1] Loss: 0.2989 Training Accuracy: 0.82963\n",
      "Epoch [1832/2000], Iter [1] Loss: 0.3075 Training Accuracy: 0.83704\n",
      "Epoch [1833/2000], Iter [1] Loss: 0.2963 Training Accuracy: 0.82222\n",
      "Epoch [1834/2000], Iter [1] Loss: 0.3035 Training Accuracy: 0.83704\n",
      "Epoch [1835/2000], Iter [1] Loss: 0.3064 Training Accuracy: 0.81481\n",
      "Epoch [1836/2000], Iter [1] Loss: 0.3031 Training Accuracy: 0.80741\n",
      "Epoch [1837/2000], Iter [1] Loss: 0.3092 Training Accuracy: 0.82222\n",
      "Epoch [1838/2000], Iter [1] Loss: 0.3111 Training Accuracy: 0.82963\n",
      "Epoch [1839/2000], Iter [1] Loss: 0.3108 Training Accuracy: 0.80741\n",
      "Epoch [1840/2000], Iter [1] Loss: 0.3165 Training Accuracy: 0.80741\n",
      "Epoch [1841/2000], Iter [1] Loss: 0.3046 Training Accuracy: 0.79259\n",
      "Epoch [1842/2000], Iter [1] Loss: 0.3008 Training Accuracy: 0.82222\n",
      "Epoch [1843/2000], Iter [1] Loss: 0.3035 Training Accuracy: 0.79259\n",
      "Epoch [1844/2000], Iter [1] Loss: 0.3068 Training Accuracy: 0.85185\n",
      "Epoch [1845/2000], Iter [1] Loss: 0.3121 Training Accuracy: 0.82222\n",
      "Epoch [1846/2000], Iter [1] Loss: 0.3146 Training Accuracy: 0.82963\n",
      "Epoch [1847/2000], Iter [1] Loss: 0.2990 Training Accuracy: 0.82222\n",
      "Epoch [1848/2000], Iter [1] Loss: 0.3024 Training Accuracy: 0.80741\n",
      "Epoch [1849/2000], Iter [1] Loss: 0.3000 Training Accuracy: 0.82222\n",
      "Epoch [1850/2000], Iter [1] Loss: 0.3098 Training Accuracy: 0.80741\n",
      "Epoch [1851/2000], Iter [1] Loss: 0.3042 Training Accuracy: 0.83704\n",
      "Epoch [1852/2000], Iter [1] Loss: 0.3027 Training Accuracy: 0.82222\n",
      "Epoch [1853/2000], Iter [1] Loss: 0.2984 Training Accuracy: 0.82222\n",
      "Epoch [1854/2000], Iter [1] Loss: 0.3020 Training Accuracy: 0.82963\n",
      "Epoch [1855/2000], Iter [1] Loss: 0.3118 Training Accuracy: 0.81481\n",
      "Epoch [1856/2000], Iter [1] Loss: 0.3067 Training Accuracy: 0.83704\n",
      "Epoch [1857/2000], Iter [1] Loss: 0.3210 Training Accuracy: 0.82222\n",
      "Epoch [1858/2000], Iter [1] Loss: 0.3070 Training Accuracy: 0.80741\n",
      "Epoch [1859/2000], Iter [1] Loss: 0.3020 Training Accuracy: 0.83704\n",
      "Epoch [1860/2000], Iter [1] Loss: 0.3088 Training Accuracy: 0.79259\n",
      "Epoch [1861/2000], Iter [1] Loss: 0.3086 Training Accuracy: 0.78519\n",
      "Epoch [1862/2000], Iter [1] Loss: 0.3094 Training Accuracy: 0.80741\n",
      "Epoch [1863/2000], Iter [1] Loss: 0.3067 Training Accuracy: 0.83704\n",
      "Epoch [1864/2000], Iter [1] Loss: 0.3096 Training Accuracy: 0.79259\n",
      "Epoch [1865/2000], Iter [1] Loss: 0.3126 Training Accuracy: 0.83704\n",
      "Epoch [1866/2000], Iter [1] Loss: 0.3084 Training Accuracy: 0.80000\n",
      "Epoch [1867/2000], Iter [1] Loss: 0.3051 Training Accuracy: 0.80741\n",
      "Epoch [1868/2000], Iter [1] Loss: 0.3029 Training Accuracy: 0.80741\n",
      "Epoch [1869/2000], Iter [1] Loss: 0.3053 Training Accuracy: 0.81481\n",
      "Epoch [1870/2000], Iter [1] Loss: 0.3075 Training Accuracy: 0.80000\n",
      "Epoch [1871/2000], Iter [1] Loss: 0.3114 Training Accuracy: 0.82963\n",
      "Epoch [1872/2000], Iter [1] Loss: 0.3004 Training Accuracy: 0.82963\n",
      "Epoch [1873/2000], Iter [1] Loss: 0.3036 Training Accuracy: 0.82963\n",
      "Epoch [1874/2000], Iter [1] Loss: 0.3039 Training Accuracy: 0.80741\n",
      "Epoch [1875/2000], Iter [1] Loss: 0.3057 Training Accuracy: 0.85185\n",
      "Epoch [1876/2000], Iter [1] Loss: 0.2988 Training Accuracy: 0.82963\n",
      "Epoch [1877/2000], Iter [1] Loss: 0.3066 Training Accuracy: 0.82222\n",
      "Epoch [1878/2000], Iter [1] Loss: 0.2995 Training Accuracy: 0.82222\n",
      "Epoch [1879/2000], Iter [1] Loss: 0.3002 Training Accuracy: 0.83704\n",
      "Epoch [1880/2000], Iter [1] Loss: 0.3074 Training Accuracy: 0.78519\n",
      "Epoch [1881/2000], Iter [1] Loss: 0.3042 Training Accuracy: 0.85185\n",
      "Epoch [1882/2000], Iter [1] Loss: 0.3007 Training Accuracy: 0.82963\n",
      "Epoch [1883/2000], Iter [1] Loss: 0.2972 Training Accuracy: 0.79259\n",
      "Epoch [1884/2000], Iter [1] Loss: 0.3021 Training Accuracy: 0.82222\n",
      "Epoch [1885/2000], Iter [1] Loss: 0.3090 Training Accuracy: 0.83704\n",
      "Epoch [1886/2000], Iter [1] Loss: 0.3061 Training Accuracy: 0.79259\n",
      "Epoch [1887/2000], Iter [1] Loss: 0.3024 Training Accuracy: 0.81481\n",
      "Epoch [1888/2000], Iter [1] Loss: 0.3027 Training Accuracy: 0.84444\n",
      "Epoch [1889/2000], Iter [1] Loss: 0.2950 Training Accuracy: 0.85185\n",
      "Epoch [1890/2000], Iter [1] Loss: 0.2981 Training Accuracy: 0.82963\n",
      "Epoch [1891/2000], Iter [1] Loss: 0.2920 Training Accuracy: 0.80000\n",
      "Epoch [1892/2000], Iter [1] Loss: 0.3106 Training Accuracy: 0.81481\n",
      "Epoch [1893/2000], Iter [1] Loss: 0.2907 Training Accuracy: 0.83704\n",
      "Epoch [1894/2000], Iter [1] Loss: 0.3044 Training Accuracy: 0.80741\n",
      "Epoch [1895/2000], Iter [1] Loss: 0.3058 Training Accuracy: 0.84444\n",
      "Epoch [1896/2000], Iter [1] Loss: 0.3015 Training Accuracy: 0.84444\n",
      "Epoch [1897/2000], Iter [1] Loss: 0.2998 Training Accuracy: 0.84444\n",
      "Epoch [1898/2000], Iter [1] Loss: 0.2871 Training Accuracy: 0.81481\n",
      "Epoch [1899/2000], Iter [1] Loss: 0.2931 Training Accuracy: 0.84444\n",
      "Epoch [1900/2000], Iter [1] Loss: 0.3226 Training Accuracy: 0.82222\n",
      "Epoch [1901/2000], Iter [1] Loss: 0.2992 Training Accuracy: 0.82222\n",
      "Epoch [1902/2000], Iter [1] Loss: 0.3002 Training Accuracy: 0.80741\n",
      "Epoch [1903/2000], Iter [1] Loss: 0.3116 Training Accuracy: 0.84444\n",
      "Epoch [1904/2000], Iter [1] Loss: 0.2975 Training Accuracy: 0.82963\n",
      "Epoch [1905/2000], Iter [1] Loss: 0.3064 Training Accuracy: 0.80741\n",
      "Epoch [1906/2000], Iter [1] Loss: 0.3093 Training Accuracy: 0.83704\n",
      "Epoch [1907/2000], Iter [1] Loss: 0.2943 Training Accuracy: 0.79259\n",
      "Epoch [1908/2000], Iter [1] Loss: 0.3119 Training Accuracy: 0.83704\n",
      "Epoch [1909/2000], Iter [1] Loss: 0.3067 Training Accuracy: 0.80000\n",
      "Epoch [1910/2000], Iter [1] Loss: 0.3061 Training Accuracy: 0.77778\n",
      "Epoch [1911/2000], Iter [1] Loss: 0.3048 Training Accuracy: 0.83704\n",
      "Epoch [1912/2000], Iter [1] Loss: 0.2988 Training Accuracy: 0.80000\n",
      "Epoch [1913/2000], Iter [1] Loss: 0.3108 Training Accuracy: 0.81481\n",
      "Epoch [1914/2000], Iter [1] Loss: 0.3084 Training Accuracy: 0.80741\n",
      "Epoch [1915/2000], Iter [1] Loss: 0.3085 Training Accuracy: 0.82222\n",
      "Epoch [1916/2000], Iter [1] Loss: 0.3121 Training Accuracy: 0.82963\n",
      "Epoch [1917/2000], Iter [1] Loss: 0.3111 Training Accuracy: 0.84444\n",
      "Epoch [1918/2000], Iter [1] Loss: 0.3139 Training Accuracy: 0.82222\n",
      "Epoch [1919/2000], Iter [1] Loss: 0.3040 Training Accuracy: 0.82963\n",
      "Epoch [1920/2000], Iter [1] Loss: 0.2940 Training Accuracy: 0.83704\n",
      "Epoch [1921/2000], Iter [1] Loss: 0.2911 Training Accuracy: 0.80000\n",
      "Epoch [1922/2000], Iter [1] Loss: 0.3018 Training Accuracy: 0.82222\n",
      "Epoch [1923/2000], Iter [1] Loss: 0.3020 Training Accuracy: 0.80000\n",
      "Epoch [1924/2000], Iter [1] Loss: 0.3029 Training Accuracy: 0.82222\n",
      "Epoch [1925/2000], Iter [1] Loss: 0.3061 Training Accuracy: 0.85185\n",
      "Epoch [1926/2000], Iter [1] Loss: 0.3120 Training Accuracy: 0.84444\n",
      "Epoch [1927/2000], Iter [1] Loss: 0.3030 Training Accuracy: 0.80000\n",
      "Epoch [1928/2000], Iter [1] Loss: 0.3143 Training Accuracy: 0.80741\n",
      "Epoch [1929/2000], Iter [1] Loss: 0.3063 Training Accuracy: 0.83704\n",
      "Epoch [1930/2000], Iter [1] Loss: 0.3009 Training Accuracy: 0.80000\n",
      "Epoch [1931/2000], Iter [1] Loss: 0.2959 Training Accuracy: 0.81481\n",
      "Epoch [1932/2000], Iter [1] Loss: 0.3014 Training Accuracy: 0.81481\n",
      "Epoch [1933/2000], Iter [1] Loss: 0.3010 Training Accuracy: 0.81481\n",
      "Epoch [1934/2000], Iter [1] Loss: 0.2889 Training Accuracy: 0.82222\n",
      "Epoch [1935/2000], Iter [1] Loss: 0.2979 Training Accuracy: 0.80000\n",
      "Epoch [1936/2000], Iter [1] Loss: 0.2987 Training Accuracy: 0.82222\n",
      "Epoch [1937/2000], Iter [1] Loss: 0.3096 Training Accuracy: 0.80000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1938/2000], Iter [1] Loss: 0.3070 Training Accuracy: 0.82222\n",
      "Epoch [1939/2000], Iter [1] Loss: 0.2850 Training Accuracy: 0.83704\n",
      "Epoch [1940/2000], Iter [1] Loss: 0.3004 Training Accuracy: 0.80741\n",
      "Epoch [1941/2000], Iter [1] Loss: 0.3080 Training Accuracy: 0.82963\n",
      "Epoch [1942/2000], Iter [1] Loss: 0.3082 Training Accuracy: 0.82963\n",
      "Epoch [1943/2000], Iter [1] Loss: 0.3050 Training Accuracy: 0.77037\n",
      "Epoch [1944/2000], Iter [1] Loss: 0.2980 Training Accuracy: 0.80741\n",
      "Epoch [1945/2000], Iter [1] Loss: 0.2953 Training Accuracy: 0.82963\n",
      "Epoch [1946/2000], Iter [1] Loss: 0.2937 Training Accuracy: 0.84444\n",
      "Epoch [1947/2000], Iter [1] Loss: 0.3006 Training Accuracy: 0.81481\n",
      "Epoch [1948/2000], Iter [1] Loss: 0.3129 Training Accuracy: 0.84444\n",
      "Epoch [1949/2000], Iter [1] Loss: 0.3018 Training Accuracy: 0.82222\n",
      "Epoch [1950/2000], Iter [1] Loss: 0.3077 Training Accuracy: 0.82963\n",
      "Epoch [1951/2000], Iter [1] Loss: 0.2977 Training Accuracy: 0.82222\n",
      "Epoch [1952/2000], Iter [1] Loss: 0.2975 Training Accuracy: 0.81481\n",
      "Epoch [1953/2000], Iter [1] Loss: 0.2982 Training Accuracy: 0.82222\n",
      "Epoch [1954/2000], Iter [1] Loss: 0.3033 Training Accuracy: 0.82963\n",
      "Epoch [1955/2000], Iter [1] Loss: 0.2996 Training Accuracy: 0.83704\n",
      "Epoch [1956/2000], Iter [1] Loss: 0.3083 Training Accuracy: 0.82222\n",
      "Epoch [1957/2000], Iter [1] Loss: 0.3014 Training Accuracy: 0.80000\n",
      "Epoch [1958/2000], Iter [1] Loss: 0.3011 Training Accuracy: 0.83704\n",
      "Epoch [1959/2000], Iter [1] Loss: 0.3090 Training Accuracy: 0.82963\n",
      "Epoch [1960/2000], Iter [1] Loss: 0.2877 Training Accuracy: 0.82222\n",
      "Epoch [1961/2000], Iter [1] Loss: 0.3082 Training Accuracy: 0.81481\n",
      "Epoch [1962/2000], Iter [1] Loss: 0.2916 Training Accuracy: 0.84444\n",
      "Epoch [1963/2000], Iter [1] Loss: 0.3004 Training Accuracy: 0.83704\n",
      "Epoch [1964/2000], Iter [1] Loss: 0.2937 Training Accuracy: 0.81481\n",
      "Epoch [1965/2000], Iter [1] Loss: 0.3127 Training Accuracy: 0.82963\n",
      "Epoch [1966/2000], Iter [1] Loss: 0.2879 Training Accuracy: 0.82963\n",
      "Epoch [1967/2000], Iter [1] Loss: 0.2936 Training Accuracy: 0.81481\n",
      "Epoch [1968/2000], Iter [1] Loss: 0.3047 Training Accuracy: 0.84444\n",
      "Epoch [1969/2000], Iter [1] Loss: 0.2935 Training Accuracy: 0.82963\n",
      "Epoch [1970/2000], Iter [1] Loss: 0.3034 Training Accuracy: 0.80000\n",
      "Epoch [1971/2000], Iter [1] Loss: 0.3071 Training Accuracy: 0.81481\n",
      "Epoch [1972/2000], Iter [1] Loss: 0.2913 Training Accuracy: 0.81481\n",
      "Epoch [1973/2000], Iter [1] Loss: 0.3039 Training Accuracy: 0.82222\n",
      "Epoch [1974/2000], Iter [1] Loss: 0.3053 Training Accuracy: 0.82222\n",
      "Epoch [1975/2000], Iter [1] Loss: 0.2967 Training Accuracy: 0.76296\n",
      "Epoch [1976/2000], Iter [1] Loss: 0.3050 Training Accuracy: 0.81481\n",
      "Epoch [1977/2000], Iter [1] Loss: 0.3032 Training Accuracy: 0.80000\n",
      "Epoch [1978/2000], Iter [1] Loss: 0.3131 Training Accuracy: 0.82222\n",
      "Epoch [1979/2000], Iter [1] Loss: 0.3081 Training Accuracy: 0.81481\n",
      "Epoch [1980/2000], Iter [1] Loss: 0.3008 Training Accuracy: 0.82222\n",
      "Epoch [1981/2000], Iter [1] Loss: 0.3006 Training Accuracy: 0.80000\n",
      "Epoch [1982/2000], Iter [1] Loss: 0.3062 Training Accuracy: 0.82963\n",
      "Epoch [1983/2000], Iter [1] Loss: 0.2983 Training Accuracy: 0.80741\n",
      "Epoch [1984/2000], Iter [1] Loss: 0.2981 Training Accuracy: 0.80741\n",
      "Epoch [1985/2000], Iter [1] Loss: 0.3040 Training Accuracy: 0.82963\n",
      "Epoch [1986/2000], Iter [1] Loss: 0.3029 Training Accuracy: 0.82222\n",
      "Epoch [1987/2000], Iter [1] Loss: 0.2976 Training Accuracy: 0.80741\n",
      "Epoch [1988/2000], Iter [1] Loss: 0.2966 Training Accuracy: 0.83704\n",
      "Epoch [1989/2000], Iter [1] Loss: 0.3001 Training Accuracy: 0.80741\n",
      "Epoch [1990/2000], Iter [1] Loss: 0.2951 Training Accuracy: 0.87407\n",
      "Epoch [1991/2000], Iter [1] Loss: 0.3022 Training Accuracy: 0.82963\n",
      "Epoch [1992/2000], Iter [1] Loss: 0.2973 Training Accuracy: 0.87407\n",
      "Epoch [1993/2000], Iter [1] Loss: 0.2884 Training Accuracy: 0.82963\n",
      "Epoch [1994/2000], Iter [1] Loss: 0.2887 Training Accuracy: 0.84444\n",
      "Epoch [1995/2000], Iter [1] Loss: 0.2979 Training Accuracy: 0.82222\n",
      "Epoch [1996/2000], Iter [1] Loss: 0.3077 Training Accuracy: 0.78519\n",
      "Epoch [1997/2000], Iter [1] Loss: 0.3049 Training Accuracy: 0.77778\n",
      "Epoch [1998/2000], Iter [1] Loss: 0.3024 Training Accuracy: 0.84444\n",
      "Epoch [1999/2000], Iter [1] Loss: 0.2992 Training Accuracy: 0.82963\n",
      "Epoch [2000/2000], Iter [1] Loss: 0.2939 Training Accuracy: 0.81481\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "loss1=[]\n",
    "train_acc=[]\n",
    "\n",
    "Epoch=2000\n",
    "\n",
    "for epoch in range(Epoch):\n",
    "    acc=0\n",
    "    \n",
    "    for i, (features, labels) in enumerate(trainloader):\n",
    "\n",
    "        features = Variable(features)\n",
    "\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        \n",
    "## Forward pass and backward pass will happen in these lines of codes----------------------------\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features=features.float()\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        \n",
    "\n",
    "        loss = criterion(outputs, labels.long())\n",
    "\n",
    "        loss.backward() ##backward \n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "#=================================================================================================#  \n",
    "\n",
    "        \n",
    "\n",
    "        if (i+1) % len(trainloader) == 0:\n",
    "            Ypred = predict(model, torch.from_numpy(X_train).float())\n",
    "            acc = np.mean(Y_train == Ypred)\n",
    "#             train_acc1=train_accuracy/len(trainloader)\n",
    "\n",
    "            train_acc1=acc/len(trainloader)\n",
    "            train_acc.append(train_acc1)\n",
    "            loss1.append(loss.data)\n",
    "\n",
    "            print ('Epoch [%d/%d], Iter [%d] Loss: %.4f Training Accuracy: %.5f' %(epoch+1, 2000, i+1, loss.data, train_acc1 ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will plot our accuracies and loss functions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_loss=loss1[0].numpy()\n",
    "for i in range(len(loss1)):\n",
    "    np_loss=np.append(np_loss, loss1[i])\n",
    "\n",
    "np_acc=train_acc[0]\n",
    "for i in range(len(train_acc)):\n",
    "    np_acc=np.append(np_acc, train_acc[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np_acc, color='blue')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxU1f3/8deHTayAKERlUUGhVkSkEHGBQm2rAipWqy1Ql/Kz4lL3FWutFrV1wdZaUYuKVb8K7hX3HRX3oMgqEhEkQgVBARGFwOf3x5kwk2SSTJLJ3JnJ+/l4zGPOuffMnc/cJJ/cOffcc83dERGR3Nck6gBERCQ9lNBFRPKEErqISJ5QQhcRyRNK6CIieUIJXUQkTyihS04ys6Zm9o2Z7ZLOtiK5TAldMiKWUMsem81sfUL9t7XdnrtvcvdW7v5ZOtumm5mVmNlPM/2+0jg1izoAaRzcvVVZ2cwWAb939xeram9mzdy9NBOxieQLHaFLVjCzq8zsATObZGZrgePM7AAze9vMvjazZWZ2k5k1j7VvZmZuZl1i9f+LrX/GzNaa2Vtm1rUebU8xs2Iz+8rMbkqIs6mZ3WhmK81soZmdaWZ1utzazE6NvcdKM/uvmXWILW8Si2+5ma02s5lm1iO27nAzmxeLu8TMzq3jLpc8pIQu2eQo4H5gW+ABoBQ4G2gP9AcGA6dU8/qRwGXA9sBnwJX1aDsU6Av8mPDP5Rex5acBvwB6AYXA0al9tPLM7BBgLHAM0AlYCtwXWz0E2B/oDmwHDAdWxdbdBZzk7q1jMbxal/eX/KSELtlkmrs/4e6b3X29u7/n7u+4e6m7LwQmAIOqef3D7l7k7hsJybF3Pdr+zd1Xu/siYGrC+l8D/3D3z919FXBt7T8mAL8F7nD3Ge7+HTAGGGRmnYGNQBvgRwDuPtfd/xd73Uagh5m1dvdV7v5+Hd9f8pASumSTJYkVM/uRmT1lZv8zszWEI9r21bz+fwnlb4FWVTVMoW1V6ztWiLNczLXQEVhcVnH3NcBXQCd3fx64DbgV+MLMbjOz1rGmRwHDgM/MbKqZ7VfH95c8pIQu2aRiX/S/gdlAN3dvA/wZsIxHVd4yoHNCfec6bmcpsGtZJZawtwM+B3D3G929D9AT6AGcF1v+jrsPA3YAngQm1/H9JQ8poUs2aw2sBtaZ2Z5U33+eKQ8C55hZRzPbDrgwhde0MLOWCY9mwCTgJDPrZWZbAX8DXnf3EjPrF3s0A9YBG4BNZra1mY00szaxrqK1wKaG+ZiSi5TQJZudD5xISFz/JpwojdqthD71WcB04ClCwq3Oc8D6hMef3P1ZQhfSY4Sj/l0I/eoAbYE7ga+BRbH1/4itOxFYHOuCOgk4Pg2fSfKE6QYXInVnZkcAN7r77lHHIqIjdJFaMLNtzGxwbDx6Z0K//mNRxyUCOkIXqRUza0UY+70HoX/7SeAcd18baWAiKKGLiOQNdbmIiOSJyCbnat++vXfp0iWqtxcRyUnTp0//0t0Lkq2LLKF36dKFoqKiqN5eRCQnmdniqtapy0VEJE/UmNDNbGJsGs/ZVaz/UWz60e/N7IL0hygiIqlI5Qj9P4RpS6uyCjgLGJeOgEREpG5q7EN399fKbgxQxfrlwHIzOyyNcYlIltm4cSMlJSV89913UYfSKLRs2ZLOnTvTvHnzlF+T0ZOiZjYaGA2wyy66X69ILikpKaF169Z06dIFs6gnvcxv7s7KlSspKSmha9euKb8uoydF3X2Cuxe6e2FBQdJRNyKSpb777jvatWunZJ4BZka7du1q/W1Io1xEJGVK5plTl32dewm9tBTuuQeWLo06EhGRrJLKsMVJwFvAHrG7jJ8Uu1v5qbH1O5lZCeGOKn+KtWnTYBE/8ACceCKMGNFgbyEi2WflypX07t2b3r17s9NOO9GpU6ct9Q0bapqSPhg1ahTz58+vts348eO57777qm2TqLi4mN69q7t9beakMsql2swZu3lt5+rapFVhYXh+7TV46CE49tiMvbWIRKddu3bMmDEDgCuuuIJWrVpxwQXlL31xd9ydJk2SH6veddddNb7PH/7wh/oHG5Hc63LZYw946qlQ/vWv4auvoo1HRCJVXFxMz549OfXUU+nTpw/Lli1j9OjRFBYWstdeezF27NgtbQcMGMCMGTMoLS2lbdu2jBkzhn322YcDDjiA5cuXA/CnP/2JG2+8EYC3336bXr16ceCBB3LhhRfWeCS+fv16TjzxRPbee2/69OnDa6+9BsCsWbPYd9996d27N7169WLhwoWsXbuWIUOGsM8++9CzZ08efvjheu+LyOZyqZehQ2G//eCdd8Lzxx9HHZFI43LOORA7Wk6b3r0hlkhra+7cudx1113cdtttAFxzzTVsv/32lJaWctBBB3HMMcfQo0ePcq9ZvXo1gwYN4pprruG8885j4sSJjBkzplybUaNGcffdd9OvX79K3waSuemmm2jRogWzZs1izpw5DB06lAULFnDLLbdwwQUX8Jvf/Ibvv/8ed+fxxx+nS5cuPPPMM1viqa/cO0Iv8+ab4XnBArjoomhjEZFI7b777uy7775b6pMmTaJPnz706dOHefPmMXfu3Eqv2XrrrRkyZAgAffv2ZdGiReXWf/nll2zYsIF+/foBMHLkyBrjmDZtGscfH27zutdee9GxY0eKi4s58MADueqqq7juuutYsmQJLVu2pFevXjz77LOMGTOGN954g2233bauH3+L3DxCB2jSJPSjDxwI118PbdvCH/8YdVQijUMdj6QbyjbbbLOlvGDBAv75z3/y7rvv0rZtW4477rik47lbtGixpdy0aVNKS0vLra/LzX+qes3xxx/PAQccwFNPPcXBBx/M3XffzcCBAykqKuLpp5/mwgsv5PDDD+eP9cxhuXuEDvCTn8C554bypZeGk6Qi0qitWbOG1q1b06ZNG5YtW8Zzzz1Xp+0UFBTQvHnzLdN8T548ucbXDBw4cMsImXnz5rFs2TK6devGwoUL6datG2effTaHHXYYM2fO5PPPP6dVq1Ycf/zxnHfeebz//vt1ijNR7h6hl/n73+Hzz+HBB8NJ0ldegZ/+NOqoRCQiffr0oUePHvTs2ZPddtuN/v3713lbEydOZNSoUbRu3ZqBAwfW2C1y5plncsopp7D33nvTvHlz7rnnHlq0aMH999/PpEmTaN68OR07duSqq67izTffZMyYMTRp0oQWLVps6f+vj8juKVpYWOhpvcHF5ZdD2dnsb7+FrbdO37ZFhHnz5rHnnntGHUZGffPNN7Rq1QqAq6++mlWrVnHDDTdk7P2T7XMzm+7uhcna53aXS6LTT4+Xd9opujhEJG9MmTKF3r1707NnT9566y0uueSSqEOqVv4k9B13hFtuCeU1a6DC8CMRkdoaOXIkM2bMYPbs2Tz55JO0b98+6pCqlT8JHeC002DatFC+9towCkZE0iaqLtrGqC77Or8SOkDiCZBBg+CRR6KLRSSPtGzZkpUrVyqpZ0DZfOgtW7as1etyf5RLMmvXQuvWoXzMMXDEETBlSrQxieS4zp07U1JSwooVK6IOpVEou2NRbeRnQm/VKgxl7NQp1J94Al5/HQYMAM3nLFInzZs3r9XdcyTz8q/LpUzHjpD41XDgwHB16aZN0cUkItKA8jehl6k4g9nEidHEISLSwPI/oR99dPn66NGQhktsRUSyTSp3LJpoZsvNbHYV683MbjKzYjObaWZ90h9mPZhVnl63b18YNy7czk5EJE+kcoT+H2BwNeuHAN1jj9HArfUPK826dy/fnw5w4YVw//3RxCMi0gBqTOju/hqwqpomRwL3ePA20NbMOqQrwLT64IPy9RNPhBdeiCYWEZE0S0cfeidgSUK9JLasEjMbbWZFZlYUyVjW3r3hiy/KLzvkkMzHISLSANKR0JMN7E56KZm7T3D3QncvLCgoSMNb18EOO8D69eWXrVkTTSwiImmUjoReAuycUO8MLE3DdhtOy5bhYqMy225bfrZGEZEclI6EPgU4ITbaZX9gtbsvS8N2G9bhh4erR8vceiskue+giEiuSGXY4iTgLWAPMysxs5PM7FQzOzXW5GlgIVAM3A7kzqHugAGwNOHLxF57waRJ0cUjIlIP+XPHovo44gh48sl4fdky3SRDRLJS47hjUX3cfXf5+n77wTffRBOLiEgdKaEDbL89fPRRvP7ZZ+GoXUQkhyihl9ljDxgxIl6fOjWyUERE6kIJPdF995Wvv/xyNHGIiNSBEnois/JH5jpKF5EcooRe0cCB8fKVV4aj9s2bo4tHRCRFSugVmcGGDfH6cceFi45ERLKcEnoyzZuXr2fLeHkRkWoooVflwQfj5R/8ILo4RERSpIRelWOPhTfeCOVbboEXX4w2HhGRGiihV+fAA+Plgw+OLg4RkRQoodfk4YfjZZ0cFZEspoRek1/9Kl4+/XTYtCm6WEREqqGEnordd4+X//rX6OIQEamGEnoqTjopXn7ppejiEBGphhJ6KsaMgXPPDeVXX4W33oo2HhGRJFJK6GY22Mzmm1mxmY1Jsn5XM3vJzGaa2VQz65z+UCNkBn//e7yeOPpFRCRLpHILuqbAeGAI0AMYYWY9KjQbB9zj7r2AscDf0h1oVqh4IwwRkSySyhF6P6DY3Re6+wZgMnBkhTY9gLLO5VeSrM8PJ5wQLz/ySHRxiIgkkUpC7wQsSaiXxJYl+hAoG993FNDazNpV3JCZjTazIjMrWrFiRV3izR7HHAOrVkUdhYjIFqkkdEuyrOKdpS8ABpnZB8Ag4HOgtNKL3Ce4e6G7FxYUFNQ62KyzfHnUEYiIbJFKQi8Bdk6odwaWJjZw96XufrS7/xi4NLZsddqizCbffx8vJ07gJSISsVQS+ntAdzPramYtgOHAlMQGZtbezMq2dQkwMb1hZpEWLeLlyy+HmTOji0VEJEGNCd3dS4EzgOeAecCD7j7HzMaa2bBYs58C883sY2BH4OoGijf77LMPTJ8edRQiIph7xe7wzCgsLPSiXL1xxNSp4erRhQvjyyLajyLSuJjZdHcvTLZOV4rWxU9/Ci+8EHUUIiLlKKHXVdeu5evr1kUTh4hIjBJ6XZmFeV3K3HZbdLGIiKCEXj8DB8bLa9ZEF4eICEro9Vc2P/rEibB5c7SxiEijpoReX5dcEp5LSuCqq6KNRUQaNSX0dHrooagjEJFGTAk9HY44IjzPnh1tHCLSqCmhp8Ott8bLZrBxY3SxiEijpYSeDp06Qbdu8fobb0QXi4g0Wkro6fLyy/HyvfdGF4eINFpK6Omyc8IMwxPzd7JJEcleSujp9Oij8fKcOdHFISKNkhJ6OvXpEy+ffXZ0cYhIo6SEnk677hovd+wYXRwi0iillNDNbLCZzTezYjMbk2T9Lmb2ipl9YGYzzWxo+kPNEVOnhud774Uvv4w0FBFpXGpM6GbWFBgPDAF6ACPMrEeFZn8i3Mnox4Rb1N2S7kBzxqBB8fJxx8HatdHFIiKNSipH6P2AYndf6O4bgMnAkRXaONAmVt6WCjeRbnTGjg3Pzz0Hl14abSwi0mikktA7AUsS6iWxZYmuAI4zsxLgaeDMZBsys9FmVmRmRStWrKhDuDnipJPi5ddeiy4OEWlUUknolmRZxRtojgD+4+6dgaHAvWZWadvuPsHdC929sKCgoPbR5orEE6IffhhdHCLSqKSS0EuAhKtm6EzlLpWTgAcB3P0toCXQPh0B5qwjE3qlPvggujhEpNFIJaG/B3Q3s65m1oJw0nNKhTafAT8HMLM9CQk9j/tUUrDvvvFynz66+YWINLgaE7q7lwJnAM8B8wijWeaY2VgzGxZrdj5wspl9CEwCfufuFbtlGpcxY+Cmm+J1XTkqIg3Mosq7hYWFXlRUFMl7Z8z8+fCjH8Xrn3+uC45EpF7MbLq7FyZbpytFG1LXruXHpffuHV0sIpL3lNAbUosW8StHAVasgFdfjSwcEclvSuiZdswxUUcgInlKCT0TEpO45ncRkQaihJ4JDz0UdQQi0ggooWfKz34WL7/ySnRxiEjeUkLPlJdeipeffjq6OEQkbymhZ1LZiJdx4+CRRyINRUTyjxJ6Jg0aBEcdFcrnnx9tLCKSd5TQM61bt/C8eDHMmBFtLCKSV5TQM61ly3j5V7+KLg4RyTtK6Jl22mnx8sKF8Oyz0cUiInlFCT3TOnQoXx8yJJo4RCTvKKFH4eWXy9eXL48mDhHJK0roUTjooPL1nj2jiUNE8ooSelQuuiheXrECTj4Zlla8s5+ISOpSSuhmNtjM5ptZsZmNSbL+H2Y2I/b42My+Tn+oeebaa+GJJ+L1O+6A3/wmunhEJOc1q6mBmTUFxgMHE24Y/Z6ZTXH3uWVt3P3chPZnAj9ugFjzz2GHwdZbw/r1ob56dbTxiEhOS+UIvR9Q7O4L3X0DMBk4spr2Iwj3FZWamMFnn8Xrs2bB669HF4+I5LRUEnonYElCvSS2rBIz2xXoCrxcxfrRZlZkZkUrVqyobaz5afvty9cHDowmDhHJeakkdEuyrKo7Sw8HHnb3TclWuvsEdy9098KCgoJUY8xvTZL8CDZsyHwcIpLzUknoJcDOCfXOQFXDMYaj7pbae/PN8vVZs6KJQ0RyWioJ/T2gu5l1NbMWhKQ9pWIjM9sD2A54K70hNgIHHADt2sXrhYXRxSIiOavGhO7upcAZwHPAPOBBd59jZmPNbFhC0xHAZHevqjtGqvPAA+XrJ50UTRwikrMsqvxbWFjoRUVFkbx31iothebN4/VPP4UuXSILR0Syj5lNd/ekX+N1pWg2adas/PS6XbvCxo3RxSMiOUUJPdvcfnv5+tix0cQhIjlHCT3bbLtt+bpGvIhIipTQs81hh8GoUfH6smXRxSIiOUUJPds0aQITJ8b70t99F77WXGciUjMl9Gx1xBHx8nbbwbp10cUiIjlBCT1b3XZb+bqGeIpIDZTQs9X225e/4cWVV0YXi4jkBCX0bNahA9xwQyi/9BLoIlwRqYYSerY799x4+Y47ootDRLKeEnq2s4TZi0eP1rh0EamSEnqu6dUL5s2LOgoRyUJK6LngrQozEj/+eDRxiEhWU0LPBfvvH06KlrnkknABkk6SikgCJfRc8bOfla+7w/r10cQiIllJCT2XVLz/6K9/HU0cIpKVUkroZjbYzOabWbGZjamiza/NbK6ZzTGz+9MbpgAweXL5+lNPwTvvRBOLiGSdGhO6mTUFxgNDgB7ACDPrUaFNd+ASoL+77wWc0wCxyrHHVp4CYP/9o4lFRLJOKkfo/YBid1/o7huAycCRFdqcDIx3968A3H15esOULfr2hU6dyi+74opIQhGR7JJKQu8ELEmol8SWJfoh8EMze8PM3jazwck2ZGajzazIzIpWrFhRt4gFSkrK1//yF/j++2hiEZGskUpCtyTLKo6XawZ0B34KjADuMLO2lV7kPsHdC929sKCgoLaxSqJp08rXW7aEb7+F116LJh4RiVwqCb0E2Dmh3hlYmqTN4+6+0d0/BeYTErw0lP79Ky876igYNAg++yzz8YhI5FJJ6O8B3c2sq5m1AIYDUyq0+S9wEICZtSd0wSxMZ6CSRHFx+frzz4fntWszH4uIRK7GhO7upcAZwHPAPOBBd59jZmPNbFis2XPASjObC7wCXOjuKxsqaInZfffkV4tqKKNIo2Qe0eXjhYWFXqS78KRPs2awaVP5ZWvWQOvW0cQjIg3CzKa7e2GydbpSNF8sWAADBpRfNmpUNLGISCSU0PNF166VR7g88gi88UY08YhIximh5xNLMsJ0wAD48kv4+uvMxyMiGaWEnm+efrrysoIC2G67zMciIhmlhJ5vDjig6nUrNfBIJJ8poeebtm1h3Di48MLK6yrOqS4ieUXDFvNZsj51CMMbK86tLiI5obphi80yHYxk0KOPwh57wF57lV9+/fXwzTfhiP2gg6KJTUTSTkfojcEvf1n1jaW//BLatctsPCJSZ7qwqLH7739hcNIZjWG5pq4XyRdK6I3F2LHJly9enNk4RKTBKKE3Fn37wmWXwSmnlF8+ZAi88EKYC+byy6OJTUTSQn3ojU1pKTRvXvX6k06Cf/4TttkmczGJSMrUhy5xzZqFKXefeAI6d668/s47K9+zVERyghJ6Y3X44TB3bvJ1q1dnNhYRSQsl9MYs1bnSv/mmYeMQkbRIKaGb2WAzm29mxWY2Jsn635nZCjObEXv8Pv2hSoM46qjky88/Pzw//3xI/Lr5tEjWqzGhm1lTYDwwBOgBjDCzHkmaPuDuvWOPO9IcpzSUqk6Q/v3vYeqAp54K9ddeg6lTMxaWiNReKkfo/YBid1/o7huAycCRDRuWZMyBB4bns89Ovv6mm8LzZZeFaQIeeywzcYlIraWS0DsBSxLqJbFlFf3KzGaa2cNmtnOyDZnZaDMrMrOiFStW1CFcSbuzzoKPPoIbb4Qvvqi5/ccfN3xMIlInqST0ZFP2VRy8/gTQxd17AS8CdyfbkLtPcPdCdy8sKCioXaTSMMzCBF4AO+wQxqmffHLV7adP17zqIlkqlYReAiQecXcGliY2cPeV7v59rHo70Dc94UnGNW0KZ55Z9fqHHoL27eGGG8I/A00dIJI1Ukno7wHdzayrmbUAhgNTEhuYWYeE6jBgXvpClIzbe2/o3z+UL7oI+vSp3OaCC8LzP/+Z/LZ3IpJxKV36b2ZDgRuBpsBEd7/azMYCRe4+xcz+RkjkpcAq4DR3/6i6berS/yxXXAzXXgu33gqffw5dulTfPqIpJEQam+ou/ddcLpK6Sy+Fv/61+jbffw8tWmQmHpFGSHO5SHpcfXXNbbbaCj78EGbPhvffb/iYRGQLJXSpnVNPDc/XXlt1m969Qz98377hqH7TpszEJtLIqctFaqe0NAxbbNIkDHNMxdNPh3nXRaTe1OUi6dOsGey4Y+haSdXQobrCVCQDlNClbn7wg/B88cWptT/66DDJl6bmFWkwSuhSN2U3yrjmmviy22+v/jXffANt24YLkrp1C/8M5syJr9cVqCL1ooQu9bd8OdxzD/z+97B5M7z0Us2v+eQTuO466NkzJPg77wxXoI4YEabsffvtho9bJM/opKg0DEs2BVAt6WIlkUp0UlQyb8KE+m/jD38IR/wikhIldGkYJ58M990HX38djrS/+67227jlFli0KF6//fYwfe/GjWkLUySfKKFLwxk5ErbdNpTLpgPYc8/abePNN+Hmm8OsjqNHh6l+f/QjGDdOJ1FFKlBCl8wwCxclzZkDf/lL8jbdulVedvzxYTrfxMnBFi6ECy+MX7UqIoASumRS06Yhsf/5z+GI+/XXw2Rehx4a1j/0UO2298QTsN12YYSNWfxuSuvXw9q16Y1dJAdolItEb9OmcPKzefP6jY7ZZhuYPx86dw51jZKRPKRRLpLdmjYNyby+1q2L3/S6zJw5ug+qNBopJXQzG2xm882s2MzGVNPuGDNzM0v630MkZcceW7fXffZZvGwWLlzaYw8YNiy+XTNYsiT560VyWI0J3cyaAuOBIUAPYISZ9UjSrjVwFvBOuoOURqRVKxg+HB54AB5+OL68vnPAPPFEOIlats1ddoEXXwzTEYjkiVSO0PsBxe6+0N03AJOBI5O0uxK4DqjDgGORmLVrYdKkcBR99NEwdmzoMmnTJoySqY9//7t8/eCD4cQTYdQouOuu8J5FRRoOKTmrWQptOgGJ309LgP0SG5jZj4Gd3f1JM7ugqg2Z2WhgNMAuu+xS+2ilcTGDyy6L15s2DUfq69fDUUeF/vInnwyjZtatC+PUa+vRR8Pzf/4TnvfdNzzrhKrkoFSO0JMNO9jy225mTYB/AOfXtCF3n+Duhe5eWFBQkHqUImXatAnzsb/5Zri46KOPwgVMdbkStTrr1oXnCRPgnXfC8MrbboP770/v+4ikUSpH6CXAzgn1zsDShHproCcw1cKQs52AKWY2zN01LlEyY/hwOOusUO7SpfyUAXXRunUYTnnKKZXX/fKX8fngyxQXQ7t2YVy8SERSOUJ/D+huZl3NrAUwHJhSttLdV7t7e3fv4u5dgLcBJXPJrIKC0E3iDp9+Gvrhd901rBs3LjwnuxK1Ku7w+OPJ122zTeWTtN27x7trRCJSY0J391LgDOA5YB7woLvPMbOxZjasoQMUqZPhw8PJ1Pffh/PPDwl6wQI4/PDUt3HUUVWvO/jgcIOOF1+M3wT7k0/gq69C98xjj8GAAeX74t3Did6XX67bZxKpga4Ulcan7GrUuXOhR6URuPXXsmW8T/+uu+CQQ6BjxzCCp02bcIRf2+GSBx8My5bB7Nnpj1dyiq4UFUn0t7/BwIFh5seyA5qmTdO3/cQTtKNGQadO8OCDIZknWrUK3nsvtW2++GL52/WJJKGELo3PmDHw6qvx+ooV8OWXYVbHhjJyZLy8bl0YLdOuHfTrBx98AFOnxtdv2hT/R7NpE5x2WsPFJXlFXS4iFX36Key2W+bf96WXwtj6rbcOd2u6+ebQLbTXXvE2Gh/f6KnLRaQ2unateirfDRsa7n1//vOQzAHGj4dnngnTDCdaubL6pP7pp/Ex9NLoKKGLJHPMMWGqgS+/LL+8WTOYNg3uvrvqYY3pMnRoeCRq3x6uvrpy2zlz4JVXwjeL/fYLJ36ff75h45Oso4QuUpWmTUM/95IlMGNG6AIxg/794YQTwgyOZWPfE+9z+uabcNFFDRfXZZdB375hCKQZdOgQZpX82c/C+rKTp+PHhyGU06aFbxZr18LkyeEf1aJFcMcdcM45VV9lu3YtHHkklJQ03GeRtFIfuki6jBwZLmgq+5sqGx45cyb06hVNTPvsAx9+WHO7hx4K30oS3Xkn/P73YaTOxIkNE5/UmvrQRTLh3nvh22/j9XvuCfPA7L03vPVWWHbxxclf+/TTDRNTKskcwj+iqmzaBPPmlV/29dfxf1yTJ2tIZZZIZS4XEUlF06bxk5oQbnBdZv/9Q9fGVluFrpr+/cPEXytWwIgR4UTsJ5/A7rtnPm6ANWtg6VKYMgWmTw8zV5Z1xdxzT3gA3HBDmFt+6lS49trQ7TNiRFi3enXlsfaSUepyEYnChg3QokXl5WbhqjK5pX8AAAsDSURBVNLXXw/dJdl+A46uXcPIGghXyD7zTJiSeOxYaNs2/FNo1iw8Fi0K/fyvvgo771ztZqVq6nIRyTbJkjmEIYeffBJGq6xdC9dfD7Nmhe6N6uaWiUpZMoeQvA86CP71r3Cyddmy8I2lefMwJLMs+f/f/4X2GzeGC6ymTSs/FHPGjDDfzQcfhPbffx++DTTkkNF84e6RPPr27esiUgvffuv+ySfuS5eWja2JP2680f3ddysvB/dNm9ynTQvl8ePdFy9279QpedtMPMaNC5/n8ssrf4bWrSu3v+aa8PyPf7hv3ux+2mmhPn9+pD+OqABFXkVeVR+6SK7YeuvyV7B26hT64DdsCFMF77tvmPPl4Yfhd78LR/pNm0KTJqHPPvEoeMkS6NMnHA1n2gUXhEdF55yTvP2qVeH53HPD0f306aH+7LNh2uSttgrz0z/2GBx6KGzeDF98Ebp11q0L3wR22KHmuBYvDjdPadmy5rYbN4ZvHtmmqkzf0A8doYvUw5w57itWuK9d63799eEovC4GDy5/NDxhQrxcUBCeO3SI7mg+lcdWW7mffXYomyVvU52ZM+PfAo49Nr78vvvc77qrcvtZs0LbRx+t2z6vJ6o5QldCF5GQCjp2DOXzz3e/5JLy6zdsKJ8gTzgh+kRe28fxx7uvX+/ev39Ixpdf7j50aOV2l1/uvnJl1f8M7rorvg8ioIQuItV7+mn3JUuqb9Opk/u//uV+772hL3vhQvcXXiif+JYudT/kkFC/887ok3g6Hhdc4D57dvh8mze733xzfF3fvvFyWYIfN879mWfcv/7a/bzzwnmPESPCt6k0qHdCBwYD84FiYEyS9acCs4AZwDSgR03bVEIXyRMXX+z+wx+WX/bppyH5JSbGQw+Nlz/8MPpEXdvHX/5Sv9fffHM4If3qq/Xa3dUl9BrHoZtZU+Bj4GDCDaPfA0a4+9yENm3cfU2sPAw43d0HV7ddjUMXaQQ2boSrrgrj0t9+G/74x3AHp4svDichFy8ON+R+9lkYNCicvK3KlClh4rHTT4dHHsncZ0inDh3CcM4NG+p8UrW6ceipJPQDgCvc/dBY/RIAd/9bFe1HACe4+5DqtquELiKVlM1/M358GKGzaVOYG/6RR8JVqQDLl4fRKACnnAJnnRXmjG/WLEw8lguKisIEa3VQXUJPZdhiJ2BJQr0E2C/Jm/wBOA9oAfysikBGA6MBdtlllxTeWkQalblzw7DDxJt6nH56+TY77BA6MRJ9/TW0agXvvgvXXRcmG1u8OMwx89BD8YuZKlq6FG69Fa68Mvn6iy4KV+z+9rd1/0zJfPBBnRN6dVK5UtSSLKt0WO/u4919d+Bi4E/JNuTuE9y90N0LCwoKahepiOS/Pfcsn8xTte224Yj+gAPCePRmzcK8OMOGhUnTpk6Fbt1C20MPDc877RS6QMaODa8rs3ZtmDL49dfDFaojR8L//he+NVSU+Lra+Oyzur2uBqkk9BIgceKFzsDSatpPBn5Zn6BERNJq0CA477xQPvfckFAXLYqv/8lPwvOSJeFIv1MnGDAgvn7HHcM3hd6948sefDDcVATKT8TWrh189FH593/hhdB3XubnP6/3R0qqqrOlZQ9Ct8xCoCuhO+VDYK8KbbonlI+gmrOwZQ+NchGRjNq82f2995Kv++67MLwwFWVj1zduDPW1a91LS91/8IOw/Morw3j3stEtzzwT2m3a5P6b37hPmlSvj1Fdfq2xD93dS83sDOA5oCkw0d3nmNnY2IanAGeY2S+AjcBXwIlp/r8jIlI/ZlCY9FximD4g1RuDT5kC69eHbh0IR/QQpmG4/PLwTaBly8r9/E2ahLnjG5CmzxURySGaPldEpBFQQhcRyRNK6CIieUIJXUQkTyihi4jkCSV0EZE8oYQuIpInlNBFRPJEZBcWmdkKYHEdX94e+DKN4aRLtsYF2Rub4qodxVU7+RjXru6edHbDyBJ6fZhZUVVXSkUpW+OC7I1NcdWO4qqdxhaXulxERPKEErqISJ7I1YQ+IeoAqpCtcUH2xqa4akdx1U6jiisn+9BFRKSyXD1CFxGRCpTQRUTyRM4ldDMbbGbzzazYzMZk+L13NrNXzGyemc0xs7Njy68ws8/NbEbsMTThNZfEYp1vZoc2YGyLzGxW7P2LYsu2N7MXzGxB7Hm72HIzs5ticc00sz4NFNMeCftkhpmtMbNzothfZjbRzJab2eyEZbXeP2Z2Yqz9AjOr9525qojrejP7KPbej5lZ29jyLma2PmG/3Zbwmr6xn39xLPZkN3evb1y1/rml+++1irgeSIhpkZnNiC3P5P6qKjdk9nesqnvTZeODcAu8T4DdiN/ftEcG378D0CdWbg18DPQArgAuSNK+RyzGrQj3ZP0EaNpAsS0C2ldYdh0wJlYeA1wbKw8FngEM2B94J0M/u/8Bu0axv4CBQB9gdl33D7A94f662wPbxcrbNUBchwDNYuVrE+LqktiuwnbeBQ6IxfwMMKQB4qrVz60h/l6TxVVh/Q3AnyPYX1Xlhoz+juXaEXo/oNjdF7r7BmAycGSm3tzdl7n7+7HyWmAe0KmalxwJTHb37939U6CY8Bky5Ujg7lj5buCXCcvv8eBtoK2ZdWjgWH4OfOLu1V0d3GD7y91fA1Yleb/a7J9DgRfcfZW7fwW8AAxOd1zu/ry7l8aqbwOdq9tGLLY27v6Wh6xwT8JnSVtc1ajq55b2v9fq4oodZf8amFTdNhpof1WVGzL6O5ZrCb0TsCShXkL1CbXBmFkX4MfAO7FFZ8S+Ok0s+1pFZuN14Hkzm25mo2PLdnT3ZRB+4YAdIoirzHDK/6FFvb+g9vsniv32/whHcmW6mtkHZvaqmf0ktqxTLJZMxFWbn1um99dPgC/cfUHCsozvrwq5IaO/Y7mW0JP1c2V83KWZtQIeAc5x9zXArcDuQG9gGeFrH2Q23v7u3gcYAvzBzAZW0zaj+9HMWgDDgIdii7Jhf1Wnqjgyvd8uBUqB+2KLlgG7uPuPgfOA+82sTQbjqu3PLdM/zxGUP2jI+P5KkhuqbFpFDPWKLdcSegmwc0K9M7A0kwGYWXPCD+w+d38UwN2/cPdN7r4ZuJ14N0HG4nX3pbHn5cBjsRi+KOtKiT0vz3RcMUOA9939i1iMke+vmNrun4zFFzsZdjjw21i3ALEujZWx8nRC//QPY3Eldss0SFx1+Lllcn81A44GHkiIN6P7K1luIMO/Y7mW0N8DuptZ19hR33BgSqbePNZHdycwz93/nrA8sf/5KKDsDPwUYLiZbWVmXYHuhJMx6Y5rGzNrXVYmnFSbHXv/srPkJwKPJ8R1QuxM+/7A6rKvhQ2k3JFT1PsrQW33z3PAIWa2Xay74ZDYsrQys8HAxcAwd/82YXmBmTWNlXcj7J+FsdjWmtn+sd/RExI+Szrjqu3PLZN/r78APnL3LV0pmdxfVeUGMv07Vp8zu1E8CGeHPyb8t700w+89gPD1ZyYwI/YYCtwLzIotnwJ0SHjNpbFY51PPM+nVxLUbYQTBh8Ccsv0CtANeAhbEnrePLTdgfCyuWUBhA+6zHwArgW0TlmV8fxH+oSwDNhKOgk6qy/4h9GkXxx6jGiiuYkI/atnv2G2xtr+K/Xw/BN4HjkjYTiEhwX4C3EzsKvA0x1Xrn1u6/16TxRVb/h/g1AptM7m/qsoNGf0d06X/IiJ5Ite6XEREpApK6CIieUIJXUQkTyihi4jkCSV0EZE8oYQuIpInlNBFRPLE/weyPIHTFrEB/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np_loss, color='red', label='Trainig loss')\n",
    "plt.title(\"Traininng Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loss and accuracy curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np_loss, color='red', label='Trainig loss')\n",
    "plt.plot(np_acc, color='blue', label='Training Accuracy')\n",
    "plt.title(\"Loss/Accuracy curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "Ypred = predict(model, torch.from_numpy(X_test).float())\n",
    "acc = np.mean(Y_test == Ypred)\n",
    "print('Test accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         5\n",
      "Iris-versicolor       0.60      1.00      0.75         6\n",
      " Iris-virginica       0.00      0.00      0.00         4\n",
      "\n",
      "       accuracy                           0.73        15\n",
      "      macro avg       0.53      0.67      0.58        15\n",
      "   weighted avg       0.57      0.73      0.63        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['data_set-setosa', 'data_set-versicolor', 'data_set-virginica']\n",
    "print(classification_report(Y_test, Ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
